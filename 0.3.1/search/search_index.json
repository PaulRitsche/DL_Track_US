{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to","text":"<p>DL_Track_US is a Python toolkit for the automated and manual analysis of muscle architecture in human lower limb ultrasonography images and videos. It provides a user-friendly graphical interface and supports fully automated batch processing of both images and videos.</p> <p>The method uses deep learning models trained on longitudinal ultrasound images of the gastrocnemius medialis, tibialis anterior, soleus, and vastus lateralis muscles. It extracts key architectural parameters:</p> <p>\u2705 Fascicle length \u2705 Pennation angle \u2705 Muscle thickness  </p> <p>Originally introduced by Cronin, Seynnes, and Finni in 2020, the method was substantially expanded in 2022 by Ritsche and colleagues. The full method was published in 2023 and 2024:  </p> <ul> <li>\ud83d\udcc4 Journal of Open Source Software, 2023</li> <li>\ud83d\udcc4 Ultrasound in Medicine &amp; Biology, 2024</li> </ul>"},{"location":"#why-use-it","title":"Why use it?","text":"<ul> <li> <p>\ud83d\udd0d Objective analysis   The automated pipeline minimizes user influence by removing the need for manual input during processing.</p> </li> <li> <p>\ud83d\ude85 Fast performance   Each image or frame is processed in under one second\u2014much faster than manual analysis.</p> </li> <li> <p>\ud83d\udcbe Efficient batch processing   Analyze full folders of images or videos in a single run.</p> </li> <li> <p>\ud83d\udc53 Graphical interface   No command-line experience needed. Use the GUI to configure settings and monitor progress.</p> </li> <li> <p>\ud83d\udc41\u200d\ud83d\udde8 Community effort   Open-source project aimed to increase reproducibility and collaboration.</p> </li> </ul>"},{"location":"#before-you-start","title":"Before You Start","text":"<ul> <li>Test the pretrained models on your own data. Retraining may be necessary for other muscles or devices.</li> <li>Image quality is critical. Ensure good contrast, brightness, and visibility of fascicles and aponeuroses.</li> <li>Generalization may be limited. Device types and acquisition settings can affect results.</li> <li>Poor predictions? Visually inspect model output and compare it to manual labels. Adjust parameters or train new models if needed.</li> <li>Use the test scripts in the <code>DL_Track_US_v0.3.0/tests</code> folder to confirm everything runs as expected on your system.</li> </ul>"},{"location":"#known-limitations","title":"Known Limitations","text":"<ul> <li>No unit tests are currently included.</li> <li>Pretrained models are limited to lower limb muscles.</li> <li>Only four ultrasound devices were represented in training.</li> <li>Long video files (e.g., &gt;2000 frames) may still take several minutes to process.</li> <li>The training data was manually annotated, so some subjectivity remains despite automation.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>DL_Track_US is open source and community-driven. If you encounter issues, have suggestions, or want to contribute improvements, visit the project on GitHub.</p>"},{"location":"about_us/","title":"\u2728 About DL_Track_US","text":""},{"location":"about_us/#mission","title":"Mission","text":"<p>Our mission is to make deep learning-based muscle architecture analysis accessible to researchers, students, and clinicians working with musculoskeletal ultrasound. DL_Track_US provides an intuitive graphical interface to segment and analyze fascicles and aponeuroses, allowing cutting-edge research without requiring advanced programming skills.</p>"},{"location":"about_us/#goals","title":"Goals","text":"<ul> <li>Democratize the use of deep learning for ultrasound image and video analysis.</li> <li>Provide a user-friendly and customizable software platform.</li> <li>Foster open-science practices by making the tool freely available.</li> <li>Continuously improve segmentation accuracy through updates and community feedback.</li> </ul>"},{"location":"about_us/#roadmap","title":"Roadmap","text":"<p>\u2705 Current Version (v0.3.0) includes:</p> <ul> <li>Automated and manual analysis of images and videos.</li> <li>Fascicle, aponeurosis segmentation and muscle parameter extraction.</li> <li>Model training module.</li> <li>Video preprocessing tools (cropping, resizing, anonymization).</li> </ul> <p>\ud83d\udd1c Future Plans:</p> <ul> <li>Add intuitive manual labelling tools.</li> <li>Combine with feature tracking algorithm, e.g., Lucas-Kanade or kalman filter.</li> <li>Finalize fascicle curvature estimation and consideration.</li> <li>Extend model architectures (e.g., attention U-Nets, transformers).</li> <li>Broaden community contribution possibilities.</li> </ul>"},{"location":"about_us/#meet-the-developers","title":"Meet the Developers","text":""},{"location":"about_us/#paul-ritsche","title":"Paul Ritsche","text":"<p>Paul is the lead developer and primary coder behind DL_Track_US. He designed, implemented, and maintains the software in close collaboration with Neil Cronin. Paul has a background in biomechanics, ultrasound imaging, and software development.</p> <p>\ud83d\udd17 Read more about Paul</p>"},{"location":"about_us/#neil-cronin","title":"Neil Cronin","text":"<p>Neil is the co-developer and main visionary behind DL_Track_US. His research in biomechanics and muscle architecture guided the core concepts and user-centered design of the tool.</p> <p>\ud83d\udd17 Read more about Neil</p>"},{"location":"about_us/#olivier-seynnes","title":"Olivier Seynnes","text":"<p>Olivier has been involved with DL_Track_US from the early stages, providing critical input, scientific validation, and feedback during development. Olivier\u2019s expertise in muscle physiology and musculoskeletal ultrasound ensured DL_Track_US met high scientific standards.</p> <p>\ud83d\udd17 Read more about Olivier</p>"},{"location":"api_advanced_analysis/","title":"Advanced Functionalities","text":""},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis","title":"<code>AdvancedAnalysis</code>","text":"<p>Function to open a toplevel where masks can either be created for training purposes or can be inspected subsequent to labelling.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.is_running","title":"<code>is_running</code>  <code>property</code> <code>writable</code>","text":"<p>Instance method to define the is_running property getter method. By defining this as a property, is_running is treated like a public attribute even though it is private.</p> <p>This is used to stop the analysis process running in a seperate thread.</p> RETURNS DESCRIPTION <code>is_running</code> <p>Boolean variable to check whether the analysis process started from the GUI is running. The process is only stopped when is_running = True.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.should_stop","title":"<code>should_stop</code>  <code>property</code> <code>writable</code>","text":"<p>Instance method to define the should_stop property getter method. By defining this as a property, should_stop is treated like a public attribute even though it is private.</p> <p>This is used to stop the analysis process running in a seperate thread.</p> RETURNS DESCRIPTION <code>should_stop</code> <p>Boolean variable to decide whether the analysis process started from the GUI should be stopped. The process is stopped when should_stop = True.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.augment_images","title":"<code>augment_images()</code>","text":"<p>Instance method to augment input images, when the \"Augment Images\" button is pressed. Input parameters for the gui_helpers.image_augmentation function are taken from the chosen image and mask directories. The newly generated data will be saved under the same directories.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.crop_video","title":"<code>crop_video()</code>","text":"<p>Crops the loaded video based on the selected start and end frames.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.display_frame","title":"<code>display_frame(frame_index)</code>","text":"<p>Displays a specific frame on the canvas.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.do_break","title":"<code>do_break()</code>","text":"<p>Instance method to break the analysis process when the button \"break\" is pressed.</p> <p>This changes the instance attribute self.should_stop to True, given that the analysis is already running. The attribute is checked befor every iteration of the analysis process.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.get_mask_dir","title":"<code>get_mask_dir()</code>","text":"<p>Instance method to ask the user to select the training mask directory path. All mask files (of the same specified filetype) in the directory are analysed.The mask files and the corresponding image must have the exact same name. This must be an absolute path.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.get_output_dir","title":"<code>get_output_dir()</code>","text":"<p>Instance method to ask the user to select the output directory path. Here, all file created during model training (model file, weight file, graphs) are saved. This must be an absolute path.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.get_train_dir","title":"<code>get_train_dir()</code>","text":"<p>Instance method to ask the user to select the training image directory path. All image files (of the same specified filetype) in the directory are analysed. This must be an absolute path.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.load_video","title":"<code>load_video()</code>","text":"<p>Loads a video and displays the first frame in the cropping interface.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.on_mask_change","title":"<code>on_mask_change(*args)</code>","text":"<p>Depending on which mask opration is selected, this function adapts the GUI.</p> <p>Instance method to open new window for model training. The window is opened upon pressing of the \"analysis parameters\" button.</p> <p>Several parameters are displayed. - Image Directory: The user must select or input the image directory. This path must to the directory containing the training images. Images must be in RGB format. - Mask Directory: The user must select or input the mask directory. This path must to the directory containing the training images. Masks must be binary. - Output Directory: The user must select or input the mask directory. This path must lead to the directory where the trained model and the model weights should be saved. - Batch Size: The user must input the batch size used during model training by selecting from the dropdown list or entering a value. Although a larger batch size has advantages during model trainig, the images used here are large. Thus, the larger the batch size, the more compute power is needed or the longer the training duration. Integer, must be non-negative and non-zero. - Learning Rate: The user must enter the learning rate used for model training by selecting from the dropdown list or entering a value. Float, must be non-negative and non-zero. - Epochs: The user must enter the number of Epochs used during model training by selecting from the dropdown list or entering a value. The total amount of epochs will only be used if early stopping does not happen. Integer, must be non-negative and non-zero. - Loss Function: The user must enter the loss function used for model training by selecting from the dropdown list. These can be \"BCE\" (binary cross-entropy), \"Dice\" (Dice coefficient) or \"FL\"(Focal loss).</p> <p>Model training is started by pressing the \"start training\" button. Although all parameters relevant for model training can be adapted, we advise users with limited experience to keep the pre-defined settings. These settings are best practice and devised from the original papers that proposed the models used here. Singularly the batch size should be adapted to 1 if comupte power is limited (no GPU or GPU with RAM lower than 8 gigabyte).</p> <p>There is an \"Augment Images\" button, which allows to generate new training images. The images and masks for the data augmentation are taken from the chosen image directory and mask directory. The new images are saved under the same directories.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.on_slider_change","title":"<code>on_slider_change(value)</code>","text":"<p>Triggered when the slider is moved. Displays the corresponding frame.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.remove_video_parts","title":"<code>remove_video_parts()</code>","text":"<p>Removes the selected parts from the video and saves the modified video.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.resize_video","title":"<code>resize_video()</code>","text":"<p>Crops the selected area from the video and saves the cropped video.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.train_model","title":"<code>train_model()</code>","text":"<p>Instance method to execute the model training when the \"start training\" button is pressed.</p> <p>By pressing the button, a seperate thread is started in which the model training is run. This allows the user to break any training process at certain stages. When the analysis can be interrupted, a tk.messagebox opens asking the user to either continue or terminate the analysis. Moreover, the threading allows interaction with the GUI during ongoing analysis process.</p>"},{"location":"api_advanced_analysis/#DL_Track_US.gui_modules.advanced_analysis.AdvancedAnalysis.update_slider_range","title":"<code>update_slider_range()</code>","text":"<p>Updates the slider to match the number of frames in the video.</p>"},{"location":"api_helpers/","title":"GUI helper modules","text":"Description <p>This module contains functions to calculate muscle architectural parameters based on binary segmentations by convolutional neural networks. The parameters include muscle thickness, pennation angle and fascicle length. First, input images are segmented by the CNNs. Then the predicted aponeuroses and fascicle fragments are thresholded and filtered. Fascicle fragments and aponeuroses are extrapolated and the intersections determined. This module is specifically designed for single image analysis. The architectural parameters are calculated and the results are plotted.</p> Functions scope <p>sortContours     Function to sort detected contours from proximal to distal. contourEdge     Function to find only the coordinates representing one edge     of a contour. doCalculations     Function to compute muscle architectural parameters based on     convolutional neural network segmentation.</p> Notes <p>Additional information and usage examples can be found at the respective functions documentations.</p> Description <p>This module contains functions for three different approaches to calculate muscle architectural parameters accounted for fascicle curvature. The parameters include muscle thickness, pennation angle and fascicle length. All calculations are based on a fascicle mask and aponeurses mask. Fascicle fragments are connected and extrapolated and the intersection points with the extrapolated aponeuroses are determined. The architectural parameters are calculated and the results are plotted. Additionally, it is possible to calculate an orientation map of the fascicles based on the fascicle mask. This module is specifically designed for single image analysis.</p> Functions scope <p>curve_polyfitting     Function to calculate the fascicle length and pennation angle accounted     for curvature following a second order polynomial fitting approach. curve_connect     Function to calculate the fascicle length and pennation angle accounted     for curvature following a approach of connecting fascicles. orientation_map     Function to calculate an orientation map based on the fascicle mask. doCalculations_curved     Function to compute muscle architectural parameters accounted for fascicle curvature.</p> Notes <p>Additional information can be found at the respective functions documentations.</p> Description <p>This module contains functions to caculate muscle architectural parameters based on binary segmentations by convolutional neural networks. The parameters include muscle thickness, pennation angle and fascicle length. First, input images are segmented by the CNNs. Then the predicted aponeuroses and fascicle fragments are thresholded and filtered. Fascicle fragments and aponeuroses are extrapolated and the intersections determined. This module is specifically designed for video analysis and is predisposed for execution from a tk.TK GUI instance. The architectural parameters are calculated. The results are plotted and converted to an output video displaying the segmentations. Each frame is evaluated separately, independently from the previous frames.</p> Functions scope <p>doCalculations     Function to compute muscle architectural parameters based on     convolutional neural netwrok segmentation.</p> Notes <p>Additional information and usage examples can be found at the respective functions documentations. See specifically do_calculations.py.</p> See Also <p>do_calculations.py</p> Description <p>This module contains all additional functions used in the module do_calculations_curved</p> Functions scope <p>adapted_contourEdge     Function to find only the coordinates representing one edge     of a contour. contourEdge     Function to find only the coordinates representing one edge     of a contour. sortContours     Function to sort detected contours from proximal to distal. do_curves_intersect     Function to detect wheter two curves are intersecting or not. adapted_filter_fascicles     Filters out fascicles that intersect with other fascicles. is_point_in_range     Function to detect wheter a point is between an upper and a lower     boundary or not. find_next_fascicle     Function to find the next fascicle contour. find_complete_fascicle     Function to find complete fascicles based on connection of single contours. crop     Function to crop the frame around ultrasound images.</p> <p>Module to filter the data subsequent to the analyses functions.</p> Description <p>This module contains functions to automatically or manually analyse muscle architecture in longitudinal ultrasonography images of human lower limb muscles. The scope of the automatic method is limited to the vastus lateralis, tibialis anterior, gastrocnemius medialis and soleus muscles due to training data availability. The scope of the manual method is not limited to specific muscles. The module was specifically designed to be executed from a GUI. When used from the GUI, the module saves the analysis results in a .xlsx file to a given directory. The user needs to provide paths to the image, model, and flipflag file directories.</p> Functions scope <p>importAndReshapeImage     Function to import and reshape an image. Moreover, based upon     user specification the image might be flipped. importImageManual      Function to import an image. importFlipFlagsList      Function to retrieve flip values from a .txt file. compileSaveResults      Function to save the analysis results to a .xlsx file. IoU     Function to compute the intersection over union score (IoU),     a measure of prediction accuracy. This is sometimes also called Jaccard     score. calculateBatch     Function to calculate muscle architecture in longitudinal ultrasonography     images of human lower limb muscles. The values computed are fascicle     length (FL), pennation angle (PA), and muscle thickness (MT). calculateBatchManual     Function used for manual calculation of fascicle length, muscle thickness     and pennation angles in longitudinal ultrasonography images of human lower     limb muscles.</p> Notes <p>Additional information and usage exaples can be found at the respective functions documentations.</p> Description <p>This module contains functions to automatically or manually analyse muscle architecture in longitudinal ultrasonography videos of human lower limb muscles. The scope of the automatic method is limited to the vastus lateralis, tibialis anterior, gastrocnemius medialis and soleus muscles due to training data availability. The scope of the manual method is not limited to specific muscles. The module was specifically designed to be executed from a GUI. When used from the GUI, the module saves the analysis results in a .xlsx file to a given directory. The user needs to provide paths to the video, model, and flipflag file directories. With both methods, every frame is analyzed seperately and the results for each frame are saved.</p> Functions scope <p>importAndReshapeImage     Function to import and reshape an image. Moreover, based upon     user specification the image might be flipped. importImageManual     Function to import an image. importFlipFlagsList     Function to retrieve flip values from a .txt file. compileSaveResults     Function to save the analysis results to a .xlsx file. calculateBatch     Function to calculate muscle architecture in longitudinal ultrasonography     images of human lower limb muscles. The values computed are fascicle length     (FL), pennation angle (PA), and muscle thickness (MT). calculateBatchManual     Function used for manual calculation of fascicle length, muscle thickness     and pennation angles in longitudinal ultrasonography images of human lower     limb muscles.</p> Notes <p>Additional information and usage examples can be found at the respective functions documentations.</p> See Also <p>calculate_architecture.py</p> Description <p>This module contains functions to automatically scale images. The scope of the automatic method is limited to scaling bars being present in the right side of the image. However, the distance between two selected points in the image required for the scaling must be known.</p> Functions scope <p>calibrateDistanceStatic     Function to calibrate an image to convert measurements     in pixel units to centimeters.</p> Description <p>This module contains a class to manually annotate longitudinal ultrasonography images and videos. When the class is initiated, a graphical user interface is opened. There, the user can annotate muscle fascicle length, pennation angle and muscle thickness. Moreover, the images can be scaled in order to get measurements in centimeters rather than pixels. By clicking the respective buttons in the GUI, the user can switch between the different parameters to analyze. The analysis is not restricted to any specific muscles. However, its use is restricted to a specific method for assessing muscle thickness, fascicle length and pennation angles. Moreover, each video frame is analyzed separately. An .xlsx file is retuned containing the analysis results for muscle fascicle length, pennation angle and muscle thickness.</p> Functions scope <p>For scope of the functions see class documentation.</p> Notes <p>Additional information and usage examples can be found at the respective functions docstrings.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations.contourEdge","title":"<code>contourEdge(edge, contour)</code>","text":"<p>Function to find only the coordinates representing one edge of a contour.</p> <p>Either the upper or lower edge of the detected contours is calculated. From the contour detected lower in the image, the upper edge is searched. From the contour detected higher in the image, the lower edge is searched.</p> PARAMETER DESCRIPTION <code>edge</code> <p>String variable defining the type of edge that is searched. The variable can be either \"T\" (top) or \"B\" (bottom).</p> <p> TYPE: <code>(T, B)</code> DEFAULT: <code>\"T\"</code> </p> <code>contour</code> <p>List variable containing sorted contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Array variable containing all x-coordinates from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array variable containing all y-coordinated from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; contourEdge(edge=\"T\", contour=[[[195 104]] ... [[196 104]]])\n[196 197 198 199 200 ... 952 953 954 955 956 957],\n[120 120 120 120 120 ... 125 125 125 125 125 125]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations.doCalculations","title":"<code>doCalculations(original_image, img_copy, h, w, calib_dist, spacing, model_apo, model_fasc, dictionary, filter_fasc, image_callback=None)</code>","text":"<p>Function to compute muscle architectural parameters based on convolutional neural network segmentation in images.</p> <p>Firstly, images are segmented by the network. Then, predictions are thresholded and filtered. The aponeuroses edges are computed and the fascicle length and pennation angle calculated. This is done by extrapolating fascicle segments above a threshold length. Then the intersection between aponeurosis edge and fascicle structures are computed. Returns none when not more than one aponeurosis contour is detected in the image.</p> PARAMETER DESCRIPTION <code>original_image</code> <pre><code>Normalized, reshaped and rescaled rayscale image to be\nanalysed as a numpy array. The image must\nbe loaded prior to model inputting, specifying a path\nis not valid.\n</code></pre> <p> TYPE: <code>ndarray</code> </p> <code>img_copy</code> <p>A copy of the input image.</p> <p> TYPE: <code>ndarray</code> </p> <code>h</code> <p>Integer variable containing the height of the input image (img).</p> <p> TYPE: <code>int</code> </p> <code>w</code> <p>Integer variable containing the width of the input image (img).</p> <p> TYPE: <code>int</code> </p> <code>calib_dist</code> <p>Integer variable containing the distance between the two specified point in pixel units. This value was either computed automatically or manually. Must be non-negative. If \"None\", the values are outputted in pixel units.</p> <p> TYPE: <code>int</code> </p> <code>spacing</code> <p>Integer variable containing the known distance in milimeter between the two placed points by the user or the scaling bars present in the image. This can be 5, 10, 15 or 20 milimeter. Must be non-negative and non-zero.</p> <p> TYPE: <code>(10, 5, 15, 20)</code> DEFAULT: <code>10</code> </p> <code>model_apo</code> <p>Contains keras model for prediction of aponeuroses</p> <p> </p> <code>model_fasc</code> <p>Contains keras model for prediction of fascicles</p> <p> </p> <code>dictionary</code> <p>Dictionary variable containing analysis parameters. These include must include apo_threshold, apo_length_tresh, fasc_threshold, fasc_cont_threshold, min_width, max_pennation, min_pennation.</p> <p> TYPE: <code>dict</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>image_callback</code> <p>Callback function to update the image display. If None, no callback is used.</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>fasc_l</code> <p>List variable contianing the estimated fascicle lengths based on the segmented fascicle fragments in pixel units as float. If calib_dist is specified, then the length is computed in centimeter.</p> <p> TYPE: <code>list</code> </p> <code>pennation</code> <p>List variable containing the estimated pennation angles based on the segmented fascicle fragments and aponeuroses as float.</p> <p> TYPE: <code>list</code> </p> <code>x_low1</code> <p>List variable containing the estimated x-coordinates of the lower edge from the upper aponeurosis as integers.</p> <p> TYPE: <code>list</code> </p> <code>x_high1</code> <p>List variable containing the estimated x-coordinates of the upper edge from the lower aponeurosis as integers.</p> <p> TYPE: <code>list</code> </p> <code>midthick</code> <p>Float variable containing the estimated distance between the lower and upper aponeurosis in pixel units. If calib_dist is specified, then the distance is computed in centimeter.</p> <p> TYPE: <code>float</code> </p> <code>fig</code> <p>Figure including the input image, the segmented aponeurosis and the extrapolated fascicles.</p> <p> TYPE: <code>figure</code> </p> Notes <p>For more detailed documentation, see the respective functions documentation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doCalculations(img=[[[[0.10113753 0.09391343 0.09030136]\n                       [0.10878626 0.10101581 0.09713058]\n                       [0.10878634 0.10101589 0.09713066]\n                       ...\n                       [0.         0.         0.        ]\n                       [0.         0.         0.        ]\n                       [0.         0.         0.        ]]]],\n                   img_copy=[[[[0.10113753 0.09391343 0.09030136]\n                       [0.10878626 0.10101581 0.09713058]\n                       [0.10878634 0.10101589 0.09713066]\n                       ...\n                       [0.         0.         0.        ]\n                       [0.         0.         0.        ]\n                       [0.         0.         0.        ]]]],\n                    h=512, w=512,calib_dist=None, spacing=10,\n                    filename=test1,\n                    apo_modelpath=\"C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-VGG16-fasc-BCE-512.h5\",\n                    fasc_modelpath=\"C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-apo-VGG-BCE-512.h5\",\n                    scale_statement=None,\n                    dictionary={'apo_treshold': '0.2', 'apo_length_tresh': '600', fasc_threshold': '0.05', 'fasc_cont_thresh': '40', 'min_width': '60', 'min_pennation': '10', 'max_pennation': '40'},\n                    filter_fasc = False)\n[1030.1118966321328, 1091.096002143386, ..., 1163.07073327008, 1080.0001937069776, 976.6099281240987]\n[19.400700671533016, 18.30126098122986, ..., 18.505345607096586, 18.727693601171197, 22.03704574228162]\n[441, 287, 656, 378, 125, 15, ..., -392, -45, -400, -149, -400]\n[1410, 1320, 1551, 1351, 1149, ..., 885, 937, 705, 869, 507]\n348.1328577\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations.filter_fascicles","title":"<code>filter_fascicles(df)</code>","text":"<p>Filters out fascicles that intersect with their neighboring fascicles based on their x_low and x_high values.</p> PARAMETER DESCRIPTION <code>df</code> <p>A DataFrame containing the fascicle data. Expected columns include 'x_low', 'y_low', 'x_high', and 'y_high'.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame with the fascicles that do not intersect with their neighbors.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = {'x_low': [1, 3, 5], 'y_low': [1, 2, 3], 'x_high': [4, 6, 7], 'y_high': [4, 5, 6]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; print(filter_fascicles(df))\n   x_low  y_low  x_high  y_high\n0      1      1       4       4\n2      5      3       7       6\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations.sortContours","title":"<code>sortContours(cnts)</code>","text":"<p>Function to sort detected contours from proximal to distal.</p> <p>The input contours belond to the aponeuroses and are sorted based on their coordinates, from smallest to largest. Moreover, for each detected contour a bounding box is built. The bounding boxes are sorted as well. They are however not needed for further analyses.</p> PARAMETER DESCRIPTION <code>cnts</code> <p>List of arrays containing the detected aponeurosis contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>cnts</code> <p>Tuple containing arrays of sorted contours.</p> <p> TYPE: <code>tuple</code> </p> <code>bounding_boxes</code> <p>Tuple containing tuples with sorted bounding boxes.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sortContours(cnts=[array([[[928, 247]], ... [[929, 247]]],\ndtype=int32),\n((array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[928, 247]], ... [[929, 247]]], dtype=int32)),\n((201, 97, 747, 29), (201, 247, 750, 96))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.adapted_contourEdge","title":"<code>adapted_contourEdge(edge, contour)</code>","text":"<p>Function to find only the coordinates representing one edge of a contour.</p> <p>Either the upper or lower edge of the detected contours is calculated. From the contour detected lower in the image, the upper edge is searched. From the contour detected higher in the image, the lower edge is searched. Allows for more points around the end of the contour than contourEdge.</p> PARAMETER DESCRIPTION <code>edge</code> <p>String variable defining the type of edge that is searched. The variable can be either \"T\" (top) or \"B\" (bottom).</p> <p> TYPE: <code>(T, B)</code> DEFAULT: <code>\"T\"</code> </p> <code>contour</code> <p>List variable containing sorted contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Array variable containing all x-coordinates from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array variable containing all y-coordinated from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; contourEdge(edge=\"T\", contour=[[[195 104]] ... [[196 104]]])\n[196 197 198 199 200 ... 952 953 954 955 956 957],\n[120 120 120 120 120 ... 125 125 125 125 125 125]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.adapted_filter_fascicles","title":"<code>adapted_filter_fascicles(df, tolerance)</code>","text":"<p>Filters out fascicles that intersect with other fascicles</p> <p>This function counts for each fascicle the number of intersections with other fascicles and ranks them based on the number of intersections. The fascicle with the highest intersection count is excluded. This ranking and exclusion process is repeated until there are no more intersections.</p> PARAMETER DESCRIPTION <code>df</code> <p>A DataFrame containing the fascicle data. Expected columns include 'coordsXY'.</p> <p> TYPE: <code>DataFrame</code> </p> <code>tolerance</code> <p>Tolerance to allow intersection points near the aponeuroses</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame with the fascicles that do not intersect with other fascicles.</p> Example <p>data = {'coordsXY': [[(78, 268), (78, 266), ...], [(43, 265), (42, 264), ...], ...]} tolerance = 100 adapted_filter_fascicles(data, tolerance)</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.contourEdge","title":"<code>contourEdge(edge, contour)</code>","text":"<p>Function to find only the coordinates representing one edge of a contour.</p> <p>Either the upper or lower edge of the detected contours is calculated. From the contour detected lower in the image, the upper edge is searched. From the contour detected higher in the image, the lower edge is searched.</p> PARAMETER DESCRIPTION <code>edge</code> <p>String variable defining the type of edge that is searched. The variable can be either \"T\" (top) or \"B\" (bottom).</p> <p> TYPE: <code>(T, B)</code> DEFAULT: <code>\"T\"</code> </p> <code>contour</code> <p>List variable containing sorted contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Array variable containing all x-coordinates from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array variable containing all y-coordinated from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; contourEdge(edge=\"T\", contour=[[[195 104]] ... [[196 104]]])\n[196 197 198 199 200 ... 952 953 954 955 956 957],\n[120 120 120 120 120 ... 125 125 125 125 125 125]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.crop","title":"<code>crop(original_image, image_fas, image_apo)</code>","text":"<p>Function to crop the frame around ultrasound images</p> <p>Additionally crops the fascicle and aponeuroses images in order that all three images have the same size</p> PARAMETER DESCRIPTION <code>original_image</code> <p>Image of the original ultrasound image</p> <p> TYPE: <code>list</code> </p> <code>image_fas</code> <p>Binary image of the fascicles within the original image</p> <p> TYPE: <code>list</code> </p> <code>image_apo</code> <p>Binary image of the aponeuroses within the original image</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>cropped_US</code> <p>Image of the original ultrasound image without frame around it</p> <p> TYPE: <code>list</code> </p> <code>cropped_fas</code> <p>Cropped binary image of fascicles within the original image</p> <p> TYPE: <code>list</code> </p> <code>cropped_apo</code> <p>Cropped binary image of the aponeuroses within the original image</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; crop(original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), image_fas = array([[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],...,[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0]], dtype=uint8), image_apo = array([[[0, 0, 0],[0, 0, 0],[0, 0, 0],...,[0, 0, 0],[0, 0, 0],[0, 0, 0]]], dtype=uint8))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.curve_connect","title":"<code>curve_connect(contours_sorted, ex_x_LA, ex_y_LA, ex_x_UA, ex_y_UA, original_image, parameters, filter_fasc, approach)</code>","text":"<p>Function to calculate the fascicle length and pennation angle accounted for curvature following linear connection between fascicles</p> <p>This function identifies individual fascicle contours and connects them if they are likely part of the same fascicle. A second-order polynomial curve is fitted through these contours; if the curvature exceeds a specified range, a linear fit is used instead. This fit is solely for detecting the contours. curve_connect_linear: The first contour of the fascicle is extrapolated to determine its intersection point with the lower aponeurosis. curve_connect_poly: The lower aponeurosis and the first contour are connected using a second-order polynomial fit based on all detected contours. Following common path for both approaches: After the initial extrapolation, the first contour is added. A linear connection is made from the last point of the current contour to the first point of the next contour, and this next contour is added. This process continues until the final contour is reached. The final contour is then used for a linear extrapolation to determine the intersection point with the upper aponeurosis. Adding all these parts together, the function calculates the fascicle length and pennation angle.</p> PARAMETER DESCRIPTION <code>contours_sorted</code> <p>List containing all (x,y)-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>ex_x_LA</code> <p>List containing all x-values of the extrapolated lower aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>ex_y_LA</code> <p>List containing all y-values of the extrapolated lower aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>ex_x_UA</code> <p>List containing all x-values of the extrapolated upper aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>ex_y_UA</code> <p>List containing all y-values of the extrapolated upper aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>original_image</code> <p>Ultrasound image to be analysed</p> <p> TYPE: <code>ndarray</code> </p> <code>parameters</code> <p>Dictionary variable containing analysis parameters. These include apo_length_threshold, apo_length_thresh, fasc_cont_thresh, min_width, max_pennation,min_pennation, tolerance, tolerance_to_apo, coeff_limit</p> <p> TYPE: <code>dict</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>approach</code> <p>Can either be curve_connect_linear or curve_connect_poly. If curve_connect_linear is used, a linear extrapolation between the lower aponeurosis and the first fascicle contour is used. If curve_connect_poly is used, a seconde order polynomial extrapolation between the lower and aponeurosis and the first fascicle contour is used; if the curvature exceeds a specified range, a linear fit is used instead.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>data</code> <p>Dictionary containing the fascicle length and pennation angle for each fascicle.</p> <p> TYPE: <code>dict</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; curve_connect(contours_sorted=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], dtype=int32), array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32), ...], ex_x_LA=[-256.0, -255.79515903180635, -255.59031806361273, -255.38547709541908, -255.18063612722545, -254.9757951590318, ...], ex_y_LA=[203.6459743268554, 203.64809836232556, 203.65022013210233, 203.6523396361857, ...], ex_x_UA=[-256.0, -255.79515903180635, -255.59031806361273, -255.38547709541908, -255.18063612722545, ...], ex_y_UA=[45.83649948451378, 45.829729965913046, 45.82296488688939, 45.81620424744281, 45.80944804757331, ...], original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), parameters={apo_length_thresh=600, fasc_cont_thresh=5, min_width=60, max_pennation=40,min_pennation=5, tolerance=10, tolerance_to_apo=100}, filter_fascicles=True, approach=\"curve_connect_linear\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.curve_polyfitting","title":"<code>curve_polyfitting(contours_sorted, ex_x_LA, ex_y_LA, ex_x_UA, ex_y_UA, original_image, parameters, filter_fasc)</code>","text":"<p>Function to calculate the fascicle length and pennation angle accounted for curvature following a second order polynomial fitting approach</p> <p>This function identifies individual fascicle contours and connects them if they are likely part of the same fascicle. A second-order polynomial curve is fitted through these contours; if the curvature exceeds a specified range, a linear fit is used instead. By knowing the positions of the aponeuroses, the intersection points between the fascicles and the lower and upper aponeuroses can be determined. Using these intersection points, the fascicle length and pennation angle are calculated.</p> Parameters <p>contours_sorted : list      List containing all (x,y)-coordinates of each detected contour  ex_x_LA : list      List containing all x-values of the extrapolated lower aponeurosis  ex_y_LA: list      List containing all y-values of the extrapolated lower aponeurosis  ex_x_UA : list      List containing all x-values of the extrapolated upper aponeurosis  ex_y_UA : list      List containing all y-values of the extrapolated upper aponeurosis  original_image : np.ndarray      Ultrasound image to be analysed  parameters : dict      Dictionary variable containing analysis parameters.      These include apo_length_threshold, apo_length_thresh, fasc_cont_thresh, min_width, max_pennation,min_pennation, tolerance, tolerance_to_apo, coeff_limit  filter_fasc : bool      If True, fascicles will be filtered so that no crossings are included.      This may reduce number of totally detected fascicles.</p> Returns <p>fascicle_length : list      List variable containing the estimated fascicle lengths      based on the segmented fascicle fragments in pixel units      as float.  pennation_angle : list      List variable containing the estimated pennation angles      based on the segmented fascicle fragments and aponeuroses      as float. x_low : list     List variable containing the intersection points between the fascicles and the lower aponeurosis x_high : list     List variable containing the intersection points between the fascicles and the upper aponeurosis  fig : matplot.figure      Figure including the input ultrasound image, the segmented aponeuroses and      the found fascicles extrapolated between the two aponeuroses.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; curve_polyfitting(contours_sorted=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], dtype=int32), array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32), ...], ex_x_LA=[-256.0, -255.79515903180635, -255.59031806361273, -255.38547709541908, -255.18063612722545, -254.9757951590318, ...], ex_y_LA=[203.6459743268554, 203.64809836232556, 203.65022013210233, 203.6523396361857, ...], ex_x_UA=[-256.0, -255.79515903180635, -255.59031806361273, -255.38547709541908, -255.18063612722545, ...], ex_y_UA=[45.83649948451378, 45.829729965913046, 45.82296488688939, 45.81620424744281, 45.80944804757331, ...], original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), parameters={apo_length_thresh=600, fasc_cont_thresh=5, min_width=60, max_pennation=40,min_pennation=5, tolerance=10, tolerance_to_apo=100}, filter_fascicles=True)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.doCalculations_curved","title":"<code>doCalculations_curved(original_image, img_copy, h, w, model_apo, model_fasc, dic, filter_fasc, calib_dist, spacing, approach, image_callback=None)</code>","text":"<p>Function to compute muscle architectural parameters accounted for fascicle curvature</p> <p>The aponeuroses edges are computed and the fascicle contours are connected to form complete fascicles. Based on three different approaches the fascicle length and pennation angle get calculated. Furthermore, it is possible to calculate an orientation map showing the slope at different points in the region of interest.</p> <p>Returns none when not more than one aponeurosis contour or no fascicle contours are detected in the image.</p> PARAMETER DESCRIPTION <code>original_image</code> <p>Ultrasound image to be analysed</p> <p> TYPE: <code>ndarray</code> </p> <code>img_copy</code> <p>A copy of the input image.</p> <p> TYPE: <code>ndarray</code> </p> <code>h</code> <p>Integer variable containing the height of the input image (original image).</p> <p> TYPE: <code>int</code> </p> <code>w</code> <p>Integer variable containing the width of the input image (original image).</p> <p> TYPE: <code>int</code> </p> <code>model_apo</code> <p>Contains keras model for prediction of aponeuroses</p> <p> </p> <code>model_fasc</code> <p>Contains keras model for prediction of fascicles</p> <p> </p> <code>dic</code> <p>Dictionary variable containing analysis parameters. These include apo_length_threshold, apo_length_thresh, fasc_cont_thresh, min_width, max_pennation,min_pennation, tolerance, tolerance_to_apo</p> <p> TYPE: <code>dict</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>calib_dist</code> <p>Integer variable containing the distance between the two specified point in pixel units. This value was either computed automatically or manually. Must be non-negative. If \"None\", the values are outputted in pixel units.</p> <p> TYPE: <code>int</code> </p> <code>spacing</code> <p>Integer variable containing the known distance in milimeter between the two placed points by the user or the scaling bars present in the image. This can be 5, 10, 15 or 20 millimeter. Must be non-negative and non-zero.</p> <p> TYPE: <code>(10, 5, 15, 20)</code> DEFAULT: <code>10</code> </p> <code>approach</code> <p>Can either be curve_polyfitting, curve_connect_linear, curve_connect_poly or orientation_map. curve_polyfitting calculates the fascicle length and pennation angle according to a second order polynomial fitting (see documentation of function curve_polyfitting). curve_connect_linear and curve_connect_poly calculate the fascicle length and pennation angle according to a linear connection between the fascicles fascicles (see documentation of function curve_connect). orientation_map calculates an orientation map and gives an estimate for the median angle of the image (see documentation of function orientation_map)</p> <p> TYPE: <code>str</code> </p> <code>image_callback</code> <p>Callback function for displaying the image. If None, no image is displayed.</p> <p> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>fascicle_length</code> <p>List variable containing the estimated fascicle lengths based on the segmented fascicle fragments in pixel units as float. If calib_dist is specified, then the length is computed in centimeter.</p> <p> TYPE: <code>list</code> </p> <code>pennation_angle</code> <p>List variable containing the estimated pennation angles based on the segmented fascicle fragments and aponeuroses as float.</p> <p> TYPE: <code>list</code> </p> <code>midthick</code> <p>Float variable containing the estimated distance between the lower and upper aponeurosis in pixel units. If calib_dist is specified, then the distance is computed in centimeter.</p> <p> TYPE: <code>float</code> </p> <code>x_low</code> <p>List variable containing the intersection points between the fascicles and the lower aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>x_high</code> <p>List variable containing the intersection points between the fascicles and the upper aponeurosis</p> <p> TYPE: <code>list</code> </p> <code>fig</code> <p>Figure including the input ultrasound image, the segmented aponeuroses and the found fascicles extrapolated between the two aponeuroses.</p> <p> TYPE: <code>figure</code> </p> Notes <p>For more detailed documentation, see the respective functions documentation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doCalculations_curved(original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), fas_image = array([[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],...,[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0]], dtype=uint8), apo_image = array([[[0, 0, 0],[0, 0, 0],[0, 0, 0],...,[0, 0, 0],[0, 0, 0],[0, 0, 0]]], dtype=uint8), dic={apo_length_thresh=600, fasc_cont_thresh=5, min_width=60, max_pennation=40,min_pennation=5, tolerance=10, tolerance_to_apo=100}, filter_fascicles=True, calib_dist = None, spacing = 10, approach = \"curve_polyfitting\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.do_curves_intersect","title":"<code>do_curves_intersect(curve1, curve2)</code>","text":"<p>Function to detect wheter two curves are intersecting or not.</p> PARAMETER DESCRIPTION <code>curve1</code> <p>List containing (x,y) coordinate pairs representing one curve</p> <p> TYPE: <code>list</code> </p> <code>curve2</code> <p>List containing (x,y) coordinate pairs representing a second curve</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Bool</code> <p>'True' if the curves have an intersection point 'False' if the curves don't have an intersection point</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; do_curves_intersect(curve1=[(98.06, 263.24), (98.26, 263.19), ...],\ncurve2=[(63.45, 258.82), (63.65, 258.76), ...])\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.find_complete_fascicle","title":"<code>find_complete_fascicle(i, contours_sorted_x, contours_sorted_y, contours_sorted, label, mid, width, tolerance, coeff_limit)</code>","text":"<p>Function to find complete fascicles based on connection of single contours.</p> <p>The function extrapolates a second order polynomial fit through the first contour. If the coefficients fall outside a specified range, the curve is considered too curved. As a result, a linear fit is calculated and used for subsequent calculations. The next contour is identified if its first point lies within a specified tolerance range in the positive and negative y-direction around the extrapolated fit. If this condition is met, both contours serve as the basis for the next polynomial fit. This process is repeated until no more possible connecting contours are found.</p> PARAMETER DESCRIPTION <code>i</code> <p>Integer value defining the starting contour</p> <p> TYPE: <code>int</code> </p> <code>contours_sorted_x</code> <p>List containing all x-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_y</code> <p>List containing all y-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted</code> <p>List containing all (x,y)-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>label</code> <p>Dictionnary containing a label true or false for every fascicle contour, true if already used for an extrapolation, false if not</p> <p> TYPE: <code>dictionnary</code> </p> <code>mid</code> <p>Integer value defining the middle of the image</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Integer value defining the width of the image</p> <p> TYPE: <code>int</code> </p> <code>tolerance</code> <p>Integer value specifing the permissible range in the positive and negative y-direction within which the next contour can be located to still be considered a part of the extrapolated fascicle</p> <p> TYPE: <code>int</code> </p> <code>coeff_limit</code> <p>Value defining the maximum value of the first coefficient in the second polynomial fit</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ex_current_fascicle_x</code> <p>List containing the x-coordinates of each found and extrapolated fascicle</p> <p> TYPE: <code>list</code> </p> <code>ex_current_fascicle_y</code> <p>List containing the y-coordinates of each found and extrapolated fascicle</p> <p> TYPE: <code>list</code> </p> <code>linear_fit</code> <p>'True' if extrapolated fit is linear 'False' if extrapolated fit follows a second order polynomial</p> <p> TYPE: <code>bool</code> </p> <code>inner_number_contours</code> <p>List containing the indices of each contour that constitute each fascicle</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_complete_fascicle(i=0, contours_sorted_x=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),...,array([481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497],dtype=int32)]), contours_sorted_y=[array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156],dtype=int32),...,array([76, 76, 76, 76, 75, 75, 75, 74, 74, 74, 74, 73, 73, 73, 72, 72, 72],dtype=int32)], contours_sorted=[[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32),...]], label={0: False, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False}, mid=256.0, width=512, tolerance=10, coeff_limit=0.000583)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.find_complete_fascicle_linear","title":"<code>find_complete_fascicle_linear(i, contours_sorted_x, contours_sorted_y, contours_sorted, label, mid, width, tolerance, coeff_limit)</code>","text":"<p>Traces a complete fascicle by iteratively fitting and extrapolating a linear model through connected contour segments in an ultrasound image.</p> <p>Starting from a seed contour <code>i</code>, the function fits a line, extrapolates it over a defined width, and collects all contour segments that lie within a vertical tolerance range around the extrapolated line. The procedure continues recursively to find connected fascicle segments along the extrapolated path.</p> PARAMETER DESCRIPTION <code>i</code> <p>Index of the initial contour used to start the fascicle tracing.</p> <p> TYPE: <code>int</code> </p> <code>contours_sorted_x</code> <p>List of arrays containing the x-coordinates of each contour segment.</p> <p> TYPE: <code>list of ndarray</code> </p> <code>contours_sorted_y</code> <p>List of arrays containing the y-coordinates of each contour segment.</p> <p> TYPE: <code>list of ndarray</code> </p> <code>contours_sorted</code> <p>List of full contour arrays (used for proximity checking).</p> <p> TYPE: <code>list of ndarray</code> </p> <code>label</code> <p>Dictionary mapping contour indices to a boolean indicating whether they have already been used. Will be updated in-place.</p> <p> TYPE: <code>dict</code> </p> <code>mid</code> <p>Midpoint x-coordinate used as center of extrapolation range.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Half-width of the extrapolation span in pixels (from <code>mid - width</code> to <code>mid + width</code>).</p> <p> TYPE: <code>int</code> </p> <code>tolerance</code> <p>Vertical distance in pixels allowed between the extrapolated line and candidate contour segments.</p> <p> TYPE: <code>int</code> </p> <code>coeff_limit</code> <p>Currently unused; placeholder for limiting slope or intercept values during fitting.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ex_current_fascicle_x</code> <p>Extrapolated x-coordinates of the complete fascicle.</p> <p> TYPE: <code>ndarray</code> </p> <code>ex_current_fascicle_y</code> <p>Corresponding y-coordinates from the final linear model fit.</p> <p> TYPE: <code>ndarray</code> </p> <code>linear_fit</code> <p>Flag indicating whether a linear fit was successfully computed (always <code>True</code> in current logic).</p> <p> TYPE: <code>bool</code> </p> <code>inner_number_contours</code> <p>List of contour indices that belong to the detected fascicle.</p> <p> TYPE: <code>list of int</code> </p> Notes <ul> <li>This function modifies the <code>label</code> dictionary in-place to prevent reprocessing contours.</li> <li>The input <code>coeff_limit</code> is accepted but currently not used for filtering.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ex_x, ex_y, is_linear, used_contours = find_complete_fascicle_linear(\n...     i=0,\n...     contours_sorted_x=contour_x_list,\n...     contours_sorted_y=contour_y_list,\n...     contours_sorted=contour_list,\n...     label={j: False for j in range(len(contour_x_list))},\n...     mid=256,\n...     width=100,\n...     tolerance=5,\n...     coeff_limit=0.5\n... )\n&gt;&gt;&gt; plt.plot(ex_x, ex_y)\n&gt;&gt;&gt; print(f\"Used {len(used_contours)} contour segments.\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.find_next_fascicle","title":"<code>find_next_fascicle(all_contours, contours_sorted_x, contours_sorted_y, x_current_fascicle, y_current_fascicle, x_range, upper_bound, lower_bound, label)</code>","text":"<p>Function to find the next fascicle contour</p> PARAMETER DESCRIPTION <code>all_contours</code> <p>List containing all (x,y)-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_x</code> <p>List containing all x-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_y</code> <p>List containing all y-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>x_current_fascicle</code> <p>List containing the x-coordinates of the currently examined fascicle</p> <p> TYPE: <code>list</code> </p> <code>y_current_fascicle</code> <p>List containing the y-coordinates of the currently examined fascicle</p> <p> TYPE: <code>list</code> </p> <code>x_range</code> <p>List containing all x-coordinates within the range of the extrapolation</p> <p> TYPE: <code>list</code> </p> <code>upper_bound</code> <p>List containing all y-coordinates of the lower boundary</p> <p> TYPE: <code>list</code> </p> <code>lower_bound</code> <p>List containing all y-coordinates of the upper boundary</p> <p> TYPE: <code>list</code> </p> <code>label</code> <p>Dictionnary containing a label true or false for every fascicle contour, true if already used for an extrapolation, false if not</p> <p> TYPE: <code>dictionnary</code> </p> RETURNS DESCRIPTION <code>new_x</code> <p>List containing the x-coordinates of the currently examined fascicle merged with the x-coordinates of the next fascicle contour within the boundary if one was found</p> <p> TYPE: <code>list</code> </p> <code>new_y</code> <p>List containing the y-coordinates of the currently examined fascicle merged with the y-coordinates of the next fascicle contour within the boundary if one was found</p> <p> TYPE: <code>list</code> </p> <code>found_fascicle</code> <p>Integer value of the found fascicle contour, -1 if no contour was found</p> <p> TYPE: <code>int</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_next_fascicle(all_contours = , contours_sorted_x=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),...,array([481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,494, 495, 496, 497],dtype=int32)]), contours_sorted_y=[array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32),...,array([76, 76, 76, 76, 75, 75, 75, 74, 74, 74, 74, 73, 73, 73, 72, 72, 72],dtype=int32)], x_current_fascicle=array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32), y_current_fascicle=array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32), x_range=array([-256.   , -255.795, -255.59 , ...,  767.59 ,  767.795,  768.   ]), upper_bound=array([243.137, 243.069, 243.001, ..., -97.372, -97.44 , -97.508]), lower_bound=array([263.137, 263.069, 263.001, ..., -77.372, -77.44 , -77.508]), label={0: True, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False})\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.is_point_in_range","title":"<code>is_point_in_range(x_point, y_point, x_poly, lb, ub)</code>","text":"<p>Function to detect wheter a point is between an upper and a lower boundary or not.</p> PARAMETER DESCRIPTION <code>x_point</code> <p>Single integer variable representing the x-coordinate of the point to be analyzed</p> <p> TYPE: <code>int</code> </p> <code>y_point</code> <p>Single integer variable representing the y-coordinate of the point to be analyzed</p> <p> TYPE: <code>integer</code> </p> <code>x_poly</code> <p>List containing all x-coordinates of the range where the point could be located</p> <p> TYPE: <code>list</code> </p> <code>lb</code> <p>List containing all y-coordinates of the lower boundary</p> <p> TYPE: <code>list</code> </p> <code>ub</code> <p>List containing all y-coordinates of the upper boundary</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Bool</code> <p>'True' if point is within both boundaries 'False' if point is outside the boundaries</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_point_in_range(x_point=250, y_point=227,\nx_poly=([-200, ..., 800]), lb=([303.165, ..., 130.044]),\nub=([323.165, ..., 150.044]))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.orientation_map","title":"<code>orientation_map(fas_image, apo_image, g, h)</code>","text":"<p>Function to calculate an orientation map based on the fascicle mask</p> <p>The function calculates the orientations of fascicles based on the fascicle mask using the OrientationPy package. It then uses linear inter- and extrapolation to determine the orientation of all points in the region between the two aponeuroses using the Rbf package. Finally, the resulting vectors are smoothed with a Gaussian filter. To approximate the median angle, the image is divided into six sections: two horizontally and three vertically. The median angle is then calculated for each of these sections. In the plot only the median for the part in the lower half and middle of the image is displayed.</p> PARAMETER DESCRIPTION <code>fas_image</code> <p>Mask of fascicles</p> <p> TYPE: <code>ndarray</code> </p> <code>apo_image</code> <p>Mask of aponeuroses</p> <p> TYPE: <code>ndarray</code> </p> <code>g</code> <p>Containing coefficients to calculate second order polynomial fit for upper aponeurosis</p> <p> TYPE: <code>poly1d</code> </p> <code>h</code> <p>Containing coefficients to calculate second order polynomial fit for lower aponeurosis</p> <p> TYPE: <code>poly1d</code> </p> RETURNS DESCRIPTION <code>split_angles_deg_median</code> <p>List variable containing the estimated pennation angles for the six parts of the image</p> <p> TYPE: <code>list</code> </p> <code>fig</code> <p>Figure showing the estimated slope at different points in the region between the two aponeuroses as a heat map</p> <p> TYPE: <code>figure</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; orientation_map(original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), fas_image = array([[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],...,[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0]], dtype=uint8), apo_image = array([[[0, 0, 0],[0, 0, 0],[0, 0, 0],...,[0, 0, 0],[0, 0, 0],[0, 0, 0]]], dtype=uint8), g=poly1d([ 0.   , -0.006, 40.841]), h=poly1d([ -0.   ,  -0.003, 204.533]))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_curved.sortContours","title":"<code>sortContours(cnts)</code>","text":"<p>Function to sort detected contours from proximal to distal.</p> <p>The input contours belond to the aponeuroses and are sorted based on their coordinates, from smallest to largest. Moreover, for each detected contour a bounding box is built. The bounding boxes are sorted as well. They are however not needed for further analyses.</p> PARAMETER DESCRIPTION <code>cnts</code> <p>List of arrays containing the detected aponeurosis contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>cnts</code> <p>Tuple containing arrays of sorted contours.</p> <p> TYPE: <code>tuple</code> </p> <code>bounding_boxes</code> <p>Tuple containing tuples with sorted bounding boxes.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sortContours(cnts=[array([[[928, 247]], ... [[929, 247]]],\ndtype=int32),\n((array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[928, 247]], ... [[929, 247]]], dtype=int32)),\n((201, 97, 747, 29), (201, 247, 750, 96))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_video.build_apo_from_edges","title":"<code>build_apo_from_edges(upp_x, upp_y, low_x, low_y, w, smooth_win=81, smooth_poly=2, tail_frac=0.5)</code>","text":"<p>Build dense, smoothed, and extrapolated aponeurosis curves from detected edges.</p> <p>This function constructs upper and lower aponeuroses on a shared dense x-grid spanning [-0.5*w, 1.5*w]. The detected aponeurosis edges are smoothed with a Savitzky\u2013Golay filter, interpolated within their detected domain, and extrapolated linearly on both left and right sides. Extrapolation uses a tangent estimated from a fraction of points at each end.</p> PARAMETER DESCRIPTION <code>upp_x</code> <p>X-coordinates of the detected upper aponeurosis edge.</p> <p> TYPE: <code>array-like of shape (N,)</code> </p> <code>upp_y</code> <p>Y-coordinates of the detected upper aponeurosis edge.</p> <p> TYPE: <code>array-like of shape (N,)</code> </p> <code>low_x</code> <p>X-coordinates of the detected lower aponeurosis edge.</p> <p> TYPE: <code>array-like of shape (M,)</code> </p> <code>low_y</code> <p>Y-coordinates of the detected lower aponeurosis edge.</p> <p> TYPE: <code>array-like of shape (M,)</code> </p> <code>w</code> <p>Image width in pixels. Used to set the dense extrapolation grid.</p> <p> TYPE: <code>int</code> </p> <code>smooth_win</code> <p>Window length for Savitzky\u2013Golay smoothing of detected edges. Must be odd and smaller than input length. Default is 81.</p> <p> TYPE: <code>int</code> DEFAULT: <code>81</code> </p> <code>smooth_poly</code> <p>Polynomial order for Savitzky\u2013Golay smoothing. Default is 2.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>tail_frac</code> <p>Fraction of points from each side (left and right) used to estimate tangent slope for extrapolation. Must be between 0 and 1. Default is 0.20 (20% of each side, minimum 5 points).</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>new_X</code> <p>Dense shared x-grid spanning [-0.5*w, 1.5*w].</p> <p> TYPE: <code>ndarray of shape (5000,)</code> </p> <code>new_Y_UA</code> <p>Smoothed + extrapolated y-values of the upper aponeurosis on <code>new_X</code>. Values outside valid regions may be NaN if extrapolation fails.</p> <p> TYPE: <code>ndarray of shape (5000,)</code> </p> <code>new_Y_LA</code> <p>Smoothed + extrapolated y-values of the lower aponeurosis on <code>new_X</code>. Values outside valid regions may be NaN if extrapolation fails.</p> <p> TYPE: <code>ndarray of shape (5000,)</code> </p> <code>segs_upper</code> <p>List of polyline segments (detected, left extrapolated, right extrapolated) for drawing the upper aponeurosis. Each segment contains integer (x,y).</p> <p> TYPE: <code>list of (K,2) ndarrays</code> </p> <code>segs_lower</code> <p>List of polyline segments (detected, left extrapolated, right extrapolated) for drawing the lower aponeurosis.</p> <p> TYPE: <code>list of (K,2) ndarrays</code> </p> Notes <ul> <li>Extrapolation uses linear regression (1<sup>st</sup>-order polynomial) on the   first/last <code>max(5, ceil(tail_frac * N))</code> points.</li> <li>Detected aponeurosis parts are smoothed with Savitzky\u2013Golay to reduce noise.</li> <li>This function ensures both aponeuroses are represented on the same dense grid.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Simulated aponeurosis edges\n&gt;&gt;&gt; upp_x = np.array([50, 100, 200, 300, 400])\n&gt;&gt;&gt; upp_y = np.array([100, 105, 110, 115, 120])\n&gt;&gt;&gt; low_x = np.array([50, 100, 200, 300, 400])\n&gt;&gt;&gt; low_y = np.array([300, 305, 310, 315, 320])\n&gt;&gt;&gt; new_X, new_Y_UA, new_Y_LA, segs_upper, segs_lower = build_apo_from_edges(\n...     upp_x, upp_y, low_x, low_y, w=512, tail_frac=0.4\n... )\n&gt;&gt;&gt; new_X.shape\n(5000,)\n&gt;&gt;&gt; np.isfinite(new_Y_UA).sum() &gt; 0\nTrue\n&gt;&gt;&gt; len(segs_upper), len(segs_lower)\n(3, 3)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_video.compute_muscle_thickness","title":"<code>compute_muscle_thickness(upp_x, upp_y, low_x, low_y)</code>","text":"<p>Estimate muscle thickness by computing the shortest distance between upper and lower aponeuroses in the central overlapping third of their x-values.</p> PARAMETER DESCRIPTION <code>upp_x</code> <p>X-coordinates of the upper aponeurosis contour.</p> <p> TYPE: <code>np.ndarray of shape (N,)</code> </p> <code>upp_y</code> <p>Y-coordinates of the upper aponeurosis contour.</p> <p> TYPE: <code>np.ndarray of shape (N,)</code> </p> <code>low_x</code> <p>X-coordinates of the lower aponeurosis contour.</p> <p> TYPE: <code>np.ndarray of shape (M,)</code> </p> <code>low_y</code> <p>Y-coordinates of the lower aponeurosis contour.</p> <p> TYPE: <code>np.ndarray of shape (M,)</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The minimum vertical distance (in pixels) between the upper and lower aponeuroses in the middle third of their overlapping x-range.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If there are fewer than 3 overlapping x-values between upper and lower aponeuroses.</p> Notes <p>This function assumes that both aponeuroses are already extracted as contours and that the x-values are integers or can be exactly matched.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; thickness = compute_muscle_thickness(\n...     upp_x=np.array([10, 20, 30, 40, 50]),\n...     upp_y=np.array([100, 95, 90, 85, 80]),\n...     low_x=np.array([20, 30, 40, 50, 60]),\n...     low_y=np.array([130, 125, 120, 115, 110])\n... )\n&gt;&gt;&gt; print(f\"Muscle thickness (px): {thickness:.2f}\")\nMuscle thickness (px): 30.00\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_video.doCalculationsVideo","title":"<code>doCalculationsVideo(vid_len, cap, vid_out, flip, apo_model, fasc_model, calib_dist, dic, step, filter_fasc, gui, segmentation_mode, frame_callback=None)</code>","text":"<p>Function to compute muscle architectural parameters based on convolutional neural network segmentation in videos.</p> <p>Firstly, images are segmented by the network. Then, predictions are thresholded and filtered. The aponeuroses edges are computed and the fascicle length and pennation angle calculated. This is done by extrapolating fascicle segments above a threshold length. Then the intersection between aponeurosis edge and fascicle structures are computed. Returns none when not more than one aponeurosis contour is detected in the image.</p> PARAMETER DESCRIPTION <code>vid_len</code> <p>Integer variable containing the number of frames present in cap.</p> <p> TYPE: <code>int</code> </p> <code>cap</code> <p>Object that contains the video in a np.ndarrray format. In this way, seperate frames can be accessed.</p> <p> TYPE: <code>VideoCapture</code> </p> <code>vid_out</code> <p>Object that is stored in the vpath folder. Contains the analyzed video frames and is titled \"..._proc.avi\" The name can be changed but must be different than the input video.</p> <p> TYPE: <code>VideoWriter</code> </p> <code>flip</code> <p>String variable defining wheter an image should be flipped. This can be \"no_flip\" (video is not flipped) or \"flip\" (video is flipped).</p> <p> TYPE: <code>(no_flip, flip)</code> DEFAULT: <code>\"no_flip\"</code> </p> <code>apo_model</code> <p>Aponeurosis neural network.</p> <p> </p> <code>fasc_model</code> <p>Fascicle neural network.</p> <p> </p> <code>calib_dist</code> <p>Integer variable containing the distance between the two specified point in pixel units. The points must be 10mm apart. Must be non-negative. If \"None\", the values are outputted in pixel units.</p> <p> TYPE: <code>int</code> </p> <code>dic</code> <p>Dictionary variable containing analysis parameters. These include must include apo_threshold, fasc_threshold, fasc_cont_threshold, min_width, max_pennation, min_pennation.</p> <p> TYPE: <code>dict</code> </p> <code>step</code> <p>Integer variable containing the step for the range of video frames. If step != 1, frames are skipped according to the size of step. This might decrease processing time but also accuracy.</p> <p> TYPE: <code>int</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the calculation process after each image.</p> <p> TYPE: <code>TK</code> </p> <code>segmentation_mode</code> <p>String variable containing the segmentation mode. This is used to determine the segmentation model used. Choose between \"stacked\" and and \"None\". When \"stacked\" is chosen, the frames are loaded in stacks of three.</p> <p> TYPE: <code>str</code> </p> <code>frame_callback</code> <p>Boolean variable determining whether the current frame is displayed in main UI.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>fasc_l_all</code> <p>List of arrays contianing the estimated fascicle lengths based on the segmented fascicle fragments in pixel units as float. If calib_dist is specified, then the length is computed in centimeter. This is computed for each frame in the video.</p> <p> TYPE: <code>list</code> </p> <code>pennation_all</code> <p>List of lists containing the estimated pennation angles based on the segmented fascicle fragments and aponeuroses as float. This is computed for each frame in the video.</p> <p> TYPE: <code>list</code> </p> <code>x_lows_all</code> <p>List of lists containing the estimated x-coordinates of the lower edge from the upper aponeurosis as integers. This is computed for each frame in the video.</p> <p> TYPE: <code>list</code> </p> <code>x_highs_all</code> <p>List of lists containing the estimated x-coordinates of the upper edge from the lower aponeurosis as integers. This is computed for each frame in the video.</p> <p> TYPE: <code>list</code> </p> <code>midthick_all</code> <p>List variable containing the estimated distance between the lower and upper aponeurosis in pixel units. If calib_dist is specified, then the distance is computed in centimeter. This is computed for each frame in the video.</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; doCalculations(vid_len=933, cap=&lt; cv2.VideoCapture 000002BFAD0560B0&gt;,\n                    vid_out=&lt; cv2.VideoWriter 000002BFACEC0130&gt;,\n                    flip=\"no_flip\",\n                    apo_modelpath=\"C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-VGG16-fasc-BCE-512.h5\",\n                    fasc_modelpath=\"C:/Users/admin/Documents/DL_Track/Models_DL_Track/Final_models/model-apo-VGG-BCE-512.h5\",\n                    calib_dist=98,\n                    dic={'apo_treshold': '0.2', 'fasc_threshold': '0.05',\n                    'fasc_cont_thresh': '40', 'min_width': '60',\n                    'min_pennation': '10', 'max_pennation': '40'},\n                    filter_fasc = False,\n                    gui=&lt;__main__.DL_Track_US object at 0x000002BFA7528190&gt;,\n                    segmentation_mode=None,\n                    display_frame=True)\n[array([60.5451731 , 58.86892027, 64.16011534, 55.46192704, 63.40711356]), ..., array([64.90849385, 60.31621836])]\n[[19.124207107383114, 19.409753216521565, 18.05706763600641, 20.54453899050867, 17.808652286488794], ..., [17.26241882195032, 16.284803480359543]]\n[[148, 5, 111, 28, -164], [356, 15, 105, -296], [357, 44, -254], [182, 41, -233], [40, 167, 42, -170], [369, 145, 57, -139], [376, 431, 32], [350, 0]]\n[[725, 568, 725, 556, 444], [926, 572, 516, 508], [971, 565, 502], [739, 578, 474], [554, 766, 603, 475], [1049, 755, 567, 430], [954, 934, 568], [968, 574]]\n[23.484416057267826, 22.465452189555794, 21.646971767045816, 21.602856412413924, 21.501286239714894, 21.331137350026623, 21.02446763240188, 21.250352548097883]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.do_calculations_video.optimize_fascicle_loop","title":"<code>optimize_fascicle_loop(contoursF3, new_Y_UA, new_Y_LA, new_X_UA, new_X_LA, width, min_pennation, max_pennation, filter_fascicles_func, fasc_cont_thresh, calib_dist)</code>","text":"<p>Extract, extrapolate, and filter fascicle contours based on angle and length criteria.</p> <p>This function fits a line to each detected fascicle contour, extrapolates it  across the image width, and finds its intersections with the extrapolated upper  and lower aponeuroses. Fascicles that intersect both aponeuroses, have lengths  above the threshold, and fall within the allowed pennation angle range are kept. Optionally, overlapping fascicles can be filtered.</p> PARAMETER DESCRIPTION <code>contoursF3</code> <p>List of fascicle contours detected in the binary fascicle mask.</p> <p> TYPE: <code>list of (N, 1, 2) ndarray</code> </p> <code>new_Y_UA</code> <p>Y-values of the extrapolated upper aponeurosis curve.</p> <p> TYPE: <code>ndarray of shape (M,)</code> </p> <code>new_Y_LA</code> <p>Y-values of the extrapolated lower aponeurosis curve.</p> <p> TYPE: <code>ndarray of shape (M,)</code> </p> <code>new_X_UA</code> <p>X-values corresponding to <code>new_Y_UA</code>.</p> <p> TYPE: <code>ndarray of shape (M,)</code> </p> <code>new_X_LA</code> <p>X-values corresponding to <code>new_Y_LA</code>.</p> <p> TYPE: <code>ndarray of shape (M,)</code> </p> <code>width</code> <p>Image width in pixels. Used to define extrapolation grid.</p> <p> TYPE: <code>int</code> </p> <code>min_pennation</code> <p>Minimum acceptable pennation angle in degrees.</p> <p> TYPE: <code>float</code> </p> <code>max_pennation</code> <p>Maximum acceptable pennation angle in degrees.</p> <p> TYPE: <code>float</code> </p> <code>filter_fascicles_func</code> <p>Optional function to further filter valid fascicles. Must accept and return a pandas.DataFrame with fascicle data.</p> <p> TYPE: <code>callable or None</code> </p> <code>fasc_cont_thresh</code> <p>Minimum number of contour points required to consider a fascicle candidate.</p> <p> TYPE: <code>int</code> </p> <code>calib_dist</code> <p>Calibration distance in pixels between two 10 mm markers.  If provided, fascicle lengths are scaled to millimeters.</p> <p> TYPE: <code>float or int</code> </p> RETURNS DESCRIPTION <code>fascicle_data</code> <p>DataFrame with one row per accepted fascicle and columns:</p> <ul> <li><code>x_low</code> : int   Starting x-coordinate (lower intersection with aponeurosis).</li> <li><code>x_high</code> : int   Ending x-coordinate (upper intersection with aponeurosis).</li> <li><code>y_low</code> : int   Starting y-coordinate (lower intersection).</li> <li><code>y_high</code> : int   Ending y-coordinate (upper intersection).</li> <li><code>coordsX</code> : ndarray   X coordinates of fascicle line between intersections.</li> <li><code>coordsY</code> : ndarray   Y coordinates of fascicle line between intersections.</li> <li><code>fasc_l</code> : float   Fascicle length (px or mm if <code>calib_dist</code> is provided).</li> <li><code>penn_a</code> : float   Pennation angle in degrees relative to lower aponeurosis.</li> </ul> <p>If no valid fascicles are found, an empty DataFrame with these columns is returned.</p> <p> TYPE: <code>DataFrame</code> </p> Notes <ul> <li>Fascicles are fitted with a first-order polynomial (straight line).</li> <li>Intersections are determined by minimizing vertical distance to aponeuroses.</li> <li>Angles are computed relative to the local slope of the lower aponeurosis.</li> <li><code>filter_fascicles_func</code> can be used to remove overlapping or outlier fascicles.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Dummy apo (straight lines) and fascicle contour\n&gt;&gt;&gt; new_X = np.linspace(0, 512, 5000)\n&gt;&gt;&gt; new_Y_UA = 100 + 0*new_X\n&gt;&gt;&gt; new_Y_LA = 400 + 0*new_X\n&gt;&gt;&gt; cnt = np.array([[[100, 120]], [[200, 250]], [[300, 380]]])  # fake fascicle\n&gt;&gt;&gt; df = optimize_fascicle_loop(\n...     contoursF3=[cnt],\n...     new_Y_UA=new_Y_UA,\n...     new_Y_LA=new_Y_LA,\n...     new_X_UA=new_X,\n...     new_X_LA=new_X,\n...     width=512,\n...     min_pennation=10,\n...     max_pennation=40,\n...     filter_fascicles_func=None,\n...     fasc_cont_thresh=3,\n...     calib_dist=98\n... )\n&gt;&gt;&gt; df[[\"fasc_l\", \"penn_a\"]]\n    fasc_l     penn_a\n0  39.7951  18.7423\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.adapted_contourEdge","title":"<code>adapted_contourEdge(edge, contour)</code>","text":"<p>Function to find only the coordinates representing one edge of a contour.</p> <p>Either the upper or lower edge of the detected contours is calculated. From the contour detected lower in the image, the upper edge is searched. From the contour detected higher in the image, the lower edge is searched. Allows for more points around the end of the contour than contourEdge.</p> PARAMETER DESCRIPTION <code>edge</code> <p>String variable defining the type of edge that is searched. The variable can be either \"T\" (top) or \"B\" (bottom).</p> <p> TYPE: <code>(T, B)</code> DEFAULT: <code>\"T\"</code> </p> <code>contour</code> <p>List variable containing sorted contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Array variable containing all x-coordinates from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array variable containing all y-coordinated from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; contourEdge(edge=\"T\", contour=[[[195 104]] ... [[196 104]]])\n[196 197 198 199 200 ... 952 953 954 955 956 957],\n[120 120 120 120 120 ... 125 125 125 125 125 125]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.adapted_filter_fascicles","title":"<code>adapted_filter_fascicles(df, tolerance)</code>","text":"<p>Filters out fascicles that intersect with other fascicles</p> <p>This function counts for each fascicle the number of intersections with other fascicles and ranks them based on the number of intersections. The fascicle with the highest intersection count is excluded. This ranking and exclusion process is repeated until there are no more intersections.</p> PARAMETER DESCRIPTION <code>df</code> <p>A DataFrame containing the fascicle data. Expected columns include 'coordsXY'.</p> <p> TYPE: <code>DataFrame</code> </p> <code>tolerance</code> <p>Tolerance to allow intersection points near the aponeuroses</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame with the fascicles that do not intersect with other fascicles.</p> Example <p>data = {'coordsXY': [[(78, 268), (78, 266), ...], [(43, 265), (42, 264), ...], ...]} tolerance = 100 adapted_filter_fascicles(data, tolerance)</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.contourEdge","title":"<code>contourEdge(edge, contour)</code>","text":"<p>Function to find only the coordinates representing one edge of a contour.</p> <p>Either the upper or lower edge of the detected contours is calculated. From the contour detected lower in the image, the upper edge is searched. From the contour detected higher in the image, the lower edge is searched.</p> PARAMETER DESCRIPTION <code>edge</code> <p>String variable defining the type of edge that is searched. The variable can be either \"T\" (top) or \"B\" (bottom).</p> <p> TYPE: <code>(T, B)</code> DEFAULT: <code>\"T\"</code> </p> <code>contour</code> <p>List variable containing sorted contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Array variable containing all x-coordinates from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Array variable containing all y-coordinated from the detected contour.</p> <p> TYPE: <code>ndarray</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; contourEdge(edge=\"T\", contour=[[[195 104]] ... [[196 104]]])\n[196 197 198 199 200 ... 952 953 954 955 956 957],\n[120 120 120 120 120 ... 125 125 125 125 125 125]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.crop","title":"<code>crop(original_image, image_fas, image_apo)</code>","text":"<p>Function to crop the frame around ultrasound images</p> <p>Additionally crops the fascicle and aponeuroses images in order that all three images have the same size</p> PARAMETER DESCRIPTION <code>original_image</code> <p>Image of the original ultrasound image</p> <p> TYPE: <code>list</code> </p> <code>image_fas</code> <p>Binary image of the fascicles within the original image</p> <p> TYPE: <code>list</code> </p> <code>image_apo</code> <p>Binary image of the aponeuroses within the original image</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>cropped_US</code> <p>Image of the original ultrasound image without frame around it</p> <p> TYPE: <code>list</code> </p> <code>cropped_fas</code> <p>Cropped binary image of fascicles within the original image</p> <p> TYPE: <code>list</code> </p> <code>cropped_apo</code> <p>Cropped binary image of the aponeuroses within the original image</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; crop(original_image=array([[[160, 160, 160],[159, 159, 159],[158, 158, 158],...[158, 158, 158],[147, 147, 147],[  1,   1,   1]],...,[[  0,   0,   0],[  0,   0,   0],[  0,   0,   0],...,[  4,   4,   4],[  3,   3,   3],[  3,   3,   3]]], dtype=uint8), image_fas = array([[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],...,[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0],[0, 0, 0, ..., 0, 0, 0]], dtype=uint8), image_apo = array([[[0, 0, 0],[0, 0, 0],[0, 0, 0],...,[0, 0, 0],[0, 0, 0],[0, 0, 0]]], dtype=uint8))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.do_curves_intersect","title":"<code>do_curves_intersect(curve1, curve2)</code>","text":"<p>Function to detect wheter two curves are intersecting or not.</p> PARAMETER DESCRIPTION <code>curve1</code> <p>List containing (x,y) coordinate pairs representing one curve</p> <p> TYPE: <code>list</code> </p> <code>curve2</code> <p>List containing (x,y) coordinate pairs representing a second curve</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Bool</code> <p>'True' if the curves have an intersection point 'False' if the curves don't have an intersection point</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; do_curves_intersect(curve1=[(98.06, 263.24), (98.26, 263.19), ...],\ncurve2=[(63.45, 258.82), (63.65, 258.76), ...])\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.find_complete_fascicle","title":"<code>find_complete_fascicle(i, contours_sorted_x, contours_sorted_y, contours_sorted, label, mid, width, tolerance, coeff_limit)</code>","text":"<p>Function to find complete fascicles based on connection of single contours.</p> <p>The function extrapolates a second order polynomial fit through the first contour. If the coefficients fall outside a specified range, the curve is considered too curved. As a result, a linear fit is calculated and used for subsequent calculations. The next contour is identified if its first point lies within a specified tolerance range in the positive and negative y-direction around the extrapolated fit. If this condition is met, both contours serve as the basis for the next polynomial fit. This process is repeated until no more possible connecting contours are found.</p> PARAMETER DESCRIPTION <code>i</code> <p>Integer value defining the starting contour</p> <p> TYPE: <code>int</code> </p> <code>contours_sorted_x</code> <p>List containing all x-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_y</code> <p>List containing all y-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted</code> <p>List containing all (x,y)-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>label</code> <p>Dictionnary containing a label true or false for every fascicle contour, true if already used for an extrapolation, false if not</p> <p> TYPE: <code>dictionnary</code> </p> <code>mid</code> <p>Integer value defining the middle of the image</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Integer value defining the width of the image</p> <p> TYPE: <code>int</code> </p> <code>tolerance</code> <p>Integer value specifing the permissible range in the positive and negative y-direction within which the next contour can be located to still be considered a part of the extrapolated fascicle</p> <p> TYPE: <code>int</code> </p> <code>coeff_limit</code> <p>Value defining the maximum value of the first coefficient in the second polynomial fit</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ex_current_fascicle_x</code> <p>List containing the x-coordinates of each found and extrapolated fascicle</p> <p> TYPE: <code>list</code> </p> <code>ex_current_fascicle_y</code> <p>List containing the y-coordinates of each found and extrapolated fascicle</p> <p> TYPE: <code>list</code> </p> <code>linear_fit</code> <p>'True' if extrapolated fit is linear 'False' if extrapolated fit follows a second order polynomial</p> <p> TYPE: <code>bool</code> </p> <code>inner_number_contours</code> <p>List containing the indices of each contour that constitute each fascicle</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_complete_fascicle(i=0, contours_sorted_x=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),...,array([481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497],dtype=int32)]), contours_sorted_y=[array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156],dtype=int32),...,array([76, 76, 76, 76, 75, 75, 75, 74, 74, 74, 74, 73, 73, 73, 72, 72, 72],dtype=int32)], contours_sorted=[[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32),...]], label={0: False, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False}, mid=256.0, width=512, tolerance=10, coeff_limit=0.000583)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.find_complete_fascicle_linear","title":"<code>find_complete_fascicle_linear(i, contours_sorted_x, contours_sorted_y, contours_sorted, label, mid, width, tolerance, coeff_limit)</code>","text":"<p>Traces a complete fascicle by iteratively fitting and extrapolating a linear model through connected contour segments in an ultrasound image.</p> <p>Starting from a seed contour <code>i</code>, the function fits a line, extrapolates it over a defined width, and collects all contour segments that lie within a vertical tolerance range around the extrapolated line. The procedure continues recursively to find connected fascicle segments along the extrapolated path.</p> PARAMETER DESCRIPTION <code>i</code> <p>Index of the initial contour used to start the fascicle tracing.</p> <p> TYPE: <code>int</code> </p> <code>contours_sorted_x</code> <p>List of arrays containing the x-coordinates of each contour segment.</p> <p> TYPE: <code>list of ndarray</code> </p> <code>contours_sorted_y</code> <p>List of arrays containing the y-coordinates of each contour segment.</p> <p> TYPE: <code>list of ndarray</code> </p> <code>contours_sorted</code> <p>List of full contour arrays (used for proximity checking).</p> <p> TYPE: <code>list of ndarray</code> </p> <code>label</code> <p>Dictionary mapping contour indices to a boolean indicating whether they have already been used. Will be updated in-place.</p> <p> TYPE: <code>dict</code> </p> <code>mid</code> <p>Midpoint x-coordinate used as center of extrapolation range.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Half-width of the extrapolation span in pixels (from <code>mid - width</code> to <code>mid + width</code>).</p> <p> TYPE: <code>int</code> </p> <code>tolerance</code> <p>Vertical distance in pixels allowed between the extrapolated line and candidate contour segments.</p> <p> TYPE: <code>int</code> </p> <code>coeff_limit</code> <p>Currently unused; placeholder for limiting slope or intercept values during fitting.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>ex_current_fascicle_x</code> <p>Extrapolated x-coordinates of the complete fascicle.</p> <p> TYPE: <code>ndarray</code> </p> <code>ex_current_fascicle_y</code> <p>Corresponding y-coordinates from the final linear model fit.</p> <p> TYPE: <code>ndarray</code> </p> <code>linear_fit</code> <p>Flag indicating whether a linear fit was successfully computed (always <code>True</code> in current logic).</p> <p> TYPE: <code>bool</code> </p> <code>inner_number_contours</code> <p>List of contour indices that belong to the detected fascicle.</p> <p> TYPE: <code>list of int</code> </p> Notes <ul> <li>This function modifies the <code>label</code> dictionary in-place to prevent reprocessing contours.</li> <li>The input <code>coeff_limit</code> is accepted but currently not used for filtering.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ex_x, ex_y, is_linear, used_contours = find_complete_fascicle_linear(\n...     i=0,\n...     contours_sorted_x=contour_x_list,\n...     contours_sorted_y=contour_y_list,\n...     contours_sorted=contour_list,\n...     label={j: False for j in range(len(contour_x_list))},\n...     mid=256,\n...     width=100,\n...     tolerance=5,\n...     coeff_limit=0.5\n... )\n&gt;&gt;&gt; plt.plot(ex_x, ex_y)\n&gt;&gt;&gt; print(f\"Used {len(used_contours)} contour segments.\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.find_next_fascicle","title":"<code>find_next_fascicle(all_contours, contours_sorted_x, contours_sorted_y, x_current_fascicle, y_current_fascicle, x_range, upper_bound, lower_bound, label)</code>","text":"<p>Function to find the next fascicle contour</p> PARAMETER DESCRIPTION <code>all_contours</code> <p>List containing all (x,y)-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_x</code> <p>List containing all x-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>contours_sorted_y</code> <p>List containing all y-coordinates of each detected contour</p> <p> TYPE: <code>list</code> </p> <code>x_current_fascicle</code> <p>List containing the x-coordinates of the currently examined fascicle</p> <p> TYPE: <code>list</code> </p> <code>y_current_fascicle</code> <p>List containing the y-coordinates of the currently examined fascicle</p> <p> TYPE: <code>list</code> </p> <code>x_range</code> <p>List containing all x-coordinates within the range of the extrapolation</p> <p> TYPE: <code>list</code> </p> <code>upper_bound</code> <p>List containing all y-coordinates of the lower boundary</p> <p> TYPE: <code>list</code> </p> <code>lower_bound</code> <p>List containing all y-coordinates of the upper boundary</p> <p> TYPE: <code>list</code> </p> <code>label</code> <p>Dictionnary containing a label true or false for every fascicle contour, true if already used for an extrapolation, false if not</p> <p> TYPE: <code>dictionnary</code> </p> RETURNS DESCRIPTION <code>new_x</code> <p>List containing the x-coordinates of the currently examined fascicle merged with the x-coordinates of the next fascicle contour within the boundary if one was found</p> <p> TYPE: <code>list</code> </p> <code>new_y</code> <p>List containing the y-coordinates of the currently examined fascicle merged with the y-coordinates of the next fascicle contour within the boundary if one was found</p> <p> TYPE: <code>list</code> </p> <code>found_fascicle</code> <p>Integer value of the found fascicle contour, -1 if no contour was found</p> <p> TYPE: <code>int</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_next_fascicle(all_contours = , contours_sorted_x=[array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32),...,array([481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,494, 495, 496, 497],dtype=int32)]), contours_sorted_y=[array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32),...,array([76, 76, 76, 76, 75, 75, 75, 74, 74, 74, 74, 73, 73, 73, 72, 72, 72],dtype=int32)], x_current_fascicle=array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],dtype=int32), y_current_fascicle=array([166, 166, 166, 165, 165, 165, 164, 164, 164, 163, 163, 163, 162, 162, 162, 161, 161, 161, 160, 160, 160, 159, 159, 159, 158, 158, 158, 157, 157, 157, 156, 156], dtype=int32), x_range=array([-256.   , -255.795, -255.59 , ...,  767.59 ,  767.795,  768.   ]), upper_bound=array([243.137, 243.069, 243.001, ..., -97.372, -97.44 , -97.508]), lower_bound=array([263.137, 263.069, 263.001, ..., -77.372, -77.44 , -77.508]), label={0: True, 1: False, 2: False, 3: False, 4: False, 5: False, 6: False, 7: False, 8: False, 9: False, 10: False, 11: False, 12: False, 13: False})\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.is_point_in_range","title":"<code>is_point_in_range(x_point, y_point, x_poly, lb, ub)</code>","text":"<p>Function to detect wheter a point is between an upper and a lower boundary or not.</p> PARAMETER DESCRIPTION <code>x_point</code> <p>Single integer variable representing the x-coordinate of the point to be analyzed</p> <p> TYPE: <code>int</code> </p> <code>y_point</code> <p>Single integer variable representing the y-coordinate of the point to be analyzed</p> <p> TYPE: <code>integer</code> </p> <code>x_poly</code> <p>List containing all x-coordinates of the range where the point could be located</p> <p> TYPE: <code>list</code> </p> <code>lb</code> <p>List containing all y-coordinates of the lower boundary</p> <p> TYPE: <code>list</code> </p> <code>ub</code> <p>List containing all y-coordinates of the upper boundary</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>Bool</code> <p>'True' if point is within both boundaries 'False' if point is outside the boundaries</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_point_in_range(x_point=250, y_point=227,\nx_poly=([-200, ..., 800]), lb=([303.165, ..., 130.044]),\nub=([323.165, ..., 150.044]))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.curved_fascicles_functions.sortContours","title":"<code>sortContours(cnts)</code>","text":"<p>Function to sort detected contours from proximal to distal.</p> <p>The input contours belond to the aponeuroses and are sorted based on their coordinates, from smallest to largest. Moreover, for each detected contour a bounding box is built. The bounding boxes are sorted as well. They are however not needed for further analyses.</p> PARAMETER DESCRIPTION <code>cnts</code> <p>List of arrays containing the detected aponeurosis contours.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>cnts</code> <p>Tuple containing arrays of sorted contours.</p> <p> TYPE: <code>tuple</code> </p> <code>bounding_boxes</code> <p>Tuple containing tuples with sorted bounding boxes.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sortContours(cnts=[array([[[928, 247]], ... [[929, 247]]],\ndtype=int32),\n((array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[228,  97]], ... [[229,  97]]], dtype=int32),\n(array([[[928, 247]], ... [[929, 247]]], dtype=int32)),\n((201, 97, 747, 29), (201, 247, 750, 96))\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.filter_data.applyFilters","title":"<code>applyFilters(data, filter_type='median', **kwargs)</code>","text":"<p>Applies a selected filter to the input data.</p> PARAMETER DESCRIPTION <code>data</code> <p>The input data to be filtered.</p> <p> TYPE: <code>list of lists</code> </p> <code>filter_type</code> <p>The type of filter to apply. Options are \"median\", \"gaussian\", \"savitzky_golay\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'median'</code> </p> <code>kwargs</code> <p>Additional parameters for the selected filter.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>list of lists</code> <p>The filtered data.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.filter_data.apply_rate_limit","title":"<code>apply_rate_limit(pa_values, max_jump=0.5)</code>","text":"<p>Apply hard rate limiting to a sequence of pennation angles (deg).</p> PARAMETER DESCRIPTION <code>pa_values</code> <p>List/array of per-frame pennation angles (floats, may contain NaN).</p> <p> TYPE: <code>array - like</code> </p> <code>max_jump</code> <p>Maximum allowed change per frame (deg). Default is 4.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Array of rate-limited pennation angles, same length as input.</p> Notes <ul> <li>NaNs are carried forward as the last valid value.</li> <li>The first frame is kept as-is (if valid).</li> <li>Each subsequent frame changes by at most <code>max_jump</code> compared to the previous output.</li> </ul>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.filter_data.hampelFilterList","title":"<code>hampelFilterList(data, win_size=5, num_dev=1, center_win=True)</code>","text":"<p>Applies a Hampel filter to a list of numerical values.</p> <p>This function detects and replaces outliers with the rolling median within a specified window size.</p> PARAMETER DESCRIPTION <code>data</code> <p>List of numerical values.</p> <p> TYPE: <code>list</code> </p> <code>win_size</code> <p>Window size for rolling median filtering. Default is 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>num_dev</code> <p>Number of standard deviations for outlier detection. Default is 1.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>center_win</code> <p>Whether the window is centered on the point. Default is True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict</code> <p>A dictionary with: - \"filtered\" : list with filtered values. - \"outliers\" : list where outliers are replaced with NaN. - \"is_outlier\" : list of boolean values indicating outliers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [10, 12, 100, 14, 13, 15, 300, 18, 16]\n&gt;&gt;&gt; result = hampel_filter_list(data, win_size=3, num_dev=2)\n&gt;&gt;&gt; print(result[\"filtered\"])\n[10, 12, 13, 14, 13, 15, 16, 18, 16]\n&gt;&gt;&gt; print(result[\"is_outlier\"])\n[False, False, True, False, False, False, True, False, False]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.calculateBatch","title":"<code>calculateBatch(rootpath, apo_modelpath, fasc_modelpath, flip_file_path, file_type, scaling, spacing, filter_fasc, settings, gui, image_frame=None)</code>","text":"<p>Function to calculate muscle architecture in longitudinal ultrasonography images of human lower limb muscles. The values computed are fascicle length (FL), pennation angle (PA), and muscle thickness (MT).</p> <p>The scope of this function is limited. Images of the vastus lateralis, tibialis anterior soleus and gastrocnemius  muscles can be analyzed. This is due to the limited amount of training data for our convolutional neural networks. This functions makes extensive use of several other functions and was designed to be executed from a GUI.</p> PARAMETER DESCRIPTION <code>rootpath</code> <p>String variable containing the path to the folder where all images to be analyzed are saved.</p> <p> TYPE: <code>str</code> </p> <code>apo_modelpath</code> <p>String variable containing the absolute path to the aponeurosis neural network.</p> <p> TYPE: <code>str</code> </p> <code>fasc_modelpath</code> <p>String variable containing the absolute path to the fascicle neural network.</p> <p> TYPE: <code>str</code> </p> <code>flip_file_path</code> <p>String variabel containing the absolute path to the flip flag .txt file containing the flip flags. Flipping is necessary as the models were trained on images of with specific fascicle orientation.</p> <p> TYPE: <code>str</code> </p> <code>file_type</code> <p>String variable containg the respective type of the images. This is needed to select only the relevant image files in the root directory.</p> <p> TYPE: <code>str</code> </p> <code>scaling</code> <p>String variabel determining the image scaling method. There are three types of scaling available: - scaling = \"manual\" (user must scale images manually) - sclaing = \"bar\" (image are scaled automatically. This is done by   detecting scaling bars on the right side of the image.) - scaling = \"No scaling\" (image is not scaled.) Scaling is necessary to compute measurements in centimeter, if \"no scaling\" is chosen, the results are in pixel units.</p> <p> TYPE: <code>(bar, manual, 'No scaling')</code> DEFAULT: <code>\"bar\"</code> </p> <code>spacing</code> <p>Distance (in milimeter) between two scaling bars in the image. This is needed to compute the pixel/cm ratio and therefore report the results in centimeter rather than pixel units.</p> <p> TYPE: <code>(10, 5, 15, 20)</code> DEFAULT: <code>10</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>settings</code> <p>Dictionary containing the analysis settings of the GUI.</p> <p> TYPE: <code>dict</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the calculation process after each image.</p> <p> TYPE: <code>TK</code> </p> See Also <p>do_calculations.py for exact description of muscle architecture parameter calculation.</p> Notes <p>For specific explanations of the included functions see the respective function docstrings in this module. To see an examplenary PDF output and .xlsx file take at look at the examples provided in the \"examples\" directory. This function is called by the GUI. Note that the functioned was specifically designed to be called from the GUI. Thus, tk.messagebox will pop up when errors are raised even if the GUI is not started.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateBatch(rootpath=\"C:/Users/admin/Dokuments/images\",\n                   apo_modelpath=\"C:/Users/admin/Dokuments/models/apo_model.h5\",\n                   fasc_modelpath=\"C:/Users/admin/Dokuments/models/apo_model.h5\",\n                   flip_file_path=\"C:/Users/admin/Dokuments/flip_flags.txt\",\n                   file_type=\"/**/*.tif, scaline=\"bar\", spacing=10, filter_fasc=False,\n                   settings=settings,\n                   gui=&lt;__main__.DL_Track_US object at 0x000002BFA7528190&gt;)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.calculateBatchManual","title":"<code>calculateBatchManual(rootpath, filetype, gui)</code>","text":"<p>Function used for manual calculation of fascicle length, muscle thickness and pennation angles in longitudinal ultrasonography images of human lower limb muscles.</p> <p>This function is not restricted to any specific muscles. However, its use is restricted to a specific method for assessing muscle thickness fascicle length and pennation angles.</p> <ul> <li>Muscle thickness:                    Exactly one segment reaching from the superficial to the                    deep aponeuroses of the muscle must be drawn.                    If multiple measurement are drawn, these are averaged.                    Drawing can be started by clickling the left mouse                    button and keeping it pressed until it is not further                    required to draw the line (i.e., the other aponeurosis                    border is reached). Only the respective y-coordinates                    of the points where the cursor was clicked and released                    are considered for calculation of muscle thickness.</li> <li>Fascicle length:                   Exactly three segments along the fascicleof the muscle                   must be drawn. If multiple fascicle are drawn, their                   lengths are averaged. Drawing can be started by clickling                   the left mouse button and keeping it pressed until one                   segment is finished (mostly where fascicle curvature                   occurs the other aponeurosis border is reached). Using                   the euclidean distance, the total fascicle length is                   computed as a sum of the segments.</li> <li>Pennation angle:                   Exactly two segments, one along the fascicle                   orientation, the other along the aponeurosis orientation                   must be drawn. The line along the aponeurosis must be                   started where the line along the fascicle ends. If                   multiple angle are drawn, they are averaged. Drawing can                   be started by clickling the left mouse button and keeping                   it pressed until it is not further required to draw the                   line (i.e., the aponeurosis border is reached by the                   fascicle). The angle is calculated using the arc-tan                   function. In order to scale the frame, it is required to draw a line of length 10 milimeter somewhere in the image. The line can be drawn in the same fashion as for example the muscle thickness. Here however, the euclidean distance is used to calculate the pixel / centimeter ratio. This has to be done for every frame.</li> </ul> <p>We also provide the functionality to extent the muscle aponeuroses to more easily extrapolate fascicles. The lines can be drawn in the same fashion as for example the muscle thickness.</p> PARAMETER DESCRIPTION <code>rootpath</code> <p>String variable containing the path to the folder where all images to be analyzed are saved.</p> <p> TYPE: <code>str</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the calculation process after each image.</p> <p> TYPE: <code>TK</code> </p> Notes <p>For specific explanations of the included functions see the respective function docstrings in this module. The function outputs an .xlsx file in rootpath containing the (averaged) analysis results for each image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateBatchManual(rootpath=\"C:/Users/admin/Dokuments/images\",\n                         filetype=\"/**/*.tif,\n                         gui\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.exportToExcel","title":"<code>exportToExcel(path, fasc_l_all, pennation_all, x_lows_all, x_highs_all, thickness_all, filename='Results', filtered_fasc=None, filtered_pennation=None, analysis_mode='video', image_filenames=None)</code>","text":"<p>Saves raw and optionally filtered analysis results to an Excel file, including an index column for each frame.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path where the Excel file will be saved.</p> <p> TYPE: <code>str</code> </p> <code>filename</code> <p>Name of the Excel file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Results'</code> </p> <code>fasc_l_all</code> <p>Raw fascicle length data.</p> <p> TYPE: <code>list of lists</code> </p> <code>pennation_all</code> <p>Raw pennation angle data.</p> <p> TYPE: <code>list of lists</code> </p> <code>x_lows_all</code> <p>X-coordinates of lower aponeurosis intersections.</p> <p> TYPE: <code>list of lists</code> </p> <code>x_highs_all</code> <p>X-coordinates of upper aponeurosis intersections.</p> <p> TYPE: <code>list of lists</code> </p> <code>thickness_all</code> <p>Muscle thickness measurements.</p> <p> TYPE: <code>list of lists</code> </p> <code>filtered_fasc</code> <p>Filtered fascicle length data.</p> <p> TYPE: <code>list of lists</code> DEFAULT: <code>None</code> </p> <code>filtered_pennation</code> <p>Filtered pennation angle data.</p> <p> TYPE: <code>list of lists</code> DEFAULT: <code>None</code> </p> <code>analysis_mode</code> <p>Analysis mode used for the calculations. If \"video\", savgol filter will be appiled.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'video'</code> </p> <code>image_filenames</code> <p>List of image filenames corresponding to the analysis results.</p> <p> TYPE: <code>list of str</code> DEFAULT: <code>None</code> </p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.getFlipFlagsList","title":"<code>getFlipFlagsList(flip_flag_path, gui)</code>","text":"<p>Function to retrieve flip values from a .txt file.</p> <p>The flip flags decide wether an image should be flipped or not. The flags can be 0 (image not flipped) or 1 (image is flipped). The flags must be specified in the .txt file and can either be on a seperate line for each image or on a seperate line for each folder. The amount of flip flags must equal the amount of images analyzed.</p> PARAMETER DESCRIPTION <code>flip_flag_path</code> <p>String variabel containing the absolute path to the flip flag .txt file containing the flip flags.</p> <p> TYPE: <code>str</code> </p> <code>gui</code> <p>The GUI object.</p> <p> TYPE: <code>Tk</code> </p> RETURNS DESCRIPTION <code>flip_flags</code> <p>A list variable containing all flip flags included in the specified .txt file</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; getFlipFlagsList(flip_flag_path=\"C:/Desktop/Test/FlipFlags/flags.txt\")\n[1, 0, 1, 0, 1, 1, 1]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.importAndReshapeImage","title":"<code>importAndReshapeImage(path_to_image, flip)</code>","text":"<p>Function to import and reshape an image. Moreover, based upon user specification the image might be flipped.</p> <p>Usually flipping is only required when the imported image is of a specific structure that is incompatible with the trained models provided here</p> PARAMETER DESCRIPTION <code>path_to_image</code> <p>String variable containing the imagepath. This should be an  absolute path.</p> <p> TYPE: <code>str</code> </p> <code>flip</code> <p>Integer value defining wheter an image should be flipped. This can be 0 (image is not flipped) or 1 (image is flipped).</p> <p> TYPE: <code>(0, 1)</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>img</code> <p>The loaded images is converted to a np.nadarray. This is done using the img_to_array kears functionality. The input image is futhter flipped (if selected), resized, respahed and normalized.</p> <p> TYPE: <code>ndarray</code> </p> <code>img_copy</code> <p>A copy of the input image.</p> <p> TYPE: <code>ndarray</code> </p> <code>non_flipped_img</code> <p>A copy of the input image. This copy is made prior to image flipping.</p> <p> TYPE: <code>ndarray</code> </p> <code>height</code> <p>Integer value containing the image height of the input image.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Integer value containing the image width of the input image.</p> <p> TYPE: <code>int</code> </p> <code>filename</code> <p>String value containing the name of the input image, not the entire path.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importAndReshapeImage(path_to_image=\"C:/Users/Dokuments/images/img1.tif\", flip=0)\n[[[[0.10113753 0.09391343 0.09030136] ... [0 0 0]]], [[[28 26 25] ... [ 0  0  0]]], [[[28 26 25] ... [ 0  0  0]]], 512, 512, img1.tif\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.importImageManual","title":"<code>importImageManual(path_to_image, flip)</code>","text":"<p>Function to import an image.</p> <p>This function is used when manual analysis of the image is selected in the GUI. For manual analysis, it is not necessary to resize, reshape and normalize the image. The image may be flipped.</p> PARAMETER DESCRIPTION <code>path_to_image</code> <p>String variable containing the imagepath. This should be an  absolute path.</p> <p> TYPE: <code>str</code> </p> <code>flip</code> <p>Integer value defining wheter an image should be flipped. This can be 0 (image is not flipped) or 1 (image is flipped).</p> <p> TYPE: <code>(0, 1)</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>img</code> <p>The loaded images as a np.nadarray in grayscale.</p> <p> TYPE: <code>ndarray</code> </p> <code>filename</code> <p>String value containing the name of the input image, not the entire path.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importImageManual(path_to_image=\"C:/Desktop/Test/Img1.tif\", flip=0)\n[[[28 26 25] [28 26 25] [28 26 25] ... [[ 0  0  0] [ 0  0  0] [ 0  0  0]]],\nImg1.tif\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture.preprocess_function","title":"<code>preprocess_function(image)</code>","text":"<p>Function to preprocess an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Image to be preprocessed.</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>ndarray</code> <p>Preprocessed image.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture_video.calculateArchitectureVideo","title":"<code>calculateArchitectureVideo(rootpath, apo_modelpath, fasc_modelpath, filetype, scaling, flip, step, filter_fasc, settings, gui, display_frame)</code>","text":"<p>Function to calculate muscle architecture in longitudinal ultrasonography videos of human lower limb muscles. The values computed are fascicle length (FL), pennation angle (PA), and muscle thickness (MT).</p> <p>The scope of this function is limited. videos of the vastus lateralis, tibialis anterior soleus and gastrocnemius  muscles can be analyzed. This is due to the limited amount of training data for our convolutional neural networks. This functions makes extensive use of several other functions and was designed to be executed from a GUI.</p> PARAMETER DESCRIPTION <code>rootpath</code> <p>String variable containing the path to the folder where all videos to be analyzed are saved.</p> <p> TYPE: <code>str</code> </p> <code>apo_modelpath</code> <p>String variable containing the absolute path to the aponeurosis neural network.</p> <p> TYPE: <code>str</code> </p> <code>fasc_modelpath</code> <p>String variable containing the absolute path to the fascicle neural network.</p> <p> TYPE: <code>str</code> </p> <code>flip</code> <p>String variable determining wheter all frames of a video are flipped vetically. Flipping is necessary as the models were trained in images of with specific fascicle orientation.</p> <p> TYPE: <code>str</code> </p> <code>filetype</code> <p>String variable containg the respective type of the videos. This is needed to select only the relevant video files in the root directory.</p> <p> TYPE: <code>str</code> </p> <code>scaling</code> <p>String variable determining the image scaling method. There are three types of scaling available: - scaling = \"manual\" (user must scale the video manually.   This only needs to be done in the first frame.)   detecting scaling bars on the right side of the image.) - scaling = \"No scaling\" (video frames are not scaled.) Scaling is necessary to compute measurements in centimeter, if \"no scaling\" is chosen, the results are in pixel units.</p> <p> TYPE: <code>str</code> </p> <code>step</code> <p>Integer variable containing the step for the range of video frames. If step != 1, frames are skipped according to the size of step. This might decrease processing time but also accuracy.</p> <p> TYPE: <code>int</code> </p> <code>filter_fasc</code> <p>If True, fascicles will be filtered so that no crossings are included. This may reduce number of totally detected fascicles.</p> <p> TYPE: <code>bool</code> </p> <code>settings</code> <p>Dictionary containing the settings of the GUI. These specify the prediction parameters for the neural networks.</p> <p> TYPE: <code>dict</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the calculation process after each image.</p> <p> TYPE: <code>TK</code> </p> <code>display_frame</code> <p>Boolean variable determining whether the current frame is displayed in main UI.</p> <p> TYPE: <code>bool</code> </p> See Also <p>do_calculations_video.py for exact description of muscle architecture parameter calculation.</p> Notes <p>For specific explanations of the included functions see the respective function docstrings in this module. To see an examplenary video output and .xlsx file take at look at the examples provided in the \"examples\" directory. This function is called by the GUI. Note that the functioned was specifically designed to be called from the GUI. Thus, tk.messagebox will pop up when errors are raised even if the GUI is not started.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateBatch(rootpath=\"C:/Users/admin/Dokuments/images\",\n                   apo_modelpath=\"C:/Users/admin/Dokuments/models/apo_model.h5\",\n                   fasc_modelpath=\"C:/Users/admin/Dokuments/models/apo_model.h5\",\n                   flip=\"Flip\", filetype=\"/**/*.avi, scaline=\"manual\",\n                   filter_fasc=False\n                   settings=settings,\n                   gui=&lt;__main__.DLTrack object at 0x000002BFA7528190&gt;,\n                   display_frame=True)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture_video.calculateArchitectureVideoManual","title":"<code>calculateArchitectureVideoManual(videopath, gui)</code>","text":"<p>Function used for manual calculation of fascicle length, muscle thickness and pennation angles in longitudinal ultrasonography videos of human lower limb muscles.</p> <p>This function is not restricted to any specific muscles. However, its use is restricted to a specific method for assessing muscle thickness fascicle length and pennation angles. Moreover, each video frame is analyzed seperately.</p> <ul> <li>Muscle thickness:                    Exactly one segment reaching from the superficial to the                    deep aponeuroses of the muscle must be drawn.                    If multiple measurement are drawn, these are averaged.                    Drawing can be started by clickling the left mouse                    button and keeping it pressed until it is not further                    required to draw the line (i.e., the other aponeurosis                    border is reached). Only the respective y-coordinates                    of the points where the cursor was clicked and released                    are considered for calculation of muscle thickness.</li> <li>Fascicle length:                   Exactly three segments along the fascicleof the muscle                   must be drawn. If multiple fascicle are drawn, their                   lengths are averaged. Drawing can be started by clickling                   the left mouse button and keeping it pressed until one                   segment is finished (mostly where fascicle curvature                   occurs the other aponeurosis border is reached). Using                   the euclidean distance, the total fascicle length is                   computed as a sum of the segments.</li> <li>Pennation angle:                   Exactly two segments, one along the fascicle                   orientation, the other along the aponeurosis orientation                   must be drawn. The line along the aponeurosis must be                   started where the line along the fascicle ends. If                   multiple angle are drawn, they are averaged. Drawing can                   be started by clickling the left mouse button and keeping                   it pressed until it is not further required to draw the                   line (i.e., the aponeurosis border is reached by the                   fascicle). The angle is calculated using the arc-tan                   function. In order to scale the frame, it is required to draw a line of length 10 milimeter somewhere in the image. The line can be drawn in the same fashion as for example the muscle thickness. Here however, the euclidean distance is used to calculate the pixel / centimeter ratio. This has to be done for every frame.</li> </ul> <p>We also provide the functionality to extent the muscle aponeuroses to more easily extrapolate fascicles. The lines can be drawn in the same fashion as for example the muscle thickness.</p> PARAMETER DESCRIPTION <code>videopath</code> <p>String variable containing the absolute path to the video to be analyzed.</p> <p> TYPE: <code>str</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the calculation process after each image frame.</p> <p> TYPE: <code>TK</code> </p> Notes <p>For specific explanations of the included functions see the respective function docstrings in this module. The function outputs an .xlsx file in rootpath containing the (averaged) analysis results for each image.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateBatchManual(videopath=\"C:/Users/admin/Dokuments/videoss\",\n                         gui\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture_video.importVideo","title":"<code>importVideo(vpath)</code>","text":"<p>Function to import a video. Video file types should be common ones like .avi or .mp4.</p> PARAMETER DESCRIPTION <code>vpath</code> <p>String variable containing the video. This should be an absolute path.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>cap</code> <p>Object that contains the video in a np.ndarrray format. In this way, seperate frames can be accessed.</p> <p> TYPE: <code>VideoCapture</code> </p> <code>vid_len</code> <p>Integer variable containing the number of frames present in cap.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Integer variable containing the image width of the input image.</p> <p> TYPE: <code>int</code> </p> <code>filename</code> <p>String variable containing the name of the input video, not the entire path.</p> <p> TYPE: <code>str</code> </p> <code>vid_out</code> <p>Object that is stored in the vpath folder. Contains the analyzed video frames and is titled \"..._proc.avi\" The name can be changed but must be different than the input video.</p> <p> TYPE: <code>VideoWriter</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importVideo(vpath=\"C:/Users/Dokuments/videos/video1.avi\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calculate_architecture_video.importVideoManual","title":"<code>importVideoManual(vpath)</code>","text":"<p>Function to import a video. Video file types should be common ones like .avi or .mp4. This function is used for manual analysis of videos.</p> <p>Here, no processed video is saved subsequent to analysis.</p> PARAMETER DESCRIPTION <code>vpath</code> <p>String variable containing the video. This should be an absolute path.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code> cap : cv2.VideoCapture</code> <p>Object that contains the video in a np.ndarrray format. In this way, seperate frames can be accessed.</p> <code>vid_len</code> <p>A copy of the input image.</p> <p> TYPE: <code>ndarray</code> </p> <code>vid_width</code> <p>Integer value containing the image width of the input video.</p> <p> TYPE: <code>int</code> </p> <code>vid_height</code> <p>Integer value containing the image height of the input video.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Integer value containing the image width of the input image.</p> <p> TYPE: <code>int</code> </p> <code>filename</code> <p>String value containing the name of the input video, not the entire path.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; importVideo(vpath=\"C:/Users/Dokuments/videos/video1.avi\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.calibrate.calibrateDistanceStatic","title":"<code>calibrateDistanceStatic(img, spacing)</code>","text":"<p>Function to calibrate an image to convert measurements in pixel units to centimeter.</p> <p>The function calculates the distance in pixel units between two scaling bars on the input image. The bars should be positioned on the right side of image. The distance (in milimeter) between two bars must be specified by the spacing variable. It is the known distance between two bars in milimeter. Then the ratio of pixel / centimeter is calculated. To get the distance, the median distance between two detected bars is calculated.</p> PARAMETER DESCRIPTION <code>img</code> <p>Input image to be analysed as a numpy array. The image must be loaded prior to calibration, specifying a path is not valid.</p> <p> TYPE: <code>ndarray</code> </p> <code>spacing</code> <p>Integer variable containing the known distance in milimeter between the two scaling bars. This can be 5, 10, 15 or 20 milimeter.</p> <p> TYPE: <code>(10, 5, 15, 20)</code> DEFAULT: <code>10</code> </p> RETURNS DESCRIPTION <code>calib_dist</code> <p>Integer variable containing the distance between the two specified point in pixel units.</p> <p> TYPE: <code>int</code> </p> <code>scale_statement</code> <p>String variable containing a statement how many milimeter correspond to how many pixels.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calibrateDistanceStatic(img=([[[[0.22414216 0.19730392 0.22414216] ... [0.2509804  0.2509804  0.2509804 ]]]), 5)\n99, 5 mm corresponds to 99 pixels\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis","title":"<code>ManualAnalysis</code>","text":"<p>Python class to manually annotate longitudinal muscle ultrasonography images/videos of human lower limb muscles. An analysis tkinter GUI is opened upon initialization of the class. By clicking the buttons, the user can switch between different parameters to analyze in the images.</p> <ul> <li>Muscle thickness:                    Exactly one segment reaching from the superficial to                    the deep aponeuroses of the muscle must be drawn.                    If multiple measurement are drawn, these are averaged.                    Drawing can be started by clickling the left mouse                    button and keeping it pressed until it is not further                    required to draw the line (i.e., the other aponeurosis                    border is reached). Only the respective y-coordinates                    of the points where the cursor was clicked and released                    are considered for calculation of muscle thickness.</li> <li>Fascicle length:                   Exactly three segments along the fascicleof the muscle                   must be drawn. If multiple fascicle are drawn, their                   lengths are averaged. Drawing can be started by                   clickling the left mouse button and keeping it pressed                   until one segment is finished (mostly where fascicle                   curvature occurs the other aponeurosis border is                   reached). Using the euclidean distance, the total                   fascicle length is computed as a sum of the segments.</li> <li>Pennation angle:                   Exactly two segments, one along the fascicle                   orientation, the other along the aponeurosis orientation                   must be drawn. The line along the aponeurosis must be                   started where the line along the fascicle ends. If                   multiple angle are drawn, they are averaged. Drawing                   can be started by clickling the left mouse button and                   keeping it pressed until it is not further required to                   draw the line (i.e., the aponeurosis border is reached                   by the fascicle). The angle is calculated using the                   arc-tan function. In order to scale the image/video frame, it is required to draw a line of length 10 milimeter somewhere in the image. The line can be drawn in the same fashion as for example the muscle thickness. Here however, the euclidean distance is used to calculate the pixel / centimeter ratio. This has to be done for every frame. We also provide the functionality to extend the muscle aponeuroses to more easily extrapolate fascicles. The lines can be drawn in the same fashion as for example the muscle thickness. During the analysis process, care must be taken to not accidentally click the left mouse button as those coordinates might mess up the results given that calculations are based on a strict analysis protocol.</li> </ul> ATTRIBUTE DESCRIPTION <code>self.image_list</code> <p>A list variable containing the absolute paths to all images / video to be analyzed.</p> <p> TYPE: <code>list</code> </p> <code>self.rootpath</code> <p>Path to root directory where images / videos for the analysis are saved.</p> <p> TYPE: <code>str</code> </p> <code>self.lines</code> <p>A list variable containing all lines that are drawn upon the image by the user. The list is emptied each time the analyzed parameter is changed.</p> <p> TYPE: <code>list</code> </p> <code>self.scale_coords</code> <p>A list variable containing the xy-coordinates coordinates of the scaling line start- and endpoints to calculate the distance between. The list is emptied each time a new image is scaled.</p> <p> TYPE: <code>list</code> </p> <code>self.thick_coords</code> <p>A list variable containing the xy-coordinates coordinates of the muscle thickness line start- and endpoints to calculate the distance between. The list is emptied each time a new image is scaled. Only the y-coordintes are used for further analysis.</p> <p> TYPE: <code>list</code> </p> <code>self.fasc_coords</code> <p>A list variable containing the xy-coordinates coordinates of the fascicle length line segments start- and endpoints to calculate the total length of the fascicle. The list is emptied each time a new image is analyzed.</p> <p> TYPE: <code>list</code> </p> <code>self.pen_coords</code> <p>A list variable containing the xy-coordinates coordinates of the pennation angle line segments start- and endpoints to calculate the angle of the fascicle. The list is emptied each time a new image is analyzed.</p> <p> TYPE: <code>list</code> </p> <code>self.coords</code> <p>Dictionary variable storing the xy-coordinates of mouse events during analysis. Mouse events are clicking and releasing of the left mouse button as well as dragging of the cursor.</p> <p> TYPE: <code>dict</code> </p> <code>self.count</code> <p>Index of image / video frame currently analysis in the list of image file / video frame paths. The default is 0 as the first image / frame analyzed always has the idex 0 in the list.</p> <p> TYPE: <code>int, default = 0</code> </p> <code>self.dataframe</code> <p>Panadas dataframe that stores the analysis results such as file name, fascicle length, pennation angle and muscle thickness. This dataframe is then saved in an .xlsx file.</p> <p> TYPE: <code>DataFrame</code> </p> <code>self.head</code> <p>tk.Toplevel instance opening a window containing the manual image analysis options.</p> <p> TYPE: <code>TK</code> </p> <code>self.mode</code> <p>tk.Stringvar variable containing the current parameter analysed by the user. The parameters are - muscle thickness : thick - fascicle length : fasc - pennation angle : pen - scaling : scale - aponeurosis drawing : apo The variable is updaten upon selection of the user. The default is muscle thickness.</p> <p> TYPE: <code>tk.Stringvar, default = thick</code> </p> <code>self.cavas</code> <p>tk.Canvas variable representing the canvas the image is plotted on in the GUI. The canvas is used to draw on the image.</p> <p> TYPE: <code>Canvas</code> </p> <code>self.img</code> <p>ImageTk.PhotoImage variable containing the current image that is analyzed. It is necessary to load the image in this way in order to plot the image.</p> <p> TYPE: <code>PhotoImage</code> </p> <code>self.dist</code> <p>Integer variable containing the length of the scaling line in pixel units. This variable is then used to scale the image.</p> <p> TYPE: <code>int</code> </p> METHOD DESCRIPTION <code>__init__</code> <p>Instance method to initialize the class.</p> <code>calculateBatchManual</code> <p>Instance method creating the GUI for manual image analysis.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.__init__","title":"<code>__init__(img_list, rootpath)</code>","text":"PARAMETER DESCRIPTION <code>img_list</code> <p>A list variable containing the absolute paths to all images / video to be analyzed.</p> <p> TYPE: <code>str</code> </p> <code>rootpath</code> <p>Path to root directory where images / videos for the analysis are saved.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; man_analysis = ManualAnalysis(\n    img_list=[\"C:/user/Dokuments/images/image1.tif\",\n    \"C:/user/Dokuments/images/image2.tif\",\n    \"C:/user/Dokuments/images/image3.tif\"],\n    rootpath=\"C:/user/Dokuments/images\")\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.calculateBatchManual","title":"<code>calculateBatchManual()</code>","text":"<p>Instance method creating a GUI for manual annotation of longitudinal ultrasoud images of human lower limb muscles.</p> <p>The GUI contains several analysis options for the current image openend. The user is able to switch between analysis of muscle thickness, fascicle length and pennation angle. Moreover, the image can be scaled and the aponeuroses can be extendet to easy fascicle extrapolation. When one image is finished, the GUI updated by clicking \"next image\". The Results can be saved by clicking \"save results\". It is possible to save each image seperately. The GUI can be closed by clicking \"break analysis\" or simply close the window.</p> Notes <p>The GUI can be initated from the main DL_Track GUI. It is also possible to initiate the ManualAnalis class from the command promt and interact with its GUI as a standalone. To do so, the lines 231 (self.head = tk.Tk()) and 314 (self.head.mainloop()) must be uncommented and the line 233 (self.head = tk.Toplevel()) must be commented.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.calculateFascicles","title":"<code>calculateFascicles(fasc_list)</code>","text":"<p>Instance method to calculate muscle fascicle legth as a sum of three annotated segments.</p> <p>The length of three line segment is calculated by summing their individual length. Here, the length of a single annotated fascicle is calculated considering the three drawn segments of the respective fascicle. The euclidean distance between the start and endpoint of each segment is calculated and summed. Then the length of the fascicle is outputted in pixel units.</p> PARAMETER DESCRIPTION <code>fasc_list</code> <p>List variable containing the xy-coordinates of the first mouse click event and the mouse release event of each annotated fascicle segment.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>fascicles</code> <p>List variable containing the fascicle length in pixel units. This value is calculated by summing the euclidean distance of each drawn fascicle segment. If multiple fascicles are drawn, multiple fascicle length values are outputted.</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateFascicles(fasc_list=[[392.0, 622.0], [632.0, 544.0],\n                                  [632.0, 544.0],\n                                  [1090.0, 415.0], [1090.0, 415.0],\n                                  [1274.0, 381.0],\n                                  [449.0, 627.0], [748.0, 541.0],\n                                  [748.0, 541.0],\n                                  [1109.0, 429.0], [1109.0, 429.0],\n                                  [1297.0, 390.0]])\n[915.2921723246823, 881.0996335404545]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.calculatePennation","title":"<code>calculatePennation(pen_list)</code>","text":"<p>Instance method to calculate muscle pennation angle between three points.</p> <p>The angle between three points is calculated using the arc tangent. Here, the points used for calculation are the start and endpoint of the line segment drawn alongside the fascicle as well as the endpoint of the segment drawn along the aponeurosis. The pennation angle is outputted in degrees.</p> PARAMETER DESCRIPTION <code>pen_list</code> <p>List variable containing the xy-coordinates of the first mouse click event and the mouse release event of each annotated pennation angle segment.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>pen_angles</code> <p>List variable containing the pennation angle in degrees. If multiple pennation angles are drawn, multiple angle values are outputted.</p> <p> TYPE: <code>list</code> </p> See Also <p>self.getAngle()</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateFascicles(pen_list=[[760.0, 579.0], [620.0, 629.0],\n                                 [620.0, 629.0],\n                                 [780.0, 631.0], [533.0, 571.0],\n                                 [378.0, 627.0], [378.0, 627.0],\n                                 [558.0, 631.0]] )\n[20.369984003523715, 21.137327423466722]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.calculateThickness","title":"<code>calculateThickness(thick_list)</code>","text":"<p>Instance method to calculate distance between deep and superficial muscle aponeuroses, also known as muscle thickness.</p> <p>The length of the segment is calculated by determining the absolute distance of the y-coordinates of two points. Here, the muscle thickness is calculated considering only the y-coordinates of the start and end points of the drawn segments. In this way, incorrect placement of the segments by drawing skewed lines can be accounted for. Then the thickness is outputted in pixel units.</p> PARAMETER DESCRIPTION <code>thick_list</code> <p>List variable containing the xy-coordinates of the first mouse click event and the mouse release event of the muscle thickness annotation. This must be the points at the beginning and end of the thickness segment.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>thickness</code> <p>List variable containing the muscle thickness in pixel units. This value is calculated using only the difference of the y-coordinates of the two points. If multiple segments are drawn, multiple thickness values are outputted.</p> <p> TYPE: <code>list</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; calculateThickness(thick_list=[[450, 41.0], [459, 200]])\n[159]\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.click","title":"<code>click(event)</code>","text":"<p>Instance method to record mouse clicks on canvas.</p> <p>When the left mouse button is clicked on the canvas, the xy-coordinates are stored for further analysis. When the button is clicked multiple times, multiple values are stored.</p> PARAMETER DESCRIPTION <code>event</code> <p>tk.Event variable containing the mouse event that is bound to the instance method.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; self.canvas.bind(\"&lt;ButtonPress-1&gt;\", self.click)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.closeWindow","title":"<code>closeWindow()</code>","text":"<p>Instance method to close window upon call.</p> <p>This method is evoked by clicking the button \"Break Analysis\". It is necessary to use a different function, otherwise the window would be destroyed upon starting.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.compileSaveResults","title":"<code>compileSaveResults()</code>","text":"<p>Instance method to save the analysis results.</p> <p>A pd.DataFrame object must be used. The results inculded in the dataframe are saved to an .xlsx file. Depending on the form of class initialization, the .xlsx file is either saved to self.root (GUI) or self.out (command prompt).</p> Notes <p>An .xlsx file is saved to a specified location containing all analysis results.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.drag","title":"<code>drag(event)</code>","text":"<p>Instance method to record mouse cursor dragging on canvas.</p> <p>When the cursor is dragged on the canvas, the xy-coordinates are stored and updated for further analysis. This is used to draw the line that follows the cursor on the screen. Coordinates are only recorded when the left mouse button is pressed.</p> PARAMETER DESCRIPTION <code>event</code> <p>tk.Event variable containing the mouse event that is bound to the instance method.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; self.canvas.bind(\"&lt;B1-Motion&gt;\", self.click)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.getAngle","title":"<code>getAngle(a, b, c)</code>","text":"<p>Instance method to calculate angle between three points.</p> <p>The angle is calculated using the arc tangent. The arc tangent is used to find the slope in radians when Y and X co-ordinates are given. The output is the arc tangent of y/x in radians, which is between PI and -PI. Then the output is converted to degrees.</p> PARAMETER DESCRIPTION <code>a</code> <p>List variable containing the xy-coordinates of the first mouse click event of the pennation angle annotation. This must be the point at the beginning of the fascicle segment.</p> <p> TYPE: <code>list</code> </p> <code>b</code> <p>List variable containing the xy-coordinates of the second mouse click event of the pennation angle annotation. This must be the point at the intersection between fascicle and aponeurosis.</p> <p> TYPE: <code>list</code> </p> <code>c</code> <p>List variable containing the xy-coordinates of the fourth mouse event of the pennation angle annotation. This must be the endpoint of the aponeurosis segment.</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <code>ang</code> <p>Float variable containing the pennation angle in degrees between the segments drawn on the canvas by the user. Only the angle smaller than 180 degrees is returned.</p> <p> TYPE: <code>float</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; getAngle(a=[904.0, 41.0], b=[450,380], c=[670,385])\n25.6\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.release","title":"<code>release(event)</code>","text":"<p>Instance method to record mouse button releases on canvas.</p> <p>When the left mouse button is released on the canvas, the xy-coordinates are stored for further analysis. When the button is released multiple times, multiple values are stored.</p> PARAMETER DESCRIPTION <code>event</code> <p>tk.Event variable containing the mouse event that is bound to the instance method.</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; self.canvas.bind(\"&lt;ButtonRelease-1&gt;\", self.click)\n</code></pre>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.saveResults","title":"<code>saveResults()</code>","text":"<p>Instance Method to save the analysis results to a pd.DataFrame.</p> <p>A list of each variable to be saved must be used. The list must contain the coordinates of the recorded events during parameter analysis. Then, the respective parameters are calculated using the coordinates in each list and inculded in a dataframe. Estimates or fascicle length, pennation angle, muscle thickness are saved.</p> Notes <p>In order to correctly calculate the muscle parameters, the amount of coordinates must be specific. See class documentatio of documentation of functions used to calculate parameters.</p> See Also <p>self.calculateThickness, self.calculateFascicle, self.calculatePennation</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.setAngles","title":"<code>setAngles()</code>","text":"<p>Instance method to display the instructions for pennation angle analysis.</p> <p>This function is bound to the \"Pennation Angle\" radiobutton and appears each time the button is selected. In this way, the user is reminded on how to analyze pennation angle.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.setApo","title":"<code>setApo()</code>","text":"<p>Instance method to display the instructions for aponeuroses extending.</p> <p>This function is bound to the \"Draw Aponeurosis\" radiobutton and appears each time the button is selected. In this way, the user is reminded on how to draw aponeurosis extension on the image.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.setFascicles","title":"<code>setFascicles()</code>","text":"<p>Instance method to display the instructions for muscle fascicle analysis.</p> <p>This function is bound to the \"Muscle Fascicles\" radiobutton and appears each time the button is selected. In this way, the user is reminded on how to analyze muscle fascicles.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.setScale","title":"<code>setScale()</code>","text":"<p>Instance method to display the instructions for image scaling.</p> <p>This function is bound to the \"Scale Image\" radiobutton and appears each time the button is selected. In this way, the user is reminded on how to scale the image.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.setThickness","title":"<code>setThickness()</code>","text":"<p>Instance method to display the instructions for muscle thickness analysis.</p> <p>This function is bound to the \"Muscle Thickness\" radiobutton and appears each time the button is selected. In this way, the user is reminded on how to analyze muscle thickness.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.stopAnalysis","title":"<code>stopAnalysis()</code>","text":"<p>Instance method to stop the current analysis on the canvas in the GUI.</p> <p>The analysis is stopped upon clicking of \"Break Analysis\" button in the GUI. The current analysis results are saved when the analysis is terminated. The analysis can be terminated at any timepoint during manual annotation. Prior to terminating, the user is asked for confirmation.</p>"},{"location":"api_helpers/#DL_Track_US.gui_helpers.manual_tracing.ManualAnalysis.updateImage","title":"<code>updateImage()</code>","text":"<p>Instance method to update the current image displayed on the canvas in the GUI.</p> <p>The image is updated upon clicking of \"Next Image\" button in the GUI. Images can be updated until all images have been analyzed. Even when the image is updated, the analysis results are stored.</p>"},{"location":"api_index/","title":"API Reference","text":"Description <p>This module contains a class with methods to automatically and manually annotate longitudinal ultrasonography images and videos. When the class is initiated,a graphical user interface is opened. This is the main GUI of the DL_Track package. From here, the user is able to navigate all functionalities of the package. These extend the methods in this class. The main functionalities of the GUI contained in this module are automatic and manual evalution of muscle ultrasonography images. Inputted images or videos are analyzed and the parameters muscle fascicle length, pennation angle and muscle thickness are returned for each image or video frame. The parameters are analyzed using convolutional neural networks (U-net, VGG16). This module and all submodules contained in the /gui_helpers modality are extensions and improvements of the work presented in Cronin et al. (2020). There, the core functionalities of this code are already outlined and the comparability of the model segmentations to manual analysis (current gold standard) is described. Here, we have improved the code by integrating everything into a graphical user interface.</p> Functions scope <p>For scope of the functions see class documentation.</p> Notes <p>Additional information and usage exaples can be found in the video tutorials provided for this package.</p> References <p>[1] VGG16: Simonyan, Karen, and Andrew Zisserman. \u201cVery deep convolutional networks for large-scale image recognition.\u201d arXiv preprint arXiv:1409.1556 (2014) [2] U-net: Ronneberger, O., Fischer, P. and Brox, T. \"U-Net: Convolutional Networks for Biomedical Image Segmentation.\" arXiv preprint arXiv:1505.04597 (2015) [3] DL_Track: Cronin, Neil J. and Finni, Taija and Seynnes, Olivier. \"Fully automated analysis of muscle architecture from B-mode ultrasound images with deep learning.\" arXiv preprint arXiv:https://arxiv.org/abs/2009.04790 (2020)</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack","title":"<code>DLTrack</code>","text":"<p>               Bases: <code>CTk</code></p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack--dltrack-graphical-user-interface-for-muscle-ultrasound-analysis","title":"DLTrack Graphical User Interface for Muscle Ultrasound Analysis","text":"<p>Main UI class of the DL_Track_US package providing both automatic and manual analysis of longitudinal B-mode ultrasound images and videos. Supports annotation, model-based segmentation, result visualization, and metric export.</p> <p>The GUI integrates U-Net and VGG16-based CNN models to compute fascicle length, pennation angle, and muscle thickness. Users can customize analysis parameters, calibrate scaling, and optionally flip video frames.</p> Dependencies <ul> <li>GUI: tkinter, customtkinter, CTkToolTip</li> <li>Image/Video Handling: OpenCV (cv2), PIL, matplotlib</li> <li>Numerical: numpy, pandas</li> <li>System: os, sys, subprocess, threading, glob, json, traceback</li> </ul> ATTRIBUTE DESCRIPTION <code>main</code> <p>Primary GUI layout container.</p> <p> TYPE: <code>CTkFrame</code> </p> <code>results</code> <p>Frame for image/video and result display.</p> <p> TYPE: <code>CTkFrame</code> </p> <code>terminal</code> <p>Panel for figure/metric visualization.</p> <p> TYPE: <code>CTkFrame</code> </p> <code>analysis_type</code> <p>Current analysis mode (\"image\", \"video\", \"image_manual\", \"video_manual\").</p> <p> TYPE: <code>StringVar</code> </p> <code>filetype</code> <p>File extension filter for input files.</p> <p> TYPE: <code>StringVar</code> </p> <code>scaling</code> <p>Chosen scaling type (\"Bar\", \"Manual\", \"None\").</p> <p> TYPE: <code>StringVar</code> </p> <code>spacing</code> <p>Millimeter spacing used in manual calibration.</p> <p> TYPE: <code>StringVar</code> </p> <code>flip</code> <p>Flip option for video analysis (\"flip\", \"no_flip\").</p> <p> TYPE: <code>StringVar</code> </p> <code>filter_fasc</code> <p>Option to enable fascicle filtering.</p> <p> TYPE: <code>StringVar</code> </p> <code>step</code> <p>Frame step interval for video analysis.</p> <p> TYPE: <code>StringVar</code> </p> <code>input_dir</code> <p>Directory containing input images or videos.</p> <p> TYPE: <code>str</code> </p> <code>apo_model</code> <p>Path to the selected aponeurosis segmentation model.</p> <p> TYPE: <code>str</code> </p> <code>fasc_model</code> <p>Path to the selected fascicle segmentation model.</p> <p> TYPE: <code>str</code> </p> <code>flipflag_dir</code> <p>Path to CSV file with flip flags for images.</p> <p> TYPE: <code>str</code> </p> <code>video_path</code> <p>Path to a single video file (for manual analysis).</p> <p> TYPE: <code>str</code> </p> <code>processed_frames</code> <p>List of currently processed image frames or matplotlib figures.</p> <p> TYPE: <code>list</code> </p> <code>current_frame_index</code> <p>Index of currently displayed frame.</p> <p> TYPE: <code>int</code> </p> <code>settings</code> <p>Dictionary of user-defined configuration parameters.</p> <p> TYPE: <code>dict</code> </p> <code>_lock</code> <p>Thread lock for thread-safe flags.</p> <p> TYPE: <code>Lock</code> </p> <code>_is_running</code> <p>Indicates whether a processing thread is active.</p> <p> TYPE: <code>bool</code> </p> <code>_should_stop</code> <p>Indicates whether a running thread should be interrupted.</p> <p> TYPE: <code>bool</code> </p> METHOD DESCRIPTION <code>__init__</code> <p>Initializes GUI layout, variables, buttons, and bindings.</p> <code>run_code</code> <p>Launches appropriate analysis pipeline in a background thread.</p> <code>do_break</code> <p>Interrupts the currently running analysis thread.</p> <code>check_processing_complete</code> <p>Monitors thread execution and invokes callback on completion.</p> <code>display_frame</code> <p>Displays either an image frame or a matplotlib figure in the canvas.</p> <code>display_results</code> <p>Plots fascicle length medians across frames and embeds in GUI.</p> <code>calibrateDistanceManually</code> <p>Initializes interactive calibration interface.</p> <code>calibrate_distance</code> <p>Computes and displays pixel-to-mm scaling from two points.</p> <code>mclick</code> <p>Captures mouse positions during manual calibration.</p> <code>get_input_dir</code> <p>File dialog utilities for selecting inputs.</p> <code>get_flipfile_path</code> <p>Select paths for flip flag CSV or single video.</p> <code>change_analysis_type</code> <p>Dynamically enables/disables GUI components based on analysis type.</p> <code>change_spacing</code> <p>Enables calibration button only for manual scaling.</p> <code>update_frame_by_slider</code> <p>Slider controls for frame navigation.</p> <code>load_settings</code> <p>Loads persistent analysis settings from JSON file.</p> <code>open_settings</code> <p>Opens editable JSON settings file in default system editor.</p> Notes <ul> <li>Intended to be launched via <code>python -m DL_Track_US_GUI</code> or integrated into apps.</li> <li>Processing callbacks are executed asynchronously to preserve UI responsiveness.</li> <li>Uses external functions from <code>gui_helpers</code> and <code>gui_modules.AdvancedAnalysis</code>.</li> </ul> See Also <p>gui_helpers.calculateBatch gui_helpers.calculateArchitectureVideo gui_helpers.calculateBatchManual gui_helpers.calculateArchitectureVideoManual</p> References <p>[1] Cronin et al. (2020), arXiv:2009.04790 [2] Ritsche et al. (2023), JOSS, https://doi.org/10.21105/joss.05206 [3] Ritsche et al. (2024), UMB, https://doi.org/10.1016/j.ultrasmedbio.2023.10.011</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.is_running","title":"<code>is_running</code>  <code>property</code> <code>writable</code>","text":"<p>Instance method to define the is_running property getter method. By defining this as a property, is_running is treated like a public attribute even though it is private.</p> <p>This is used to stop the analysis process running in a seperate thread.</p> RETURNS DESCRIPTION <code>is_running</code> <p>Boolean variable to check whether the analysis process started from the GUI is running. The process is only stopped when is_running = True.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.should_stop","title":"<code>should_stop</code>  <code>property</code> <code>writable</code>","text":"<p>Instance method to define the should_stop property getter method. By defining this as a property, should_stop is treated like a public attribute even though it is private.</p> <p>This is used to stop the analysis process running in a seperate thread.</p> RETURNS DESCRIPTION <code>should_stop</code> <p>Boolean variable to decide whether the analysis process started from the GUI should be stopped. The process is stopped when should_stop = True.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":""},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.calibrateDistanceManually","title":"<code>calibrateDistanceManually()</code>","text":"<p>Function to manually calibrate an image to convert measurements in pixel units to centimeters.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.calibrate_distance","title":"<code>calibrate_distance()</code>","text":"<p>Calculate the distance between two points and display the result.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.change_analysis_type","title":"<code>change_analysis_type(*args)</code>","text":"<p>Unified instance method to display the required parameters based on the selected analysis type.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.change_spacing","title":"<code>change_spacing(*args)</code>","text":"<p>Enable the spacing button only when the scaling type is set to \"Manual\".</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.check_processing_complete","title":"<code>check_processing_complete(thread, callback)</code>","text":"<p>Checks if the processing thread has finished.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.display_frame","title":"<code>display_frame(item)</code>","text":"<p>Display the current frame or figure on the GUI canvas.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.display_results","title":"<code>display_results(input_df_raw, input_df_filtered, input_df_medians, input_df_pa_raw=None, input_df_pa_filtered=None, input_df_pa_medians=None)</code>","text":"<p>Display analysis results (video mode): median Fascicle Length (FL) curves and, optionally, Pennation Angle (PA) curves. Both y-axes are shown on the LEFT side with offset spines.</p> PARAMETER DESCRIPTION <code>input_df_raw</code> <p>Raw FL results (first column is an index/frame number).</p> <p> TYPE: <code>DataFrame</code> </p> <code>input_df_filtered</code> <p>Filtered FL results (first column is an index/frame number).</p> <p> TYPE: <code>DataFrame</code> </p> <code>input_df_medians</code> <p>Per-frame median FL (first column is an index/frame number or a single column).</p> <p> TYPE: <code>DataFrame</code> </p> <code>input_df_pa_raw</code> <p>Raw PA results (layout identical to input_df_raw).</p> <p> TYPE: <code>DataFrame</code> DEFAULT: <code>None</code> </p> <code>input_df_pa_filtered</code> <p>Filtered PA results (layout identical to input_df_filtered).</p> <p> TYPE: <code>DataFrame</code> DEFAULT: <code>None</code> </p> <code>input_df_pa_medians</code> <p>Per-frame median PA (layout identical to input_df_medians).</p> <p> TYPE: <code>DataFrame</code> DEFAULT: <code>None</code> </p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.do_break","title":"<code>do_break()</code>","text":"<p>Instance method to break the analysis process when the button \"break\" is pressed.</p> <p>This changes the instance attribute self.should_stop to True, given that the analysis is already running. The attribute is checked befor every iteration of the analysis process.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.get_apo_model_path","title":"<code>get_apo_model_path()</code>","text":"<p>Instance method to ask the user to select the apo model path. This must be an absolute path and the model must be a .h5 file.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.get_fasc_model_path","title":"<code>get_fasc_model_path()</code>","text":"<p>Instance method to ask the user to select the fascicle model path. This must be an absolute path and the model must be a .h5 file.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.get_flipfile_path","title":"<code>get_flipfile_path()</code>","text":"<p>Instance method to ask the user to select the flipfile path. The flipfile should contain the flags used for flipping each image. If 0, the image is not flipped, if 1 the image is flipped. This must be an absolute path.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.get_input_dir","title":"<code>get_input_dir()</code>","text":"<p>Instance method to ask the user to select the input directory. All image files (of the same specified filetype) in the input directory are analysed.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.get_video_path","title":"<code>get_video_path()</code>","text":"<p>Instance method to ask the user to select the video path for manual video analysis. This must be an absolute path.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.load_settings","title":"<code>load_settings()</code>","text":"<p>Instance Method to load the setting file for.</p> <p>Executed each time when the GUI or a toplevel is openened. The settings specified by the user will then be transferred to the code and used.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.mclick","title":"<code>mclick(event)</code>","text":"<p>Instance method to detect mouse click coordinates in image.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.on_processing_complete","title":"<code>on_processing_complete()</code>","text":"<p>Callback to execute after processing completes. Updates the slider range based on the processed frames or figures.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.open_settings","title":"<code>open_settings()</code>","text":"<p>Instance Method to open the setting file for.</p> <p>Executed when the button \"Settings\" in master GUI window is pressed. A python file is openend containing a dictionary with relevant variables that users should be able to customize.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.resource_path","title":"<code>resource_path(relative_path)</code>","text":"<p>Get absolute path to resource (for dev and PyInstaller)</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.run_code","title":"<code>run_code()</code>","text":"<p>Instance method to execute the analysis process when the \"run\" button is pressed.</p> <p>Which analysis process is executed depends on the user selection. By pressing the button, a seperate thread is started in which the analysis is run. This allows the user to break any analysis process. Moreover, the threading allows interaction with the main GUI during ongoing analysis process. This function handles most of the errors occuring during specification of file and analysis parameters. All other exeptions are raised in other function of this package.</p> RAISES DESCRIPTION <code>AttributeError</code> <p>The execption is raised when the user didn't specify the file or training parameters correctly. A tk.messagebox is openend containing hints how to solve the issue.</p> <code>FileNotFoundError</code> <p>The execption is raised when the user didn't specify the file or training parameters correctly. A tk.messagebox is openend containing hints how to solve the issue.</p> <code>PermissionError</code> <p>The execption is raised when the user didn't specify the file or training parameters correctly. A tk.messagebox is openend containing hints how to solve the issue.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.update_frame_by_slider","title":"<code>update_frame_by_slider(value)</code>","text":"<p>Update the displayed frame based on the slider position.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.DLTrack.update_slider_range","title":"<code>update_slider_range()</code>","text":"<p>Updates the slider to match the number of processed frames or figures.</p>"},{"location":"api_index/#DL_Track_US.DL_Track_US_GUI.runMain","title":"<code>runMain()</code>","text":"<p>Function that enables usage of the gui from command promt as pip package.</p> Notes <p>The GUI can be executed by typing 'python -m DL_Track_US_GUI.py' in a terminal subsequtently to installing the pip package and activating the respective library.</p> <p>It is not necessary to download any files from the repository when the pip package is installed.</p> <p>For documentation of DL_Track see top of this module.</p>"},{"location":"api_model_training/","title":"Model Training","text":""},{"location":"api_model_training/#model_training","title":"model_training","text":"Description <p>This module contains functions to train a VGG16 encoder U-net decoder CNN. The module was specifically designed to be executed from a GUI. When used from the GUI, the module saves the trained model and weights to a given directory. The user needs to provide paths to the image and label/ mask directories. Instructions for correct image labelling can be found in the Labelling directory.</p> Functions scope <p>conv_block     Function to build a convolutional block for the U-net decoder path of     the network.     The block is built using several keras.layers functionalities. decoder_block     Function to build a decoder block for the U-net decoder path of     the network.     The block is built using several keras.layers functionalities. build_vgg16_model     Function that builds a convolutional network consisting of an VGG16     encoder path and a U-net decoder path. IoU     Function to compute the intersection over union score (IoU),     a measure of prediction accuracy. This is sometimes also called     Jaccard score. dice_score     Function to compute the Dice score, a measure of prediction accuracy. focal_loss      Function to compute the focal loss, a measure of prediction accuracy. load_images     Function to load images and manually labeled masks from a specified     directory. train_model     Function to train a convolutional neural network with VGG16 encoder and     U-net decoder. All the steps necessary to properly train a neural     network are included in this function.</p> Notes <p>Additional information and usage examples can be found at the respective functions documentations.</p> Description <p>Python module which contains a function, which allows to generate new training images from the input images. The newly generated data will be saved under the same directories as the input data.</p> <p>This module provides a function, image_augmentation, that performs data augmentation on input images and their corresponding masks to generate new training data for machine learning models. Data augmentation is a common technique used to artificially increase the diversity of the training dataset by applying various transformations to the original images.</p> Functions scope <p>image_augmentation     Function, which generates new training data from the input images through data augmentation.</p> <p>Module to perform analysis of image files used for training model</p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.StopTrainingCallback","title":"<code>StopTrainingCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>A custom Keras callback to stop training early based on a GUI signal.</p> <p>This callback checks the <code>should_stop</code> attribute of a GUI instance at the beginning of each epoch and stops training if the flag is set to <code>True</code>.</p> PARAMETER DESCRIPTION <code>gui_instance</code> <p>The GUI instance that contains the <code>should_stop</code> attribute. It should be set to <code>True</code> when the user requests training to stop.</p> <p> TYPE: <code>object</code> </p> ATTRIBUTE DESCRIPTION <code>gui_instance</code> <p>Reference to the GUI instance for checking the stopping condition.</p> <p> TYPE: <code>object</code> </p> METHOD DESCRIPTION <code>on_epoch_begin</code> <p>Checks if <code>should_stop</code> is set to <code>True</code> and stops training if necessary.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tensorflow.keras.models import Sequential\n&gt;&gt;&gt; from tensorflow.keras.layers import Dense\n&gt;&gt;&gt; model = Sequential([Dense(10, activation='relu', input_shape=(20,))])\n&gt;&gt;&gt; model.compile(optimizer='adam', loss='mse')\n&gt;&gt;&gt; gui_instance.should_stop = False  # Simulating a GUI stop flag\n&gt;&gt;&gt; stop_callback = StopTrainingCallback(gui_instance)\n&gt;&gt;&gt; model.fit(X_train, y_train, epochs=100, callbacks=[stop_callback])\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.StopTrainingCallback.__init__","title":"<code>__init__(gui_instance)</code>","text":"PARAMETER DESCRIPTION <code>gui_instance</code> <p>The GUI instance containing the <code>should_stop</code> attribute.</p> <p> TYPE: <code>object</code> </p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.StopTrainingCallback.on_epoch_begin","title":"<code>on_epoch_begin(epoch, logs=None)</code>","text":"<p>Called at the beginning of each epoch. Checks if training should stop.</p> PARAMETER DESCRIPTION <code>epoch</code> <p>The index of the current epoch.</p> <p> TYPE: <code>int</code> </p> <code>logs</code> <p>Dictionary containing training metrics (default is None).</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.IoU","title":"<code>IoU(y_true, y_pred, dtype=tf.float32)</code>","text":"<p>Function to compute the intersection over union score (IoU), a measure of prediction accuracy. This is sometimes also called Jaccard score.</p> <p>The IoU can be used as a loss metric during binary segmentation when convolutional neural networks are applied. The IoU is calculated for both the training and validation set.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True positive image segmentation label predefined by the user. This is the mask that is provided prior to model training.</p> <p> TYPE: <code>Tensor</code> </p> <code>y_pred</code> <p>Predicted image segmentation by the network.</p> <p> TYPE: <code>Tensor</code> </p> <code>dtype</code> <p>Data type of the IoU calculation. The default is tf.float32.</p> <p> TYPE: <code>default = tff.float32</code> DEFAULT: <code>float32</code> </p> RETURNS DESCRIPTION <code>iou</code> <p>IoU representation in the same shape as y_true, y_pred.</p> <p> TYPE: <code>Tensor</code> </p> Notes <p>The IoU is usually calculated as IoU = intersection / union. The intersection is calculated as the overlap of y_true and y_pred, whereas the union is the sum of y_true and y_pred.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; IoU(y_true=Tensor(\"IteratorGetNext:1\", shape=(1, 512, 512, 1), dtype=float32),\n        y_pred=Tensor(\"VGG16_U-Net/conv2d_8/Sigmoid:0\",\n        shape=(1, 512, 512, 1), dtype=float32), dtype=float32)\nTensor(\"truediv:0\", shape=(1, 512, 512), dtype=float32)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.build_vgg16_unet","title":"<code>build_vgg16_unet(input_shape)</code>","text":"<p>Function that builds a convolutional network consisting of a VGG16 encoder path and a U-net decoder path.</p> <p>The model is built using several Tensorflow.Keras functions. First, the whole VGG16 model is imported and built using pretrained imagenet weights and the input shape. Then, the encoder layers are pulled from the model incldung the bridge. Subsequently the decoder path from the U-net is built. Lastly, a 1x1 convolution is applied with sigmoid activation to perform binary segmentation on the input.</p> PARAMETER DESCRIPTION <code>input_shape</code> <p>Tuple describing the input shape. Must be of shape (...,...,...). Here we used (512,512,3) as input shape. The image size (512,512,) can be easily adapted. The channel numer (,,3) is given by the model and the pretrained weights. We advide the user not to change the image size segmentation results were best with the predefined size.</p> <p> TYPE: <code>tuple</code> </p> RETURNS DESCRIPTION <code>model</code> <p>The built VGG16 encoder U-net decoder convolutional network for binary segmentation on the input. The model can subsequently be used for training.</p> Notes <p>See our paper () and references for more detailed model description</p> References <p>[1] VGG16: Simonyan, Karen, and Andrew Zisserman. \u201cVery deep convolutional networks for large-scale image recognition.\u201d arXiv preprint arXiv:1409.1556 (2014) [2] U-net: Ronneberger, O., Fischer, P. and Brox, T. \"U-Net: Convolutional Networks for Biomedical Image Segmentation.\" arXiv preprint arXiv:1505.04597 (2015)</p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.conv_block","title":"<code>conv_block(inputs, num_filters)</code>","text":"<p>Function to build a convolutional block for the U-net decoder path of the network to be build. The block is built using several keras.layers functionalities.</p> <p>Here, we decided to use 'padding = same' and and a convolutional kernel of 3. This is adaptable in the code but will influence the model outcome. The convolutional block consists of two convolutional layers. Each creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.</p> PARAMETER DESCRIPTION <code>inputs</code> <p>Concattenated Tensorflow.Keras Tensor outputted from previous layer. The Tensor can be altered by adapting, i.e. the filter numbers but this will change the model training output. The input is then convolved using the built kernel.</p> <p> TYPE: <code>KerasTensor</code> </p> <code>num_filters</code> <p>Integer variable determining the number of filters used during model training. Here, we started with 'num_filers = 512'. The filter number is halfed each layer. The number of filters can be adapted in the code. Must be non-negative and non-zero.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Tensorflow.Keras Tensor used during model Training. The Tensor can be altered by adapting the input paramenters to the function or the upsampling but this will change the model training. The number of filters is halfed.</p> <p> TYPE: <code>KerasTensor</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; conv_block(inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 128),\n               dtype=tf.float32, name=None),\n               num_filters=128)\nKerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 64),\ndtype=tf.float32, name=None)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.decoder_block","title":"<code>decoder_block(inputs, skip_features, num_filters)</code>","text":"<p>Function to build a decoder block for the U-net decoder path of the network to be build. The block is build using several keras.layers functionalities.</p> <p>The block is built by applying a deconvolution (Keras.Conv2DTranspose) to upsample to input by a factor of 2. A concatenation with the skipped features from the mirrored vgg16 convolutional layer follows. Subsequently a convolutional block (see conv_block function) is applied to convolve the input with the built kernel.</p> PARAMETER DESCRIPTION <code>inputs</code> <p>Concattenated Tensorflow.Keras Tensor outputted from previous layer. The Tensor can be altered by adapting, i.e. the filter numbers but this will change the model training output.</p> <p> TYPE: <code>KerasTensor</code> </p> <code>skip_features</code> <p>Skip connections to the encoder path of the vgg16 encoder.</p> <p> TYPE: <code>Keras Tensor</code> </p> <code>num_filters</code> <p>Integer variable determining the number of filters used during model training. Here, we started with 'num_filers = 512'. The filter number is halfed each layer. The number of filters can be adapted in the code. Must be non-neagtive and non-zero.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>x</code> <p>Tensorflow.Keras Tensor used during model Training. The tensor is upsampled using Keras.Conv2DTranspose with a kernel of (2,2), 'stride=2' and 'padding=same'. The upsampling increases image size by a factor of 2. The number of filters is halfed. The Tensor can be altered by adapting the input paramenters to the function or the upsampling but this will change the model training.</p> <p> TYPE: <code>KerasTensor</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; decoder_block(inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 512),\n                  skip_features=KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 512),\n                  dtype=tf.float32, name=None)),\n                  num_filters=256)\nKerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 256),\ndtype=tf.float32, name=None)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.dice_score","title":"<code>dice_score(y_true, y_pred)</code>","text":"<p>Function to compute the Dice score, a measure of prediction accuracy.</p> <p>The Dice score can be used as a loss metric during binary segmentation when convolutional neural networks are applied. The Dice score is calculated for both the training and validation set.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True positive image segmentation label predefined by the user. This is the mask that is provided prior to model training.</p> <p> TYPE: <code>Tensor</code> </p> <code>y_pred</code> <p>Predicted image segmentation by the network.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>score</code> <p>Dice score representation in the same shape as y_true, y_pred.</p> <p> TYPE: <code>Tensor</code> </p> Notes <p>The IoU is usually calculated as Dice = 2 * intersection / union. The intersection is calculated as the overlap of y_true and y_pred, whereas the union is the sum of y_true and y_pred.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; IoU(y_true=Tensor(\"IteratorGetNext:1\", shape=(1, 512, 512, 1),\n        dtype=float32),\n        y_pred=Tensor(\"VGG16_U-Net/conv2d_8/Sigmoid:0\",\n        shape=(1, 512, 512, 1), dtype=float32),\n        smooth=1)\nTensor(\"dice_score/truediv:0\", shape=(1, 512, 512), dtype=float32)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.focal_loss","title":"<code>focal_loss(y_true, y_pred, alpha=0.8, gamma=2)</code>","text":"<p>Function to compute the focal loss, a measure of prediction accuracy.</p> <p>The focal loss can be used as a loss metric during binary segmentation when convolutional neural networks are applied. The focal loss score is calculated for both, the training and validation set. The focal loss is specifically applicable when class imbalances, i.e. between foregroung (muscle aponeurosis) and background (not muscle aponeurosis), are existent.</p> PARAMETER DESCRIPTION <code>y_true</code> <p>True positive image segmentation label predefined by the user. This is the mask that is provided prior to model training.</p> <p> TYPE: <code>Tensor</code> </p> <code>y_pred</code> <p>Predicted image segmentation by the network.</p> <p> TYPE: <code>Tensor</code> </p> <code>alpha</code> <p>Coefficient used on positive exaples, must be non-negative and non-zero.</p> <p> TYPE: <code>float</code> DEFAULT: <code>= 0.8</code> </p> <code>gamma</code> <p>Focussing parameter, must be non-negative and non-zero.</p> <p> TYPE: <code>float</code> DEFAULT: <code>= 2</code> </p> RETURNS DESCRIPTION <code>f_loss</code> <p>Tensor containing the calculated focal loss score.</p> <p> TYPE: <code>Tensor</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; focal_loss(y_true=Tensor(\"IteratorGetNext:1\", shape=(1, 512, 512, 1),\n        dtype=float32),\n        y_pred=Tensor(\"VGG16_U-Net/conv2d_8/Sigmoid:0\",\n        shape=(1, 512, 512, 1), dtype=float32),\n        smooth=1)\nTensor(\"focal_loss/Mean:0\", shape=(), dtype=float32)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.loadImages","title":"<code>loadImages(img_path, mask_path)</code>","text":"<p>Function to load images and manually labeled masks from a specified directory.</p> <p>The images and masks are loaded, resized and normalized in order to be suitable and usable for model training. The specified directories must lead to the images and masks. The number of images and masks must be equal. The images and masks can be in any common image format. The names of the images and masks must match. The image and corresponding mask must have the same name.</p> PARAMETER DESCRIPTION <code>img_path</code> <p>Path that leads to the directory containing the training images. Image must be in RGB format.</p> <p> TYPE: <code>str</code> </p> <code>mask_path</code> <p>Path that leads to the directory containing the mask images. Masks must be binary.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>train_imgs</code> <p>Resized, normalized training images stored in a numpy array.</p> <p> TYPE: <code>ndarray</code> </p> <code>mask_imgs</code> <p>Resized, normalized training masks stored in a numpy array.</p> <p> TYPE: <code>ndarray</code> </p> Notes <p>See labelling instruction for correct masks creation and use, if needed, the supplied ImageJ script to label your images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; loadImages(img_path = \"C:/Users/admin/Dokuments/images\",\n               mask_path = \"C:/Users/admin/Dokuments/masks\")\ntrain_imgs([[[[0.22414216 0.19730392 0.22414216] ... [0.22414216 0.19730392 0.22414216]]])\nmask_imgs([[[[0.] ... [0.]]])\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.model_training.trainModel","title":"<code>trainModel(img_path, mask_path, out_path, batch_size, learning_rate, epochs, loss, gui)</code>","text":"<p>Function to train a convolutional neural network with VGG16 encoder and U-net decoder. All the steps necessary to properly train an neural network are included in this function.</p> <p>This functions build upon all the other functions included in this module. Given that all input parameters are correctly specified, the images and masks are loaded, splittet into test and training sets, the model is compiled according to user specification and the model is trained.</p> PARAMETER DESCRIPTION <code>img_path</code> <p>Path that leads to the directory containing the training images. Image must be in RGB format.</p> <p> TYPE: <code>str</code> </p> <code>mask_path</code> <p>Path that leads to the directory containing the mask images. Masks must be binary.</p> <p> TYPE: <code>str</code> </p> <code>out_path</code> <p>Path that leads to the directory where the trained model is saved.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Integer value that determines the batch size per iteration through the network during model training. Although a larger batch size has advantages during model trainig, the images used here are large. Thus, the larger the batch size, the more compute power is needed or the longer the training duration. Must be non-negative and non-zero.</p> <p> TYPE: <code>int</code> </p> <code>learning_rate</code> <p>Float value determining the learning rate used during model training. Must be non-negative and non-zero.</p> <p> TYPE: <code>float</code> </p> <code>epochs</code> <p>Integer value that determines the amount of epochs that the model is trained befor training is aborted. The total amount of epochs will only be used if early stopping does not happen. Must be non-negative and non-zero.</p> <p> TYPE: <code>int</code> </p> <code>loss</code> <p>String variable that determines the loss function used during training. So far, only one type is supported here: - Binary cross-entropy. loss == \"BCE\"</p> <p> TYPE: <code>BCE</code> DEFAULT: <code>\"BCE\"</code> </p> <code>gui</code> <p>A tkinter.TK class instance that represents a GUI. By passing this argument, interaction with the GUI is possible i.e., stopping the model training model process.</p> <p> TYPE: <code>TK</code> </p> Notes <p>For specific explanations for the included functions see the respective function docstrings in this module. This function can either be run from the command prompt or is called by the GUI. Note that the functioned was specifically designed to be called from the GUI. Thus, tk.messagebox will pop up when errors are raised even if the GUI is not started.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; trainModel(img_path= \"C:/Users/admin/Dokuments/images\",\n               mask_path=\"C:/Users/admin/Dokuments/masks\",\n               out_path=\"C:/Users/admin/Dokuments/results\",\n               batch_size=1, learning_rate=0.005,\n               epochs=3, loss=\"BCE\", gui)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.data_augmentation.image_augmentation","title":"<code>image_augmentation(input_img_folder, input_mask_folder, gui)</code>","text":"<p>Function, which generates new training data from the input images through data augmentation. At the moment the number of added images is set to five. Perform data augmentation on images and masks in the specified input directories.</p> <p>The function applies data augmentation techniques to the images and masks located in the specified input directories. It creates augmented images and masks based on various augmentation parameters, and saves the augmented images and masks back to their respective input directories.</p> PARAMETER DESCRIPTION <code>input_img_folder</code> <p>Path to the folder containing the original input images.</p> <p> TYPE: <code>str</code> </p> <code>input_mask_folder</code> <p>Path to the folder containing the original input masks corresponding to the images.</p> <p> TYPE: <code>str</code> </p> <code>gui</code> <p>The main tkinter GUI object to display information to the user.</p> <p> TYPE: <code>Tk</code> </p> RETURNS DESCRIPTION <code>None</code> Notes <ul> <li>Although nothing is returned, the images in the input folders will be augmented three-fold.</li> <li>The function uses the Keras ImageDataGenerator for data augmentation.</li> <li>Augmented images and masks will be saved to their respective input directories with   filenames prefixed with numbers representing the index of the original images.</li> <li>The function will display information to the user in the specified tkinter GUI.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; root = tk.Tk()\n&gt;&gt;&gt; image_augmentation(\"data/images/\", \"data/masks/\", root\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.file_analysis.collect_images","title":"<code>collect_images(root_dir, target_dir, image_type)</code>","text":"<p>Searches through the root directory and its subdirectories for images of the specified type and copies them to the target directory with modified names to avoid overwriting. The modified name format is original_filename_n.extension, where n starts from 0.</p> PARAMETER DESCRIPTION <code>root_dir</code> <p>The root directory to search for images.</p> <p> TYPE: <code>str</code> </p> <code>target_dir</code> <p>The directory where found images will be stored.</p> <p> TYPE: <code>str</code> </p> <code>image_type</code> <p>The type of the images to search for (e.g., 'jpg', 'png', 'tiff').</p> <p> TYPE: <code>str</code> </p> Notes <p>The function creates the target directory if it does not already exist.</p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.file_analysis.find_outliers","title":"<code>find_outliers(dir1, dir2)</code>","text":"<p>Find image filenames that do not occur in both directories and check if both directories have the same number of images.</p> PARAMETER DESCRIPTION <code>dir1</code> <p>Path to the first directory containing images.</p> <p> TYPE: <code>str</code> </p> <code>dir2</code> <p>Path to the second directory containing images.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A DataFrame with columns: - 'Outlier Image': Names of the outlier images. - 'Directory': The directory in which the outlier was found. - 'Images in Dir1': The number of images in dir1. - 'Images in Dir2': The number of images in dir2.</p> <p> TYPE: <code>dataframe</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; find_outliers('/path/to/dir1', '/path/to/dir2')\n              Outlier Image         Directory  Images in Dir1  Images in Dir2\n0           image3.jpg      /path/to/dir1            NaN                 NaN\n1           image4.jpg      /path/to/dir2            NaN                 NaN\n2           NaN                 NaN                     5                   4\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.file_analysis.overlay_directory_images","title":"<code>overlay_directory_images(image_dir, mask_dir, alpha=0.5, start_index=0)</code>","text":"<p>Overlay binary masks on ultrasound images from given directories.</p> PARAMETER DESCRIPTION <code>image_dir</code> <p>Directory containing the ultrasound images.</p> <p> TYPE: <code>str</code> </p> <code>mask_dir</code> <p>Directory containing the corresponding binary masks.</p> <p> TYPE: <code>str</code> </p> <code>alpha</code> <p>Opacity level of the mask when overlaid on the ultrasound. Default is 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>start_index</code> <p>Index to start displaying the image/mask pairs. Default is 0 (first image).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>None</code> <p>Displays an interactive plot of overlaid image pairs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; overlay_directory_images('/path/to/ultrasound_images/', '/path/to/masks/', start_index=2)\n</code></pre>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.file_analysis.redact_images_in_directory","title":"<code>redact_images_in_directory(directory)</code>","text":"<p>Redacts the upper 50 pixels of every image in the specified directory by drawing a black rectangle over them. The images are saved under the same names in the same directory.</p> PARAMETER DESCRIPTION <code>directory</code> <p>The path to the directory containing the images to be redacted.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api_model_training/#DL_Track_US.gui_helpers.file_analysis.show_outliers_popup","title":"<code>show_outliers_popup(df, dir1, dir2)</code>","text":"<p>Display a pop-up window with a table showing outlier images between two directories.</p> PARAMETER DESCRIPTION <code>df</code> <p>DataFrame containing outlier information.</p> <p> TYPE: <code>DataFrame</code> </p> <code>dir1</code> <p>Path to the first directory containing images.</p> <p> TYPE: <code>str</code> </p> <code>dir2</code> <p>Path to the second directory containing images.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>None</code>"},{"location":"automated_image_analysis/","title":"\ud83d\uddbc Automated Image Analysis","text":"<p>On this page, you will learn how to perform automated image analysis with DL_Track_US. The images are evaluated without user input and can be scaled. Scaling ensures that estimated muscle architectural parameters are converted into centimetre units.</p> <p>Important: This type of analysis works only for single images, not videos. All images must be contained in a single folder, for example: <code>DL_Track_US_v0.3.0/images</code>.</p> <p>If you have not downloaded the example dataset yet, please do so now: DL_Track_US - Examples &amp; Models.</p> <p>\ud83d\udce6 Unzip the folder and save it somewhere easily accessible.</p>"},{"location":"automated_image_analysis/#1-creating-image-directory-flipflagtxt-file","title":"1. Creating Image Directory &amp; FlipFlag.txt File","text":"<ul> <li>All images you want to analyze must be placed in one folder.</li> <li>The folder <code>DL_Track_US_v0.3.0/images</code> contains 4 images and a flip_flag.txt file.</li> <li>It is not required to have the <code>flip_flag.txt</code> file in the same folder as the images, but it is convenient.</li> </ul> <p>Let's take a closer look at the <code>flip_flag.txt</code> file:</p> <ul> <li>For each image, there must be a corresponding flip-flag.</li> <li>The flip-flag determines whether an image should be flipped during analysis:  <ul> <li><code>0</code> = no flip  </li> <li><code>1</code> = flip the image</li> </ul> </li> <li>If the number of flip-flags and images does not match, an error will be raised.</li> </ul> <p></p> <p>When using multiple subfolders, the flip-flag file format looks like this:</p> <p></p> <ul> <li>None of the example images must be flipped \u2014 their fascicle orientation is already correct, with fascicles originating at the bottom left and inserting at the top right.</li> <li>Below is a visual representation of the correct fascicle orientation. If your fascicles are oriented differently, please specify a <code>1</code> flip-flag for those images.</li> </ul> <p></p>"},{"location":"automated_image_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<p>Once the GUI is opened, the first step of every analysis in DL_Track_US is to specify the input directories.</p> <ul> <li>First, specify the path to the folder containing the images to be analysed.   Remember, this was the folder <code>DL_Track_US_v0.3.0/images</code>.<ul> <li>Click the Inputs button. A selection window opens where you can select the folder.  </li> <li>Click Select folder to confirm your choice.</li> </ul> </li> </ul> <p> </p> <ul> <li>Second, specify the absolute path to the aponeurosis neural network.   This is located in the <code>DL_Track_US_v0.3.0/DL_Track_US_models</code> folder.<ul> <li>Click the Apo Model button to open a selection window.</li> <li>Select the aponeurosis model and click Open.</li> </ul> </li> </ul> <p> </p> <ul> <li>Third, specify the absolute path to the fascicle neural network (also located in <code>DL_Track_USv0.3.0/DL_Track_US_models</code>).<ul> <li>Click the Fasc Model button </li> <li>Select the VGG16 fascicle neural network file from <code>DL_Track_US_v0.3.0/models</code>.</li> <li>Confirm by clicking Open.</li> </ul> </li> </ul> <p> </p> <p>In the next step, you will specify all relevant analysis parameters, including the analysis type. We will explain what each parameter controls.</p>"},{"location":"automated_image_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":"<p>As a first step, select the correct analysis type in the GUI:</p> <ul> <li>Select image in the dropdown menu.</li> </ul> <p></p> <p>Next, you need to specify the Image Type.</p> <ul> <li>The ending of the Image Type must match the ending of your input images \u2014 otherwise no files will be found.</li> <li>You can either select a pre-specified ending from the dropdown list or type in your own.</li> <li>Please keep the formatting similar to the provided examples.</li> <li>In the <code>DL_Track_US_v0.3.0/images</code> folder, the images are of type \u201c/*.tif\u201d.</li> </ul> <p></p> <p>Then, specify the Scaling Type.</p> <ul> <li>Scaling converts measurements from pixels to centimetres.</li> <li>There are three scaling types available:</li> <li>None (no scaling),</li> <li>Bar (based on visible scaling bars),</li> <li>Manual (manual point selection).</li> </ul> <p>For this tutorial, select None.</p> <p></p> <p>If you choose the Bar Scaling Type:</p> <ul> <li>This method works if clear scaling bars are present in your ultrasound image (usually on the right side).</li> <li>The bars do not need to be identical to the example, but they must be clearly separated and visible.</li> </ul> <p></p> <ul> <li>We recommend testing the Bar Scaling Type on a small sample of your images first.</li> <li>Files that cannot be analysed with this scaling method will be listed in a <code>failed_images.txt</code> inside your input folder.</li> </ul> <p>If you choose the Manual Scaling Type:</p> <ul> <li>You must manually place two points on the image at a known distance.</li> <li>Click the Calibrate button to begin manual calibration.</li> </ul> <p></p> <ul> <li>Click once with the left mouse button to set the first point (a red dot will appear).</li> <li>Place the second point according to the real-world distance (5, 10, 15, or 20 mm).</li> <li>Afterwards, click Confirm.</li> </ul> <p></p> <ul> <li>A message box will appear showing the corresponding pixel distance.</li> </ul> <p></p> <p>Regardless of scaling type, whenever you use Bar or Manual scaling:</p> <ul> <li>You must also set the Spacing parameter.</li> <li>Select 5, 10, 15, or 20 millimetres from the dropdown.</li> </ul> <p>For this tutorial, the Spacing parameter is not necessary since scaling is set to None.</p> <p></p> <p>Another setting is the Filter Fascicle option:</p> <ul> <li>You can choose YES or NO.</li> <li>If set to YES, all fascicles that overlap are removed during post-processing.</li> </ul> <p></p> <p>Below you can see examples of the results with and without fascicle filtering:</p> <p></p> <p>As the next step, specify the path to the flip_flag.txt file:</p> <ul> <li>Click the Flip Flags button.</li> <li>Select the flip_flag.txt file, located in the <code>DL_Track_US_example/images</code> folder.</li> <li>Ensure that the number of flip flags matches the number of images.</li> </ul> <p> </p>"},{"location":"automated_image_analysis/#4-adjusting-settings","title":"4. Adjusting Settings","text":"<p>As a last step before starting the analysis, you need to adjust the settings used by the aponeurosis and fascicle neural networks.</p> <ul> <li> <p>Click the settings wheel button in the GUI. This will open the <code>settings.py</code> file in your default text editor.</p> </li> <li> <p>The <code>settings.txt</code> file contains a dictionary of all relevant analysis parameters.</p> </li> <li>Default values are listed, and each parameter is explained at the top of the file.</li> </ul> <p> </p> <p>Key parameters explained:</p> <ul> <li> <p>Aponeurosis detection threshold:   Minimum probability a pixel must reach to be classified as aponeurosis.   Lower values include more pixels.</p> </li> <li> <p>Aponeurosis length threshold:   Minimum or maximum length of structures detected as aponeuroses.</p> </li> <li> <p>Fascicle detection threshold and fascicle length threshold:   Same as above, but for fascicles.</p> </li> <li> <p>Minimal muscle width:   Minimum distance between superficial and deep aponeuroses.</p> </li> <li> <p>Minimal and Maximal Pennation angles:   Range of physiologically possible pennation angles for the muscle.</p> </li> <li> <p>Fascicle calculation method:   Method used to calculate fascicle length.   Options:</p> <ol> <li><code>linear_extrapolation</code></li> <li><code>curve_polyfitting</code></li> <li><code>curve_connect_linear</code></li> <li><code>curve_connect_poly</code></li> <li><code>orientation_map</code></li> </ol> </li> <li> <p>Fascicle contour tolerance:   Minimum acceptable fascicle length. Lower values allow shorter segments.</p> </li> <li> <p>Aponeurosis distance tolerance:   Maximal allowed distance from fascicles to aponeuroses for valid measurements.</p> </li> <li> <p>Selected filter:   Method used to filter outliers from fascicle length and pennation angle data.</p> </li> </ul> <p>Options:</p> <pre><code>1. `hampel` (default)\n2. `median`\n3. `gaussian`\n4. `savitzky_golay`\n</code></pre> <ul> <li> <p>Hampel window size:   Number of data points considered during outlier detection.</p> </li> <li> <p>Hampel number of deviations:   Number of standard deviations for outlier detection.</p> </li> <li> <p>Segmentation mode:  </p> </li> <li><code>none</code> \u2794 images are segmented separately  </li> <li><code>stacked</code> \u2794 three frames are stacked (used for videos only)</li> </ul> <p>\ud83d\udca1 Tip: Adapt these parameters according to your images or videos! Correct parameter settings can greatly impact your analysis results.</p> <p>For future analyses, always test parameter settings on a small sample of your data first. Especially, choose a pennation angle range that is physiologically plausible with a minimum range.</p> <p>If you accidentally disrupt the <code>settings.txt</code> file, there is a backup available: <code>_backup_settings.py</code> (only when downloading the repository).</p>"},{"location":"automated_image_analysis/#5-running-breaking-dl_track_us","title":"5. Running / Breaking DL_Track_US","text":"<p>After completing all setup steps, you are ready to start the analysis.</p> <ul> <li>Click the Run button in the main GUI window to begin processing.</li> <li>You will also see a Break button in the GUI.</li> <li>Clicking Break allows you to stop the analysis at any time.</li> <li>The currently processed image will be completed,</li> <li>Then the analysis will terminate..</li> </ul> <p></p> <p>Two output files will be created inside the <code>DL_Track_US_example/images</code> folder:</p> <ul> <li>ResultImages.pdf:</li> <li>A multi-page PDF.</li> <li> <p>Each page shows an input image with predicted fascicles and aponeuroses drawn over it.</p> </li> <li> <p>Results.xlsx:</p> </li> <li>An Excel file containing the estimated architectural parameters for each image.</li> <li>Each row corresponds to an input image.</li> <li>Median fascicle length, median pennation angle, and calculated muscle thickness are provided.</li> </ul> <p>\ud83d\udcc4 Important: The <code>ResultImages.pdf</code> can only be opened once the <code>Results.xlsx</code> has been fully generated.</p> <p></p> <p>\ud83c\udf89 Congratulations! You have now completed the DL_Track_US tutorial for automated image analysis!</p> <p>Before finishing, it is important to review the error handling procedures in case something unexpected occurs.</p>"},{"location":"automated_image_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>During analysis, DL_Track_US has robust error handling to ensure you can identify and correct problems easily.</p> <p>Whenever an error occurs:</p> <ul> <li>A message box will open inside the GUI.</li> <li>It will explain what went wrong and suggest how to fix it.</li> </ul> <p>Example of an error message:</p> <p></p> <p>We have tried to make all error messages as concise and informative as possible. Simply follow the instructions in the error box and restart the analysis after resolving the issue.</p> <p>\u26a0\ufe0f Note: If an unexpected error occurs that is not caught by a message box, please report it in the DL_Track_US Discussion Forum - Q&amp;A Section.</p> <p>When reporting an issue:</p> <ul> <li>Please include a description of the problem,</li> <li>Steps to reproduce the issue,</li> <li>And (if possible) screenshots of the GUI and error message.</li> </ul> <p>By following these guidelines, we can continuously improve DL_Track_US.</p>"},{"location":"automated_image_analysis_test/","title":"\ud83e\uddea Testing Automated Image Analysis","text":"<p>This page explains how to test the automated image analysis in DL_Track_US.</p>"},{"location":"automated_image_analysis_test/#test-preparation","title":"Test Preparation","text":"<p>Before starting:</p> <ul> <li>Images are evaluated without user input.</li> <li>Only single images (not videos) are required.</li> <li>Test data is located at: <code>DL_Track_US_example/tests/test_images_automatic</code></li> </ul> <p>Make sure the following are correct:</p> <ul> <li>Use the correct images: <code>DL_Track_US_example/tests/test_images_automated</code></li> <li>Use the provided pre-trained models: <code>DL_Track_US_example/DLTrack_models</code></li> <li>Keep all parameter settings in <code>settings.py</code> (accessible via the Settings Wheel) as they are.</li> <li>In v0.2.1, select NO for the Filter Fascicles option.</li> <li>Use the correct flip_flag.txt file: <code>DL_Track_US_example/tests/test_images_automated/flip_flags.txt</code></li> <li>Then click the Run button to start the analysis.</li> </ul>"},{"location":"automated_image_analysis_test/#_1","title":"Automated Image Analysis Test","text":""},{"location":"automated_image_analysis_test/#after-running-the-test","title":"After Running the Test","text":"<p>After running the analysis, two new files will be created:</p> <ul> <li>ResultImages.pdf </li> <li>Results.xlsx </li> </ul> <p>Both will appear in: <code>DL_Track_US_example/tests/test_images_automated</code></p>"},{"location":"automated_image_analysis_test/#validating-the-results","title":"Validating the Results","text":"<ul> <li>Open the Results.xlsx file.</li> <li>Compare the analysis results to the expected outputs shown below.</li> </ul> <p>\u2705 If the results are similar, DL_Track_US works properly for automated image analysis!</p>"},{"location":"automated_video_analysis/","title":"\ud83c\udfa5 Automated Video Analysis","text":"<p>This page introduces the automated video analysis in DL_Track_US.</p> <ul> <li>Videos are evaluated without user input.</li> <li>Videos must be contained in a single folder, e.g., <code>DL_Track_US_example/videos</code>.</li> </ul> <p>If you have not downloaded the example folder yet, please do so: DL_Track_US - Examples &amp; Models.</p> <p>\ud83d\udce6 Unzip the folder and save it somewhere easily accessible.</p> <p>The automated video analysis is very similar to automated image analysis: Only a few analysis parameters differ between the two types.</p> <p>After the analysis, a <code>proc.avi</code> file will be created in the input video directory. It can be opened with VLC Player (Windows) or OmniPlayer (macOS).</p>"},{"location":"automated_video_analysis/#1-creating-video-and-network-directories","title":"1. Creating Video and Network Directories","text":"<ul> <li>Videos should be stored in a single folder.</li> <li>The <code>DL_Track_US_example/videos</code> folder contains one video.</li> </ul>"},{"location":"automated_video_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<p>Once the GUI is open:</p> <ul> <li>Click the Inputs button to specify the folder containing your video.</li> <li>Select the <code>DL_Track_US_example/videos</code> folder and click Select folder.</li> </ul> <p></p> <p>Next, specify the aponeurosis model:</p> <ul> <li>Click the Apo Model button.</li> <li>Select the aponeurosis neural network file from <code>DL_Track_US_v0.3.0/models</code>.</li> <li>Click Open.</li> </ul> <p> </p> <p>Then, specify the fascicle model:</p> <ul> <li>Click the Fasc Model button.</li> <li>Select the IFSS fascicle neural network file from <code>DL_Track_US_v0.3.0/models</code>.</li> <li>Click Open.</li> </ul> <p> </p>"},{"location":"automated_video_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":""},{"location":"automated_video_analysis/#31-selecting-the-analysis-type","title":"3.1 Selecting the Analysis Type","text":"<ul> <li>Choose Video from the dropdown menu.</li> </ul>"},{"location":"automated_video_analysis/#32-setting-the-video-type","title":"3.2 Setting the Video Type","text":"<ul> <li>The file extension must match your videos (e.g., <code>.mp4</code>).</li> <li>Select or type /*.mp4.</li> </ul>"},{"location":"automated_video_analysis/#33-choosing-the-scaling-type","title":"3.3 Choosing the Scaling Type","text":"<ul> <li>Select None for this tutorial.</li> </ul> <p>Alternatively, you could use Manual scaling:</p> <ul> <li>Place two points on a known distance (5, 10, 15, or 20 mm).</li> <li>Click Calibrate.</li> </ul> <p>  After calibration, a messagebox shows the pixel distance:</p> <p></p>"},{"location":"automated_video_analysis/#34-filtering-fascicles","title":"3.4 Filtering Fascicles","text":"<ul> <li>Select YES to remove overlapping fascicles.</li> </ul> <p>Example difference between filtered and unfiltered:</p> <p></p>"},{"location":"automated_video_analysis/#35-setting-flip-options","title":"3.5 Setting Flip Options","text":"<ul> <li>Choose the appropriate flip setting:</li> <li>Flip to flip the video vertically,</li> <li>Don\u2019t Flip otherwise.</li> </ul> <p>For the example video, flipping is required to correct fascicle orientation.</p>"},{"location":"automated_video_analysis/#36-setting-frame-steps","title":"3.6 Setting Frame Steps","text":"<ul> <li>Set Frame Step to 1 (every frame analyzed).</li> <li>Larger steps (e.g., 3, 10) reduce computation time but skip frames.</li> </ul>"},{"location":"automated_video_analysis/#4-adjusting-settings","title":"4. Adjusting Settings","text":"<p>Open the settings by clicking the Settings Wheel.</p> <ul> <li>A txt script <code>settings.txt</code> opens in your default editor.</li> <li>Default values are listed.</li> <li>Make sure that \"segmentation_mode\" is set to \"stacked\".</li> </ul> <p> </p> <p>You can find an explanation on all setting in this chapter.</p> <p>\ud83d\udca1 Tip: Adapt these parameters according to your images or videos! Correct parameter settings can greatly impact your analysis results.</p> <p>For future analyses, always test parameter settings on a small sample of your data first. Especially, choose a pennation angle range that is physiologically plausible with a minimum range.</p> <p>If you accidentally disrupt the <code>settings.txt</code> file, there is a backup available: <code>_backup_settings.py</code> (only when downloading the repository).</p>"},{"location":"automated_video_analysis/#5-running-breaking-dl_track_us","title":"5. Running / Breaking DL_Track_US","text":"<ul> <li>Click the Run button to start analysis.</li> <li>Use the Break button if you need to stop the analysis.</li> </ul> <p>Once analysis completes, navigate back to <code>DL_Track_US_example/videos</code>.</p> <p>You will find two new files:</p> <ul> <li>calf_raise_proc.avi:</li> <li> <p>Video showing overlaid segmentation results.</p> </li> <li> <p>calf_raise.xlsx:</p> </li> <li>Excel file containing estimated muscle parameters (fascicle length, pennation angle, muscle thickness).</li> </ul> <p></p> <p>Line graph results include:</p> <ul> <li>Median Fascicle Length</li> <li>Median Filtered Fascicle Length</li> <li>Filtered Median Fascicle Length (based on chosen filter)</li> </ul> <p></p>"},{"location":"automated_video_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>If an error occurs:</p> <ul> <li>A messagebox will open to explain the issue.</li> </ul> <p></p> <p>We have tried to make all error messages as concise and informative as possible. Simply follow the instructions in the error box and restart the analysis after resolving the issue.</p> <p>\u26a0\ufe0f Note: If an unexpected error occurs that is not caught by a message box, please report it in the DL_Track_US Discussion Forum - Q&amp;A Section.</p> <p>When reporting an issue:</p> <ul> <li>Please include a description of the problem,</li> <li>Steps to reproduce the issue,</li> <li>And (if possible) screenshots of the GUI and error message.</li> </ul> <p>By following these guidelines, we can continuously improve DL_Track_US.</p>"},{"location":"automated_video_analysis_test/","title":"\ud83d\uddbc Testing Automated Video Analysis","text":"<p>This page explains how to test the automated video analysis functionality in DL_Track_US.</p> <ul> <li>Single video frames are evaluated automatically without user input.</li> <li>For this test, videos are required.</li> <li>The test video is located in: <code>DL_Track_US_example/tests/test_video_automated</code>.</li> </ul> <p>Before running the test, ensure that:</p> <ul> <li>You are using the correct video located at <code>DL_Track_US_example/tests/test_video_automated</code>.</li> <li>You are using the provided pre-trained models, located at <code>DL_Track_US_example/models/</code> (use IFSS for fascicles).</li> <li>You have kept the default parameter settings in the <code>settings.json</code> file unchanged.</li> <li>You click the Run button in the GUI to start the analysis.</li> </ul>"},{"location":"automated_video_analysis_test/#_1","title":"Automated Video Analysis Test","text":"<p>When the analysis is complete, two new files will appear in the <code>DL_Track_US_example/tests/test_video_automated</code> folder:</p> <ul> <li>calf_raise_proc.avi (processed video with predictions)</li> <li>calf_raise.xlsx (results file)</li> </ul>"},{"location":"automated_video_analysis_test/#how-to-verify-the-results","title":"How to verify the results:","text":"<ol> <li>Open the <code>calf_raise.xlsx</code> file.</li> <li>For all frames, calculate the average values for:</li> <li>Fascicle length</li> <li>Pennation angle</li> <li> <p>Muscle thickness</p> </li> <li> <p>Compare your results to the reference results shown below:</p> </li> </ol> <p></p> <p>\u2705 If your values are similar, DL_Track_US is working correctly for automated video analysis!</p>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#1-our-pledge","title":"1. Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our  community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and  expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and  orientation.</p>"},{"location":"code_of_conduct/#2-our-standards","title":"2. Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"code_of_conduct/#3-our-responsibilities","title":"3. Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior \u00a8 and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits,  issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any  contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#4-scope","title":"4. Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project  or its community in public spaces. Examples of representing a project or community include using an official project e-mail  address, posting via an official social media account, or acting as an appointed representative at an online or offline event.  Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#5-enforcement","title":"5. Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [paul.ritsche@unibas.ch].  All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances.  The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement  policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined  by other members of the project\u2019s leadership.</p>"},{"location":"code_of_conduct/#6-attribution","title":"6. Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant homepage, version 1.4, available at https://www.contributor-covenant.org/version/1/4/ code-of-conduct.html. For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We are happy whenever you decide to contribute to DL_Track_US. However, when contributing to DL_Track_US, please first file an issue in our Github issue section. Label the issue with \u201cimprovement\u201d and describe your suggestion. Please formulate the title of the issue similar to this: Contribution - Your contribution idea. Please also state in the issue whether you want to implement it yourself / already implemented it in your code or if you would like us to implement it. We will be in touch with you. Please note we have a code of conduct, please follow it in all your interactions with the project.</p> <p>In case you have decided to implement your suggestion yourself and we agreed that you should file a pull request, take a look at the steps listed below.</p>"},{"location":"contributing/#1-pull-request-process-for-contributing-own-material","title":"1. Pull Request Process for contributing own material","text":"<ol> <li>Update the DL_Track_US_tutorial.pdf in the docs folder with details of changes to the interface should they be relevant for the user. Simply add the step where it is during the analysis or create a new chapter.</li> <li>Add a changelog to the changelog.d folder describing exactly what you changed in the project and the environment.</li> <li>When adding new functions, please follow the code and docstring styles used throughout the code. FYI, we used the Numpy styleguide.</li> <li>Create a new branch (named yourchange_yourinitials) and a Pull Request to merge your work on the main branch of the project.</li> </ol>"},{"location":"contributing/#2-report-a-bug","title":"2. Report a bug","text":"<p>In order to report a bug, please file an issue in our issue section on Github. Label your issue with the \u201cbug\u201d label and describe the bug you found. Please formulate the title of the issue similar to this: Bugreport - Your bug that occured. Please describe the occurence of the bug as reproducible as possible. It\u2019s best to share with us the following in the issue:</p> <ul> <li>operating system</li> <li>the error raised by your code</li> <li>all steps to reproduce the bug</li> <li>code that produced the bug</li> </ul> <p>We will then be in touch with you and try to solve the problem as quickly as possible. Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"contributing/#3-getting-support","title":"3. Getting Support","text":"<p>If you have any questions about the project, encountered problems / need help during the installation procedure or encountered problems / need help during the usage of DL_Track_US_US not related to bugs, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum. This is the space to have conversations, ask questions and post answers without opening issues.</p>"},{"location":"crop_video/","title":"\u2702\ufe0f Crop Video \u2014 Cut Out Unnecessary Frames","text":"<p>In order to save time when analysing videos, you can cut out frames without muscle movement at the start and/or end of videos.</p>"},{"location":"crop_video/#1-opening-the-crop-video-tool","title":"1. Opening the Crop Video Tool","text":"<ul> <li>Once you have started the GUI and the main window opened, click on Advanced Methods.</li> <li>In the Select Method dropdown, select \"Crop Video\".</li> <li>A separate Crop Video Window will pop up.</li> </ul>"},{"location":"crop_video/#2-loading-the-video","title":"2. Loading the Video","text":"<ul> <li>Click the Load Video button.</li> <li>Select the video file you want to crop.</li> </ul> <p>After successfully loading the video, the UI should update.</p>"},{"location":"crop_video/#3-selecting-frames-to-keep","title":"3. Selecting Frames to Keep","text":"<ul> <li>Use the yellow slider to watch through the video and identify sections without muscle movement.</li> <li>Then, type in the desired Start Frame and End Frame.</li> </ul>"},{"location":"crop_video/#4-saving-the-cropped-video","title":"4. Saving the Cropped Video","text":"<ul> <li>Click the Browse button to select the output folder where the cropped video will be saved.</li> <li>Finally, click Crop Video to save the trimmed video.</li> </ul> <p>\ud83d\udca1 Tip: Cropping your videos to include only the relevant frames with visible muscle movement significantly reduces processing time and improves analysis efficiency!</p>"},{"location":"inspecting_masks/","title":"\ud83d\udd0d Inspecting Masks","text":"<p>Data quality is of utmost importance when labeling the images. In version 0.2.1 of DL_Track_US, we included an option to inspect labeled images and corresponding masks.</p>"},{"location":"inspecting_masks/#1-accessing-the-mask-inspection-tool","title":"1. Accessing the Mask Inspection Tool","text":"<ul> <li>Open the UI.</li> <li>In the main window, click on Advanced Methods.</li> <li>In the Select Method dropdown, choose \"Inspect Masks\".</li> <li>The Mask Inspection Window will open.</li> </ul>"},{"location":"inspecting_masks/#2-selecting-relevant-directories","title":"2. Selecting Relevant Directories","text":"<p>You need to specify three directories:</p> <ul> <li>\ud83d\udcc1 output_images \u2014 contains the original labeled images</li> <li>\ud83d\udcc1 fascicle_masks \u2014 contains fascicle masks</li> <li>\ud83d\udcc1 aponeurosis_masks \u2014 contains aponeurosis masks</li> </ul> <p>These folders should have been created during the Image Labeling process.</p> <p>Fascicle and aponeurosis masks must be inspected separately.</p>"},{"location":"inspecting_masks/#specify","title":"Specify:","text":"<ul> <li>Click Image Dir to select the <code>output_images</code> folder.</li> <li>Click Mask Dir to select the respective masks folder (either fascicle or aponeurosis masks).</li> <li>Use the Start Index to choose the starting image number.</li> </ul>"},{"location":"inspecting_masks/#3-starting-the-inspection","title":"3. Starting the Inspection","text":"<ul> <li>Click on Inspect Masks to start inspecting.</li> </ul> <p>One of two things will happen:</p>"},{"location":"inspecting_masks/#case-1-everything-matches","title":"Case 1: Everything Matches","text":"<ul> <li>Number of images and masks is equal.</li> <li>Naming conventions are correct.</li> <li>You will see a messagebox confirming everything is OK.</li> </ul> <p>Click OK to continue to the Mask Inspection GUI.</p>"},{"location":"inspecting_masks/#case-2-mismatch-detected","title":"Case 2: Mismatch Detected","text":"<ul> <li>Number of images and masks is not equal and/or</li> <li>Naming is incorrect.</li> </ul> <p>In this case:</p> <ul> <li>A table appears showing:</li> <li>Incorrect image names</li> <li>Which directory they occur in</li> <li>If the number of files differs</li> </ul> <p>\ud83d\udca1 Tip: Adjust the files according to the table, then restart the inspection.</p> <p></p>"},{"location":"inspecting_masks/#4-using-the-mask-inspection-ui","title":"4. Using the Mask Inspection UI","text":"<p>Once inspection starts:</p> <p></p> <p>In this UI:</p> <ul> <li>The original images are shown.</li> <li>The masks are overlaid in semi-transparent green.</li> <li>Follow the instructions displayed inside the GUI carefully.</li> </ul> <p>\ud83d\uded1 Delete Warning: Clicking the Delete button will permanently delete the selected image-mask pair!</p> <p>We recommend making backups of the folders before starting inspection, especially if you might want to correct masks later.</p>"},{"location":"inspecting_masks/#you-can-now-inspect-validate-and-clean-your-datasets","title":"\u2705 You can now inspect, validate, and clean your datasets.","text":""},{"location":"installation/","title":"\ud83d\ude80 Installation Guide","text":"<p>DL_Track_US is designed with ease-of-use in mind. Whether you're a developer or completely new to coding, getting started is simple.</p> <p>We provide two ways to install DL_Track_US:</p>"},{"location":"installation/#option-1-download-the-installer-recommended-for-beginners-windows-only","title":"Option 1: Download the Installer (Recommended for Beginners &amp; Windows only)","text":"<p>No programming required</p> <ol> <li>Visit our OSF project page and anvigate to `Files/v0.3.0/\u00b4</li> <li>Download and unzip the file: <code>DL_Track_US_example0.3.0.zip</code></li> <li>Inside the unzipped folder, open: <code>DL_Track_US_example/DL_Track_US_Installer/DLTrackUS_Installer_Windows.exe</code></li> <li>Follow the instructions in the installation wizard.</li> <li>Double-click the desktop icon to launch the DL_Track_US GUI</li> </ol> <p>If you want to change the analysis settings, run DL_Track_US as administrator.</p> <p>You can now test the app using the provided example files. Check our Tutorial and Testing sections for more.</p> <p>\u26a0\ufe0f Note: The DL_Track_US installer is available for Windows only. We are working on a MacOS version.</p> <p>\u26a0\ufe0f You might see a warning from your antivirus software. You can safely dismiss it, this app is secure and verified.</p>"},{"location":"installation/#option-2-install-via-pip-and-github-recommended-for-developers-macos","title":"Option 2: Install via pip and GitHub (Recommended for Developers &amp; MacOS)","text":"<p>Ideal for customization, development, or contributing</p>"},{"location":"installation/#step-1-install-anaconda","title":"Step 1 - Install Anaconda","text":"<ul> <li>Download and install Anaconda</li> <li>Be sure to check the box: Add Anaconda to my PATH environment variable</li> </ul>"},{"location":"installation/#step-2-create-a-virtual-environment","title":"Step 2 - Create a virtual environment","text":"<pre><code>conda create -n DL_Track_US0.3.0 python=3.10\nconda activate DL_Track_US0.3.0\n</code></pre>"},{"location":"installation/#step-3-install-dl_track_us","title":"Step 3 - Install DL_Track_US","text":""},{"location":"installation/#windows-users","title":"Windows users:","text":"<pre><code>pip install DL-Track-US==0.3.0\n</code></pre>"},{"location":"installation/#macos-users","title":"MacOS users:","text":"<ol> <li>Download the repo or just the <code>requirements.txt</code> file from GitHub</li> <li>Navigate to the folder where <code>requirements.txt</code> is located:</li> </ol> <pre><code>cd path/to/DL_Track_US\n</code></pre> <ol> <li>Install dependencies and the package:</li> </ol> <pre><code>pip install -r requirements.txt\npython -m pip install -e .\n</code></pre> <p>\u26a0\ufe0f Note: DL_Track_US was tested on Windows 10 and 11 as well as M1/M2 Macs.</p>"},{"location":"installation/#step-4-start-the-dl_track_us-gui","title":"Step 4 - Start the DL_Track_US GUI","text":"<p>You have two options:</p>"},{"location":"installation/#option-a-from-the-installed-package","title":"Option A From the installed package:","text":"<pre><code>python -m DL_Track_US\n</code></pre>"},{"location":"installation/#option-b-from-the-cloned-repository","title":"Option B From the cloned repository:","text":"<pre><code>cd DL_Track_US/DL_Track_US\npython DL_Track_US_GUI.py\n</code></pre>"},{"location":"installation/#optional-gpu-setup-for-faster-inference","title":"Optional: GPU Setup for Faster Inference","text":"<p>For Windows/NVIDIA users:</p> <ol> <li>Install NVIDIA GPU drivers</li> <li>Download:</li> <li>CUDA 11.2</li> <li>cuDNN 8.5</li> <li>Follow this video tutorial (minutes 9-13)</li> </ol> <p>For Mac (M1/M2) users:</p> <ul> <li>Follow this Apple Silicon TensorFlow guide for optional GPU support</li> </ul>"},{"location":"installation/#need-help","title":"Need Help?","text":"<ul> <li>Visit the DL_Track_US Q&amp;A on GitHub</li> <li>Tag your post with Problem</li> <li>Attach screenshots or logs if possible</li> </ul> <p>We're happy to help you get up and running!</p>"},{"location":"manual_image_analysis/","title":"\ud83d\uddbc Manual Image Analysis","text":"<p>This page covers manual image analysis in DL_Track_US. In this mode, images are evaluated manually by drawing muscle thickness, fascicle length, and pennation angles directly onto the images.</p> <p>Important: Manual analysis is applicable only for single images (not videos). All images must be contained in a single folder, e.g., <code>DL_Track_US_example/images_manual</code>.</p> <p>If you have not downloaded the example folder yet, please do so: DL_Track_US - Examples &amp; Models.</p> <p>\ud83d\udce6 Unzip the folder and save it somewhere easily accessible.</p>"},{"location":"manual_image_analysis/#1-creating-image-directory","title":"1. Creating Image Directory","text":"<ul> <li>Place all images to be analyzed into a single folder.</li> <li>The <code>DL_Track_US_example/images_manual</code> folder contains 2 images.</li> </ul> <ul> <li>Unlike automated analysis, you do not need a <code>flip_flag.txt</code> file or neural networks.</li> <li>In manual analysis, you are the \"neural network.\"</li> </ul>"},{"location":"manual_image_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<ul> <li>Click the Inputs button in the GUI to open a selection window.</li> <li>Choose the <code>DL_Track_US_example/images_manual</code> folder.</li> <li>Click Select folder to confirm.</li> </ul>"},{"location":"manual_image_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":"<ul> <li>Select image_manual from the dropdown menu.</li> </ul> <p>Then specify the Image Type:</p> <ul> <li>The file extension must match your images (e.g., <code>.tif</code>).</li> <li>Either select it from the dropdown or type it manually.</li> <li>For this tutorial, select /*.tif.</li> </ul> <p></p> <ul> <li>After setting the image type, click Run to start the manual analysis.</li> </ul> <p></p>"},{"location":"manual_image_analysis/#4-manual-analysis-of-image","title":"4. Manual Analysis of Image","text":"<p>After clicking Run, the Manual Analysis window opens:</p> <p></p>"},{"location":"manual_image_analysis/#important-rules","title":"Important rules:","text":"<ul> <li>The start and end points of each line are critical \u2014 not the line itself.</li> <li>Start drawing by pressing the left mouse button; end by releasing it.</li> <li>Avoid any unwanted clicks!   If extra clicks happen, restart the current analysis step.</li> </ul>"},{"location":"manual_image_analysis/#41-manual-scaling","title":"4.1 Manual Scaling","text":"<ul> <li>Select Scale Image in the Manual Analysis window.</li> <li>Draw a 1-centimetre straight line based on scaling bars in the image.</li> <li>A messagebox will guide you.</li> </ul> <p>Example of the drawn line:</p> <p></p>"},{"location":"manual_image_analysis/#42-drawing-aponeuroses","title":"4.2 Drawing Aponeuroses","text":"<ul> <li>Select Draw Aponeurosis to manually extend aponeuroses.</li> <li>A messagebox will instruct you.</li> </ul>"},{"location":"manual_image_analysis/#43-measuring-muscle-thickness","title":"4.3 Measuring Muscle Thickness","text":"<ul> <li>Select Muscle Thickness.</li> <li>Draw three straight lines from superficial to deep aponeurosis across the muscle image.</li> </ul>"},{"location":"manual_image_analysis/#44-drawing-fascicles","title":"4.4 Drawing Fascicles","text":"<ul> <li>Select Muscle Fascicles.</li> <li>Draw at least three fascicles in different regions.</li> <li>Each fascicle must have three segments:</li> <li>Each segment must start where the previous segment ended.</li> <li>Avoid extra mouse clicks.</li> </ul>"},{"location":"manual_image_analysis/#45-measuring-pennation-angles","title":"4.5 Measuring Pennation Angles","text":"<ul> <li>Select Pennation Angle.</li> <li>Draw at least three pennation angles:</li> <li>Each must have two segments:<ol> <li>Along the fascicle</li> <li>Along the deep aponeurosis</li> </ol> </li> </ul>"},{"location":"manual_image_analysis/#5-saving-breaking-next-image","title":"5. Saving / Breaking / Next Image","text":""},{"location":"manual_image_analysis/#51-saving-results","title":"5.1 Saving Results","text":"<ul> <li>Press the Save Results button after finishing each image.</li> <li>It saves:</li> <li>An Excel file (<code>Manual_Results.xlsx</code>)</li> <li>A screenshot of your drawing.</li> </ul> <p>Saved results are stored in <code>DL_Track_US_example/images_manual</code>.</p> <p></p>"},{"location":"manual_image_analysis/#52-going-to-next-image","title":"5.2 Going to Next Image","text":"<ul> <li>Click the Next Image button to proceed.</li> <li>Always save results first before moving to the next image!</li> </ul>"},{"location":"manual_image_analysis/#53-breaking-analysis","title":"5.3 Breaking Analysis","text":"<ul> <li>Click Break Analysis to terminate analysis.</li> <li>A messagebox will confirm your choice.</li> <li>After confirming, you return to the main GUI.</li> </ul> <p>After saving all results, your folder should contain:</p> <ul> <li>Input images</li> <li>Saved screenshots</li> <li>The Manual_Results.xlsx file</li> </ul> <p></p>"},{"location":"manual_image_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>If any error occurs:</p> <ul> <li>A messagebox will open explaining the issue.</li> </ul> <p></p> <p>We have tried to make all error messages as concise and informative as possible. Simply follow the instructions in the error box and restart the analysis after resolving the issue.</p> <p>\ud83d\udcac Note: If an unexpected error occurs that is not caught by a message box, please report it in the DL_Track_US Discussion Forum - Q&amp;A Section.</p> <p>When reporting an issue:</p> <ul> <li>Please include a description of the problem,</li> <li>Steps to reproduce the issue,</li> <li>And (if possible) screenshots of the GUI and error message.</li> </ul> <p>By following these guidelines, we can continuously improve DL_Track_US.</p>"},{"location":"manual_image_video_analysis_test/","title":"\ud83e\uddea Testing Manual Image / Video Analysis","text":"<p>This page explains how to test the manual image and manual video analysis modes.</p> <ul> <li>For this test, single images (not videos) are a prerequisite.</li> <li>The test image you must use is located in the <code>DL_Track_US_example/tests/test_images_manual</code> folder.</li> <li>Both manual image analysis and manual video analysis use the same Python class (<code>ManualAnalysis</code> located in <code>manual_tracing.py</code>).</li> <li>Therefore, testing one (manual image analysis) is sufficient.</li> </ul>"},{"location":"manual_image_video_analysis_test/#test-preparation","title":"Test Preparation","text":"<p>Before starting:</p> <ul> <li>Make sure to use the correct image: <code>DL_Track_US_example/tests/test_image_manual</code>.</li> <li>Click the Run button to start the analysis.</li> </ul> <p></p>"},{"location":"manual_image_video_analysis_test/#running-the-test","title":"Running the Test","text":"<p>After clicking Run:</p> <ul> <li>The Manual Analysis window should pop up containing the test image.</li> </ul> <p></p>"},{"location":"manual_image_video_analysis_test/#reanalysing-the-test-image","title":"Reanalysing the Test Image","text":"<p>Follow these steps:</p> <ol> <li>Scale the image </li> <li>Follow the one-centimetre long scaling line shown on the left of the image.  </li> <li> <p>Scale the image accordingly.</p> </li> <li> <p>Draw Aponeuroses </p> </li> <li> <p>Redraw the superficial and deep aponeurosis extension lines.</p> </li> <li> <p>Measure Muscle Thickness </p> </li> <li> <p>Redraw the three vertical muscle thickness lines using one segment each.</p> </li> <li> <p>Trace Fascicles </p> </li> <li> <p>Redraw the three diagonal fascicle lines using three segments each.</p> </li> <li> <p>Measure Pennation Angles </p> </li> <li>Redraw the three pennation angles using two segments each.</li> </ol> <p>\u26a1 Important: Always select the correct Radiobutton corresponding to the parameter you are analyzing.</p> <p>After reanalyzing all lines:</p> <ul> <li>Click the Save Results button to save your analysis.</li> </ul> <p>A new file will be created:</p> <ul> <li><code>Manual_Results.xlsx</code> inside the <code>DL_Track_US_example/tests/test_image_manual</code> folder.</li> </ul>"},{"location":"manual_image_video_analysis_test/#validating-the-results","title":"Validating the Results","text":"<ul> <li>Open the newly created <code>Manual_Results.xlsx</code> file.</li> <li>Compare the analysis results to the expected results shown below.</li> </ul> <p>If the results are similar, DL_Track_US works properly for manual image and video analysis!</p> <p></p>"},{"location":"manual_video_analysis/","title":"\ud83c\udfa5 Manual Video Analysis","text":"<p>The next and last analysis type this tutorial covers is manual video analysis. In this mode, video frames are evaluated manually by drawing muscle thickness, fascicle length, and pennation angles directly onto the images.</p> <p>Important: Manual video analysis is applicable only for single videos. All videos must be contained in a single folder, e.g., <code>DL_Track_US_example/videos_manual</code>.</p> <p>If you have not downloaded the example folder yet, please do so: DL_Track_US - Examples &amp; Models.</p> <p>\ud83d\udce6 Unzip the folder and save it somewhere easily accessible.</p> <p>The manual video analysis type is identical to manual image analysis. The only difference: You specify the absolute video path instead of a file type.</p> <ul> <li>The video is first converted into individual frames.</li> <li>Each frame image is then analyzed separately.</li> </ul>"},{"location":"manual_video_analysis/#1-creating-a-video-directory","title":"1. Creating a Video Directory","text":"<ul> <li>Place all videos into a single folder.</li> <li>The <code>DL_Track_US_example/video_manual</code> folder contains one video file.</li> </ul>"},{"location":"manual_video_analysis/#2-specifying-input-directory-in-the-gui","title":"2. Specifying Input Directory in the GUI","text":"<ul> <li>Select video_manual from the dropdown menu.</li> </ul> <p>Next, specify the absolute file path of the video file to be analyzed:</p> <ul> <li>The example video file is placed in the <code>DL_Track_US_example/video_manual</code> folder.</li> <li>Click the Video Path button in the GUI to open a selection window.</li> <li>Select the video file.</li> <li>Click Open to confirm.</li> </ul> <p></p> <p>Finally, start the analysis:</p> <ul> <li>Click the Run button in the main GUI.</li> </ul> <p></p> <p>Once you click Run:</p> <ul> <li>The Manual Analysis window will open.</li> <li>From here, all further steps are identical to manual image analysis.</li> </ul> <p>Differences to Manual Image Analysis:</p> <ul> <li>A new folder is created next to your input video.</li> <li>This new folder contains all extracted single frame images.</li> </ul> <p>All operations \u2014 scaling, aponeurosis drawing, muscle thickness measurement, fascicle measurement, pennation angle measurement \u2014 work exactly as in manual image analysis.</p> <p>Saving results, moving to the next frame, terminating analysis, and error handling are also identical.</p> <p>\ud83d\udcda For detailed instructions, please refer to the Manual Image Analysis page.</p>"},{"location":"model_training_test/","title":"\ud83e\uddea Testing Model Training","text":"<p>This page explains how to test the model training using the DL_Track_US GUI.</p>"},{"location":"model_training_test/#1-before-you-start","title":"1. Before You Start","text":"<ul> <li>A working GPU is highly recommended; otherwise, model training will take significantly longer.</li> <li> <p>Instructions to setup the GUI and environment are found in the Installation Guidelines.</p> </li> <li> <p>The test training images and masks you need are located in:</p> </li> <li><code>DL_Track_US_example/tests/model_training/</code></li> </ul>"},{"location":"model_training_test/#2-important-setup-instructions","title":"2. Important Setup Instructions","text":"<p>For this test, ensure the following:</p> <ul> <li>Click on Advanced Methods and select \"Train Model\" in the dropdown menu.</li> <li>Ignore the main GUI window for now \u2014 you will only use the Model Training window.</li> <li>Use the correct training images:</li> <li><code>DL_Track_US_example/tests/model_training/apo_img_example</code></li> <li>Use the correct training masks:</li> <li><code>DL_Track_US_example/tests/model_training/apo_mask_example</code></li> <li>Keep the parameter settings exactly as shown.</li> <li>Critical: Set the number of Epochs to 3 (for quick test training).</li> </ul> <p></p>"},{"location":"model_training_test/#3-starting-the-training","title":"3. Starting the Training","text":"<ul> <li> <p>After setting all parameters, click Start Training.</p> </li> <li> <p>During the process, you will encounter several messageboxes:</p> </li> <li>Confirm each by clicking OK.</li> <li>These confirm that:<ul> <li>Images and masks have been loaded.</li> <li>Model compilation was successful.</li> <li>Training completed successfully.</li> </ul> </li> </ul>"},{"location":"model_training_test/#4-after-training","title":"4. After Training","text":"<p>Once training finishes, you should find three new files in your selected output folder:</p> <ul> <li>\ud83d\udcc4 Test_apo.xlsx \u2014 Training summary file</li> <li>\ud83d\udcc4 Test_apo.h5 \u2014 The trained model</li> <li>\ud83d\udcc4 Training_results.tif \u2014 A plot of the loss curve over epochs</li> </ul> <p>\u26a0\ufe0f Note: Because neural network training includes uncertainty, your results (e.g., final loss values) may slightly differ from ours.  </p> <p>If the three files are generated correctly, it means your DL_Track_US installation works properly for model training!</p>"},{"location":"news/","title":"\ud83d\udcf0 News","text":""},{"location":"news/#v030-release-notes","title":"v0.3.0 Release Notes","text":"<ul> <li>Released version 0.3.0 with major upgrades and bugfixes!</li> <li>New features: manual scaling tool, resize Video tool, crop video length tool &amp; remove video parts tool.</li> <li>New installer for Windows.</li> <li>Faster model predictions &amp; optional stacked (sequential) predictions.</li> <li>Improved user interface with visualization of model predictions and filtering/plotting of results.</li> <li>Automatic <code>settings.json</code> in GUI for easy switching of model parameters.</li> <li>Filtering of fascicle length and pennation angle data using hampel sand savgol filters.</li> </ul> <p>\ud83d\udc49 See the Full Changelog on GitHub</p>"},{"location":"news/#faster-model-predicitions-on-gpu-cpu","title":"Faster model predicitions on GPU &amp; CPU","text":"<p>In version 0.3.0, we reduced processing time per frame by 40% from version 0.2.1 (RTX4080 &amp; Intel i9), respectively.</p>"},{"location":"news/#improved-user-interface","title":"Improved user interface","text":"<p>In version 0.3.0, we improved the user interface and included real time visualization of model predictions as well as a results terminal at the end of analyis. The analysis process is now more transparent and felxibel, since we included more analysis options in the settings. </p> <p></p>"},{"location":"news/#video-analysis","title":"Video Analysis","text":""},{"location":"news/#new-model-with-bi-directional-short-long-term-memory-for-video-analysis","title":"New model with bi-directional short long term memory for video analysis","text":"<p>We further provide a new model with a new overall aproach for fascicle anaylsis in videos. For the first time, we provide a model with memory and awareness of surrounding frames. The model is taken from Chanti et al. (2021) and is called IFSS-NET. </p> <p>In our approach, we use a bi-directional short long term memory (BiLSTM) to capture the temporal context of the video. We excluded the siamese encoder from the orginal model. Furhtermore, we used a hybrid loss combination of the Dice loss and binary cross entropy loss, both weighted equally. </p> <p>To reach this decision, we compared different models and their performance compared to a manual ground thruth and a kalman-filter based tracking apporach (UltraTimTrack) proposed by van der Zee et al. (2025). </p>"},{"location":"news/#model-training-results","title":"Model Training results","text":"<p>We compard our previous vgg16unet model (Ritsche et al. (2024)) to SegFormer, uNet3+ and IFSS-Net architectures. The Results on a unseen test set of 120 images with examplary predictions can be seen below. </p> <p></p> <p>Moreover, we compared the models due to similar performance to the one of the validation videos from the original paper (Ritsche et al. (2024)). This video was recently used to compare the performance of different methods for fascicle tracking (van der Zee et al. (2025)). We demonstrate improvement in the results from DL_Track_US in terms of RMSD compared to manual annotation as displayed below. Of all networks, the IFSS-Net model performed best in a trade-off between pennation angle and fascicle length RMSD. </p> <p> </p> <p>Note that, compared to v0.2.1, we introduced hampel-filtering of the fascicle values in each frame and additionally applied a savitzky-golay filter to the median fascicle data to furhter reduce root mean squared distance. The results for two different tasks are displayed below. </p>"},{"location":"news/#calf-raise","title":"Calf Raise","text":""},{"location":"news/#vl-fixed-end-maximal-knee-extentsion","title":"VL fixed end maximal knee extentsion","text":""},{"location":"news/#benchmarking","title":"Benchmarking","text":"<p>Benchmark results for the IFSS-Net model will be published soon.</p> <p>\ud83d\udea8 More comparsions will follow in the upcoming publication.</p>"},{"location":"news/#image-analysis","title":"Image Analysis","text":""},{"location":"news/#benchmarking_1","title":"Benchmarking","text":"<p>We compared the results of our VGG16Unet models to the results of the benchmark dataset from UMUD (Ritsche et al. (2025)). We used the standard settings. The results were calcualted on a RTX4080 GPU and Intel i9 CPU. Better rsults can be achieved by adapting the analysis settings. The benchmark results can be viewed in the OSF repository.</p> Muscle Thickness (mm) Fascicle length (mm) Pennation angle (\u00b0) MEAN 0.126911 3.874385 -0.407885 STD 3.339651 4.581032 1.942850 <p>\ud83d\udea8 We are currently working on implementing tracking of fascicles accounting for their curvature.</p>"},{"location":"remove_video_parts/","title":"\u2702\ufe0f Remove Video Parts","text":"<p>In DL_Track_US you can remove parts of a video to:</p> <ul> <li>Improve segmentation accuracy:   Sometimes the superficial aponeurosis can be wrongly segmented if the skin or subcutaneous tissue is thick or highly echogenic.   Cropping out the upper regions can avoid these errors.</li> <li>Support data anonymization:   Sensitive or identifying areas can be cropped to protect subject privacy.</li> </ul> <p>\u2705 This is especially useful before running automated segmentation.</p>"},{"location":"remove_video_parts/#1-accessing-the-remove-video-parts-tool","title":"1. Accessing the Remove Video Parts Tool","text":"<ul> <li>Once the GUI is open, click on the Advanced Methods button.</li> <li>In the Select Method dropdown, select \u201cCrop Video\u201d.</li> <li>A separate Crop Video Window will pop up.</li> </ul>"},{"location":"remove_video_parts/#2-loading-the-video","title":"2. Loading the Video","text":"<ul> <li>Click the Load Video button.</li> <li>Select the video file you want to crop.</li> </ul> <p>After successfully loading the video, the UI will look like this:</p> <ul> <li>Use the yellow slider to scroll through the video frames.</li> <li>Click and drag with the left mouse button to select the part to keep.</li> </ul> <p></p> <p>Your result (after re-loading) will look like this:</p> <p></p>"},{"location":"remove_video_parts/#3-saving-the-video","title":"3. Saving the Video","text":"<ul> <li>Click the Browse button to select the output folder where the cropped video will be saved.</li> <li>Finally, click Remove Parts to save the cropped video.</li> </ul> <p>\u2705 That's it! Now your video is ready for analysis or anonymized storage.</p>"},{"location":"resize_video/","title":"\u2702\ufe0f Resize Video","text":"<p>In DL_Track_US you can resize a video so that only the selected region remains:</p> <ul> <li>Improve segmentation accuracy:    Resizing the video to focus on the muscle region can improve model performance by cropping out irrelevant background information.</li> <li>Support data anonymization:   Sensitive areas (e.g., patient IDs or other identifying marks) can be removed entirely by resizing.</li> </ul> <p>\u2705 This is especially useful before running automated segmentation!</p>"},{"location":"resize_video/#1-accessing-loading","title":"1. Accessing / Loading","text":"<ul> <li>Once the GUI is open, click on the Advanced Methods button.</li> <li>In the Select Method dropdown, select Resize Video.</li> <li>A separate Resize Video Window will pop up.</li> </ul> <p>This process is similar to the remove video parts function. Have a look there if you are uncertain.</p>"},{"location":"resize_video/#2-loading-the-video","title":"2. Loading the Video","text":"<ul> <li>Click the Load Video button.</li> <li>Select the video file you want to resize.</li> </ul> <p>After successfully loading the video:</p> <ul> <li>Use the yellow slider to scroll through the video frames.</li> <li>Click and drag with the left mouse button to select the part you want to keep.</li> </ul> <p></p> <p>Your result (after re-loading) will look like this:</p> <p></p>"},{"location":"resize_video/#3-saving-the-resized-video","title":"3. Saving the Resized Video","text":"<ul> <li>Click the Browse button to select the output folder.</li> <li>Then click Resize Video to crop and save the video.</li> </ul> <p>Again, the process is similar to the remove video parts function. Have a look there if you are uncertain.</p> <p>\ud83e\udde0 Tip: Resizing the video can speed up the analysis, reduce errors, and protect privacy \u2014 especially in clinical settings or publications. However, we did not include resized images during the training process.</p>"},{"location":"training_your_own_networks/","title":"\ud83e\udde0 Training Your Own Networks","text":"<p>The DL_Track_US package GUI includes the possibility to train your own neural networks.</p>"},{"location":"training_your_own_networks/#why-train-your-own-model","title":"Why train your own model?","text":"<ul> <li>To create models tailored to your own dataset.</li> <li>To improve segmentation if the example models don't generalize well enough.</li> <li>To learn more about deep learning for muscle ultrasound.</li> </ul> <p>\ud83d\udea8 It\u2019s highly recommended to have a working GPU setup; otherwise training can take much longer. Check out our GitHub repository for setup instructions.</p> <p>If you're new to neural networks, we recommend this introduction course.</p> <p>\ud83d\udcdd Note: DL_Track_US UI allows training but not modifying network architectures!</p> <p>The paired images and labeled masks needed for training are located in: \ud83d\udcc1 <code>DL_Track_US_example/model_training</code></p> <p>Download DL_Track_US Examples &amp; Models if you haven\u2019t already.</p> <p>In this tutorial, we will train a model for aponeurosis segmentation. Training a fascicle segmentation model is identical \u2014 only the images and masks would differ.</p>"},{"location":"training_your_own_networks/#1-data-preparation-and-image-labeling","title":"1. Data Preparation and Image Labeling","text":"<p>Inside the <code>DL_Track_US_example/model_training</code> folder:</p> <ul> <li>\ud83d\udcc1 apo_img_example \u2192 Original images  </li> <li>\ud83d\udcc1 apo_mask_example \u2192 Corresponding labeled masks</li> </ul> <p></p> <p>\u26a1 IMPORTANT: Image names and mask names must match exactly!</p> <p>Example:</p> <p></p>"},{"location":"training_your_own_networks/#2-specifying-relevant-directories","title":"2. Specifying Relevant Directories","text":"<ul> <li>Open the UI.</li> <li>Click the Advanced Methods button.</li> <li>In the dropdown, select Train Model.</li> </ul> <p>Now specify the directories:</p>"},{"location":"training_your_own_networks/#select-image-directory","title":"Select Image Directory","text":"<ul> <li>Click Images.</li> <li>Select <code>DL_Track_US_example/model_training/apo_img_example</code>.</li> </ul>"},{"location":"training_your_own_networks/#select-mask-directory","title":"Select Mask Directory","text":"<ul> <li>Click Masks.</li> <li>Select <code>DL_Track_US_example/model_training/apo_mask_example</code>.</li> </ul>"},{"location":"training_your_own_networks/#select-output-directory","title":"Select Output Directory","text":"<ul> <li>Click Output.</li> <li>Choose a folder to save the trained model, loss plots, and CSV results.</li> </ul>"},{"location":"training_your_own_networks/#3-image-augmentation-optional-but-recommended","title":"3. Image Augmentation (Optional but Recommended)","text":"<p>Image augmentation artificially increases your dataset size by applying random transformations.</p> <p>\ud83d\udea8 Especially recommended if you have fewer than 1500 images.</p> <ul> <li>Click Augment Images.</li> </ul> <p></p> <p>A messagebox will notify you once augmentation is complete.</p>"},{"location":"training_your_own_networks/#4-specifying-training-parameters","title":"4. Specifying Training Parameters","text":"<ul> <li>Keep the default settings for this tutorial.</li> <li>NEVER use just 3 epochs for real training.   (3 epochs are okay only for testing.)</li> </ul> <p>Now click:</p> <ul> <li>Start Training</li> </ul> <p></p> <p>Three messageboxes will guide you during the training process.</p> <p>Once training is finished, you\u2019ll find:</p> <ul> <li>Trained model (<code>Test_Apo.h5</code>)</li> <li>Training loss plot (<code>Training_Results.tif</code>)</li> <li>Loss values per epoch (<code>Test_apo.csv</code>)</li> </ul>"},{"location":"training_your_own_networks/#5-using-your-own-networks","title":"5. Using Your Own Networks","text":"<p>You can use your trained models like this:</p> <ul> <li>Click Apo Model or Fasc Model in the GUI to load your trained model.</li> </ul> <p></p> <p>\u26a1 IMPORTANT: Never use the same images for training and inference. Always validate on unseen data to check your model's performance.</p> <p>If unsure, feel free to ask in our DL_Track_US Discussion Forum!</p>"},{"location":"training_your_own_networks/#6-error-handling","title":"6. Error Handling","text":"<p>Errors during training trigger a messagebox:</p> <p></p> <p>Follow the instructions shown. Uncaught errors should be reported in the DL_Track_US Discussion Forum.</p> <p>See here for guidance on how to best report errors.</p>"},{"location":"training_your_own_networks/#7-labeling-your-own-images","title":"7. Labeling Your Own Images","text":"<p>To train your networks, you must label images correctly.</p> <p>\ud83d\udee0 We provide a semi-automated script!</p> <p>You need:</p> <ul> <li>\ud83d\udcc1 Original images folder</li> <li>\ud83d\udcc1 <code>output_images</code> folder</li> <li>\ud83d\udcc1 <code>fascicle_masks</code> folder</li> <li>\ud83d\udcc1 <code>aponeurosis_masks</code> folder</li> </ul> <p>You\u2019ll use ImageJ/Fiji and our script: \ud83d\uddc2 <code>DL_Track_US/DL_Track_US/gui_helpers/gui_files/Image_Labeling_DL_Track_US.ijm</code></p> <p>Drag the <code>.ijm</code> file into a running Fiji/ImageJ window to start.</p> <p> </p>"},{"location":"training_your_own_networks/#labeling-steps","title":"Labeling Steps","text":"<ol> <li> <p>Set Directories:    a. Input images    b. Aponeurosis masks    c. Fascicle masks    d. Output images</p> </li> <li> <p>Label Aponeuroses:    Use polygon tool to select superficial and then deep aponeurosis.</p> <p> </p> </li> <li> <p>Label Fascicles:    Use segmented line tool for clearly visible fascicle parts only.</p> <p></p> </li> <li> <p>Save and Move to Next Image.</p> </li> </ol> <p>\u2705 You are now ready to create your own high-quality training datasets!</p>"}]}