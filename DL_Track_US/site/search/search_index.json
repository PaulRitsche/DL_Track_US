{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to DL_Track_US","text":""},{"location":"#overview","title":"Overview","text":"<p>Automated analysis of human lower limb ultrasonography images</p> <p>So, what is DL_Track_US all about? The DL_Track_US algorithm was first presented by Neil Cronin, Olivier Seynnes and Taija Finni in 2020. The algorithm makes extensive use of fully convolutional neural networks trained on a fair amount of ultrasonography images of the human lower limb. Specifically, the dataset included longitudinal ultrasonography images from the human gastrocnemius medialis, tibialis anterior, soleus and vastus lateralis. The algorithm is able to analyse muscle architectural parameters (muscle thickness, fasciclelength and pennation angle) in both, single image files as well as videos. By employing deep learning models, the DL_Track_US algorithm is one of the first fully automated algorithms, requiring no user input during the analysis. Then in 2022, we (Paul Ritsche, Olivier Synnes, Neil Cronin) have updated the code and deep learning models substantially, added a graphical user interface, manual analysis and an extensive documentation. Moreover we turned everything into an openly available Pypi package.</p>"},{"location":"#why-use-dl_track_us","title":"Why use DL_Track_US?","text":"<p>Using the DL_Track_US python package to analyze muscle architectural parameters in human lower limb muscle ultrasonography images hase two main advantages. The analysis is objectified when using the automated analysis types for images and videos because no user input is required during the analysis process. Secondly, the required analysis time for image or video analysis is drastically reduced compared to manual analysis. Whereas an image or video frame manual analysis takes about one minute, DL_Track_US analyzes images and video frames in less than one second. This allows users to analyze large amounts of images without supervision during the analysis process in relatively short amounts of time.</p>"},{"location":"#good-to-know","title":"Good to know","text":"<p>Before you get started, here are some important tips:</p> <ul> <li>Test the algorithm first and train your own models if necessary, especially if  you plan to analyze images taken from different muscles.</li> <li>Be cautious about the generalizability of the models, even though  extensive data augmentation was used during the model training process.  Different device types, muscle regions, and settings during image  acquisition may impact model performance.</li> <li>Image quality is crucial. The images should have good contrast, appropriate  brightness, clearly visible fascicles and aponeuroses, and clear alignment of  the probe with the fascicle plane.</li> <li>If model performance is poor, visually inspect the output of the models and  compare them to manual analysis results. Adjust analysis parameters or  train a separate model if necessary.</li> <li>Follow the provided testing procedures in the DL_Track_US/tests folder to  ensure proper functionality on your computer.</li> </ul>"},{"location":"#limitations","title":"Limitations","text":"<p>Currently, we have not provided unit testing for the functions and modules included in the DL_Track_US package. Moreover, the muscles included in the training data set are limited to the lower extremities. Although we included images from as many ultrasonography devices as possible, we were only able to include images from four different devices. Therefore, users aiming to analyze images from different muscles or different ultrasonography devices might be required to train their own models because the provided pre-trained models result in bad segmentations. The time required for image analysis compared to manual analysis is tremendously reduced. However, employing the networks for analysis of long videos containing many frames (&gt;2000) may still require a few hours. Lastly, even though  DL_Track_US objectifies the analysis of ultrasonography images when using the automated analysis types, we labeled the images manually. Therefore, we introduced some subjectivity into the datasets.</p>"},{"location":"about_us/","title":"About Us","text":""},{"location":"about_us/#mission","title":"Mission","text":""},{"location":"about_us/#goals","title":"Goals","text":""},{"location":"about_us/#roadmap","title":"Roadmap","text":""},{"location":"about_us/#meet-de-developers","title":"Meet de developers","text":""},{"location":"about_us/#meet-the-contributors","title":"Meet the contributors","text":""},{"location":"automated_image_analysis/","title":"Autamated Image Analysis","text":"<p>On this page you get to know the automated image analysis. The images are evaluated without user input and may be scaled.  Scaling the images will ensure estimated muscle architectural parameters are converted to centimetre units. For this type of analysis, single images (not videos) are a prerequisite. These images should be contained in a single folder, like in the \u201cDL_Track_US_example/images\u201d folder.</p> <p>If you haven\u2019t downloaded this folder yet, please do so now (link: DL_Track_US - Examples &amp; Models). Unzip the folder and put it somewhere accessible.</p>"},{"location":"automated_image_analysis/#1-creating-image-directory-flipflagtxt-file","title":"1. Creating Image Directory &amp; FlipFlag.txt File","text":"<ul> <li>All images you want to analyze should be placed in one folder.</li> <li>The \u201cDL_Track_US_example/images\u201c folder contains 4 images and  a flip_flag.txt file.</li> <li>It is not required to have the flip_flag.txt file in the same folder as the images to be analysed, but it is convenient.</li> </ul> <ul> <li>Lets take a closer look at the flip_flag.txt file.</li> <li>For every image there must be a flip-flag. </li> <li>The flip-flag determines if an image is flipped during analysis or not. A \u201c0\u201d stands for no flipping, whereas \u201c1\u201d means flip the image.</li> <li>If the number of flip-flags and images doesn\u2019t match, an error is raised.</li> </ul> <ul> <li>Another possible way to specify is displayed below. This is relevant when multiple subfolders are included, as each line then represents a subfolder.</li> </ul> <ul> <li>None of the example images must be flipped. Their fascicle orientation is correct, with fascicles originating at the bottom left and inserting on the top right.</li> <li>Below is a visual representation of a correct fascicle orientation. If the fascicles in your image are orientated differently, please specify a \u201c1\u201d as a flip-flag for those images.</li> </ul>"},{"location":"automated_image_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<p>Once the GUI is openend, the first step of every analysis type in DL_Track_US is to specify the input directories in the graphical user interface (GUI).</p> <ul> <li>First, specify the path to the folder containing the images to be analysed. Remember this was the folder \u201cDL_Track_US_example/images\u201d.<ul> <li>By clicking on the Inputs button a selection window opens were you need to select the images folder.  </li> <li>Click Select folder to specify the path in the GUI.</li> </ul> </li> </ul> <p> </p> <ul> <li>Secondly, specify the absolute path to the aponeurosis neural network which is located in the \u201cDL_Track_US_example/DL_Track_US_models\u201d Folder.<ul> <li>By clicking on the Apo Model button, a selection window opens were you need to select the aponeurosis neural network.</li> <li>Click Open to specify the path to the aponeurosis neural network in the GUI.</li> </ul> </li> </ul> <p> </p> <ul> <li>Thirdly, specify the absolute path to the fascicle neural network which is also located in the \u201cDL_Track_US_example/DL_Track_US_models\u201d folder.<ul> <li>By clicking on the Fasc Model button, a selection window opens were you need to select the fascicle neural network.</li> <li>Click Open to specify the path to the fascicle neural network in the GUI.</li> </ul> </li> </ul> <p> </p> <p>In the next section you will specify all relevant analysis parameters, including the analysis type. We will also explain what each parameter is used for.</p>"},{"location":"automated_image_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":"<p>As a first step, you will select the right analysis type in the GUI.</p> <ul> <li>Select image in the dropdown-menu.</li> </ul> <p></p> <p>Next, you need to specify the Image Type.</p> <ul> <li>The ending of the Image Type must match the ending of your images, otherwise no files are found by DL_Track_US.</li> <li>You can either select a pre-specified ending from the dropdown list or type in your own ending.</li> <li>Please keep the formatting similar to the Image Types provided in the dropdown list.</li> <li>All the images in the \u201cDL_Track_US_example/images\u201d folder are of the Image Type \u201c.tif\u201d. Thus, you should select the \u201c/*.tif\u201d Image Type.</li> </ul> <p></p> <p>Subsequently, you need to specify the image Scaling Type.</p> <ul> <li>Scaling in general has the huge advantage that the resulting estimated muscle architectural features are in centimetre units rather than pixel units.</li> <li>There are three Scyling Types in the DL_Track_US package.</li> <li>For this tutorial however, you will select the \"None\" option as displayed below.</li> </ul> <p></p> <ul> <li>Another Scaling Type is \u201cBar\u201d. This Scaling Type is only applicable if there are scaling bars in the right side of the ultrasonography image:</li> <li>The scaling bars do not need to look exactly like the ones in the image below. They just need to be next to the image and clearly separated from each other.</li> <li>We advise you to try this Scaling type on a few of your images and find out for yourself if it works.</li> <li>Files that cannot be analysed with this Scaling type will be recorded in an failed_images.txt file in the image input folder.</li> </ul> <p></p> <p>The last of the three Scaling Types is  \u201cManual\u201d. This Scaling Type requires input from the user.</p> <ul> <li>Whenever you use \u201cBar\u201d or \u201cManual\u201d as your Scaling Type, make sure that the minimal distance between the scaling bars or the known distance between the manually specified points is represented in the Spacing parameter.</li> <li>Select the Spacing parameter from the dropdown list as 5, 10, 15 or 20 millimetre. For this tutorial it is not necessary to select anything, as the Spacing parameter is not used during an analysis with Scaling Type \u201cNone\u201d.</li> </ul> <p></p> <ul> <li>When you choose \u201cManual\u201d as your Scaling type, you need to manually place two points on the image using the left mouse button.</li> <li>In order to do this, you need to click Calibrate.</li> </ul> <p></p> <ul> <li>Then, just click one time with your left mouse button to record the first point (a red dot will apear).</li> <li>Place the second point at a known distance of either 5, 10, 15 or 20 millimetre.</li> <li>Afterwards, click Confirm.</li> </ul> <p></p> <p>After confirming a messagebox should appear with the distance of the spacing parameter in pixels.</p> <p></p> <ul> <li>In version 0.2.1 we introduced a new feature to DL_Track_US, called the Filter Fascicle option.</li> <li>Here, you have two options, \u201cYES\u201d or \u201cNO\u201d.</li> <li>Using \u201cYES\u201d all fascicles that overlap will be removed.</li> </ul> <p></p> <p>Here are some results demonstrating the difference.</p> <p></p> <p>As a next Stept you need to specify the absolute path to the flip_flag.txt file.</p> <ul> <li>By clicking the Flip Flags button, a dialogue will pop up and you can select the flip_flag.txt file.</li> <li>In this example, the flip_flag.txt file is located at \u201cDL_Track_US_example/images\u201d.</li> <li>Remember, the amount of flip-flags in the flip_flag.txt file must equal the amount of images in the images folder.</li> </ul> <p> </p>"},{"location":"automated_image_analysis/#4-adjusting-settings","title":"4. Adjusting Settings","text":"<p>As a last step, you need to adjust the settings for the aponeurosis and fascicle neural networks. If you click on the settings wheel a  python script with the name \"settings.py\" opens up in your default text editor. On this page, all parameters used by the aponeurosis and fascicles neural networks during inference are specified. The default values are always listed on the right hand side of the parameters. The settings are explained in detail at the top of the settings.py file.</p> <p> </p> <ul> <li> <p>The aponeurosis detection threshold determines the threshold of the minimal acceptable probability by which a pixel is predicted as aponeurosis. The lower, the more pixels will be classified as aponeurosis.</p> </li> <li> <p>Changing the aponeurosis length threshold will result in longer or shorter structures detected as aponeurosis.</p> </li> <li> <p>The fascicle detection threshold and the fascicle lenght threshold are the same thing, just for the fasicles.</p> </li> <li> <p>The minimal muscle width determines the minimal acceptable distance between superficial and deep aponeurosis.</p> </li> <li> <p>Minimal and Maximal Pennation describe the respective minimal and maximal pennation angle that is physiologically possible in the analysed image/muscle.</p> </li> <li> <p>The fascile calculation method determines the approach by which the fascile length is calculated. This can either be linear_extrapolation, curve_polyfitting, curve_connect_linear, curve_connect_poly or orientation_map.</p> </li> <li> <p>The lower the fascile contour tolerance, the shorter the minimal acceptable length of detected fascicle segments to be included in the results.</p> </li> <li> <p>The lower the aponeurosis distance tolerance, the nearer a fascicle fragment must be to the aponeurosis. This increases certainty of pennation angle calculation and extrapolation.</p> </li> </ul> <p>For this tutorial, you can leave all parameters the way they are. You can set the parameters by saving the python file. Adapt these parameters according to your images in analyses. For future analyses, it\u2019s best you test the ideal parameter configuration in a small sample of your images prior to the actual analysis. If you should somehow distruct the settings.py file there is a backup called _backup_settings.py.</p>"},{"location":"automated_image_analysis/#5-running-breaking-dl_track_us","title":"5. Running / Breaking DL_Track_US","text":"<ul> <li>By clicking the Run button in the main GUI window, you can start the analysis.</li> <li>Moreover, you can see that there is a Break button placed in the GUI as well.</li> <li>Clicking the Break button allows you to stop the analysis at any point. The currently evaluated image will be processed and then the analysis isterminated.</li> </ul> <p>After running the analyis the three lines are displayed in the line graph:</p> <ul> <li>Median Fascicle Length</li> <li>Median Filtered Fascicle Length</li> <li>Filtered Median Fascicle Length</li> </ul> <p></p> <ul> <li>In the \u201cDL_Track_US_example/images\u201d folder, you will see that two files will be / have been created, ResultImages.pdf and Results.xlsx.</li> <li>The ResultImages.pdf file contains each original input image and concomitant prediction results with fascicles and aponeurosis displayed.</li> <li>The Results.xlsx file contains the actual architectural parameter estimates for each input image. There, the median value of all detected muscle fascicle length and pennation angles as well a the calculated muscle thickness will be displayed. Each input image is displayed in a separate row.</li> <li>Note that the ResultImages.pdf file can be opened only after the Results.xlsx was created.</li> </ul> <p></p> <p>You have now completed the DL_Track_US tutorial for automated image analysis! There is one more thing though, error handling. Take a look at the next section to get more information.</p>"},{"location":"automated_image_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>Whenever an error occurs during the analysis process, the DL_Track_US GUI will open a messagebox. This looks always similar to this:</p> <p></p> <p>We tried to formulate these messageboxes as concise as possible. Just follow their instructions to fix the error and run the analysis anew. In case an error occurs that is not caught by an error messagebox, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum. Please take a look here how do best do this.</p>"},{"location":"automated_video_analysis/","title":"Automated Video Analysis","text":"<p>On this page you get to know the automated video analysis. The videos are evaluated without user input and may be scaled. The videos should be contained in a single folder, like in the \u201cDL_Track_US_example/videos\u201d folder.</p> <p>If you haven\u2019t downloaded this folder, please do so now (link: DL_Track_US - Examples &amp; Models). Unzip the folder and put it somewhere accessible.</p> <p>The automated video analysis is very similar to the automated image analysis.  In fact, the inputted video is analysed frame by frame and each frame is therefore treated like an independent image. Moreover, only few analysis parameters are different between both analysis types. </p> <p>Once the analysis of the video file is finished, a \u201eproc.avi\u201c file will be created at the directoy of the input video. The \u201eproc.avi\u201c file can be openend with, i.e., VLC-Player on windows and Omni-Player on macOS.</p>"},{"location":"automated_video_analysis/#1-creating-video-and-network-directories","title":"1. Creating Video and Network Directories","text":"<ul> <li>In order for DL_Track_US to recognize your videos, they should best be in a single folder.</li> <li>The \u201cDL_Track_US_example/videos\u201c folder contains one video.</li> </ul>"},{"location":"automated_video_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<p>Once the GUI is openend, the first step of every analysis type in DL_Track_US is to specify the input directories in the graphical user interface (GUI).</p> <ul> <li>Start the analysis with specifying the path to the folder containing the video to be analysed.</li> <li>Remember this was the folder \"DL_Track_US_example/video\". By clicking on the Inputs button in the GUI a selection window opens were you need to select the images folder.</li> <li>Click select folder to specify the path in the GUI.</li> </ul> <p></p> <p>Now, you will specify the absolute path to the aponeurosis neural network.</p> <ul> <li>Remember that the model is in the \u201cDL_Track_US_example/models\u201d folder.</li> <li>By clicking on the Apo Model button in the GUI a selection window opens were you need to select the aponeurosis neural network in the models folder.</li> <li>Click open to specify the path to the aponeurosis neural network in the GUI</li> </ul> <p> </p> <p>Next, you will specify the absolute path to the fascicle neural network.</p> <ul> <li>The model is in the \u201cDL_Track_US_example/models\u201d folder.</li> <li>By clicking on the Fasc Model button in the GUI a selection window opens were you need to select the fascicle neural network in the models folder.</li> <li>Click open to specify the path to the fascicle neural network in the GUI.</li> </ul> <p> </p>"},{"location":"automated_video_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":"<p>As a first step, you will select the right analysis type in the GUI.</p> <ul> <li>Please select Video from the dropdown-menu.</li> </ul> <p></p> <p>You now need to specify the Video Type.</p> <ul> <li>The ending of the Video Type must match the ending of your videos, otherwise no files are found by DL_Track_US.</li> <li>You can either select a pre-specified ending from the dropdown list or type in your own ending.</li> <li>Please keep the formatting similar to those Video Type provided in the dropdown list.</li> <li>The video in the \u201cDL_Track_US_example/video\u201d folder are of the Video Type \u201c.mp4\u201d. Thus, you should select the \u201c/*.mp4\u201d Video Type.</li> </ul> <p></p> <p>Subsequently, you need to specify the video Scaling Type.</p> <ul> <li>Scaling in general has the advantage that the resulting estimated muscle architectural features are in centimetre units rather than pixel units.</li> <li>There are two Scaling Types in the DL_Track_US package.</li> <li>For this tutorial however, you will select the \u201cNone\u201d option as displayed below.</li> </ul> <p></p> <p>The other Scaling Type is \u201cManual\u201d. This Scaling Type requires input from the user.</p> <ul> <li>Whenever you use \u201cManual\u201d as your Scaling Type, make sure that the minimal distance between the scaling bars or the known distance between the manually specified points is represented in the Spacing parameter.</li> <li>Select the Spacing parameter from the dropdown list as 5, 10, 15 or 20 millimetre. For this tutorial it is not necessary to select anything, as the Spacing parameter is not used during an analysis with Scaling Type \u201cNone\u201d.</li> </ul> <p></p> <ul> <li>When you choose \u201cManual\u201d as your Scaling type, you need to manually place two points on the image using the left mouse button.</li> <li>In order to do this, you need to click Calibrate.</li> </ul> <p></p> <ul> <li>Then, just click one time with your left mouse button to record the first point (a red dot will apear).</li> <li>Place the second point at a known distance of either 5, 10, 15 or 20 millimetre.</li> <li>Afterwards, click Confirm.</li> </ul> <p></p> <p>After confirming a messagebox should appear with the distance of the spacing parameter in pixels.</p> <p></p> <ul> <li>In version 0.2.1 we introduced a new feature to DL_Track_US, called the Filter Fascicle option.</li> <li>Here, you have two options, \u201cYES\u201d or \u201cNO\u201d.</li> <li>Using \u201cYES\u201d all fascicles that overlap will be removed.</li> </ul> <p></p> <p>Here are some results demonstrating the difference in an image, for video frames the effect would be similar.</p> <p></p> <p>Another parameter that you need to specify is the Flip Options parameters.</p> <ul> <li>The Flip Options parameter determines if the whole video is flipped along the vertical axis. \u201cFlip\u201d stands for flipping the video, whereas \u201cDon\u2019t Flip\u201d means please do not flip the video.</li> <li>The example video must be flipped.</li> <li>Its fascicle orientation is incorrect, with fascicles originating at the bottom right and inserting on the top left.</li> <li>Below is a visual representation of a correct fascicle orientation.</li> <li>The fascicles are originating at the bottom left and are inserting on the top right.</li> <li>Note that all videos in the specified input folder, in this case the DL_Track_US_example/video\u201d folder, MUST have the same fascicle orientation, since the Flip Option is applied to all of them.</li> </ul> <p> </p> <p>The next step is to specify the Frame Steps.</p> <ul> <li>You can either select a pre-specified Frame Step from the dropdown list or type your Frame Step.</li> <li>The Frame Step is used during the analysis as a step size while iterating through all the frames in a video.</li> <li>In this tutorial you should specify a Frame Step of 1. This means that every video frame is analysed. With a Frame Step of 3, every 3rd frame is analysed. With a Frame Step of 10, every 10th frame an so on.</li> <li>Although information is lost when you skip frames during the analysis, it also reduces the overall analysis time.</li> </ul> <p></p>"},{"location":"automated_video_analysis/#4-adjusting-settings","title":"4. Adjusting Settings","text":"<p>As a last step, you need to adjust the settings for the aponeurosis and fascicle neural networks. If you click on the settings wheel a  python script with the name \"settings.py\" opens up in your default text editor. On this page, all parameters used by the aponeurosis and fascicles neural networks during inference are specified. The default values are always listed on the right hand side of the parameters. The settings are explained in detail at the top of the settings.py file.</p> <p> </p> <ul> <li> <p>The aponeurosis detection threshold determines the threshold of the minimal acceptable probability by which a pixel is predicted as aponeurosis. The lower, the more pixels will be classified as aponeurosis.</p> </li> <li> <p>Changing the aponeurosis length threshold will result in longer or shorter structures detected as aponeurosis.</p> </li> <li> <p>The fascicle detection threshold and the fascicle lenght threshold are the same thing, just for the fasicles.</p> </li> <li> <p>The minimal muscle width determines the minimal acceptable distance between superficial and deep aponeurosis.</p> </li> <li> <p>Minimal and Maximal Pennation describe the respective minimal and maximal pennation angle that is physiologically possible in the analysed image/muscle.</p> </li> <li> <p>The fascile calculation method determines the approach by which the fascile length is calculated. This can either be linear_extrapolation, curve_polyfitting, curve_connect_linear, curve_connect_poly or orientation_map.</p> </li> <li> <p>The lower the fascile contour tolerance, the shorter the minimal acceptable length of detected fascicle segments to be included in the results.</p> </li> <li> <p>The lower the aponeurosis distance tolerance, the nearer a fascicle fragment must be to the aponeurosis. This increases certainty of pennation angle calculation and extrapolation.</p> </li> </ul> <p>For this tutorial, you can leave all parameters the way they are. You can set the parameters by saving the python file. Adapt these parameters according to your images in analyses. For future analyses, it\u2019s best you test the ideal parameter configuration in a small sample of your images prior to the actual analysis. If you should somehow distruct the settings.py file there is a backup called _backup_settings.py.</p>"},{"location":"automated_video_analysis/#5-running-breaking-dl_track_us","title":"5. Running / Breaking DL_Track_US","text":"<ul> <li>By clicking the Run button in the main GUI window, you can start the analysis.</li> <li>Moreover, you can see that there is a Break button placed in the GUI as well.</li> <li>Clicking the Break button allows you to stop the analysis at any point. The currently evaluated image will be processed and then the analysis is terminated.</li> </ul> <p>After running the analyis the three lines are displayed in the line graph:</p> <ul> <li>Median Fascicle Length</li> <li>Median Filtered Fascicle Length</li> <li>Filtered Median Fascicle Length</li> </ul> <p></p> <p>Subsequently to clicking the Run button in the main GUI, navigate again to the \u201cDL_Track_US_example/video\u201d.</p> <ul> <li>You will see that two files will be / have been created, calf_raise_proc.avi  and calf_raise.xlsx.</li> <li>The calf_raise_proc.avi file contains each the input video with overlaid segmented fascicles and aponeurosis. This file allows you to visually inspect the model outputs.</li> <li>The calf_raise.xlsx file contains the actual architectural parameter estimates for each video frame. There, all detected muscle fascicle lengths and pennation angles as well a the calculated muscle thickness will be displayed. Each video frame is displayed in a separate row.</li> <li>Note that the calf_raise_proc.avi file can be opened only after the calf_raise.xlsx. was created.</li> </ul> <p></p>"},{"location":"automated_video_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>Whenever an error occurs during the analysis process, the DL_Track_US GUI will open a messagebox. This looks always similar to this:</p> <p></p> <p>We tried to formulate these messageboxes as concise as possible. Just follow their instructions to fix the error and run the analysis anew. In case an error occurs that is not caught by an error messagebox, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum. Please take a look here how do best do this.</p>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#1-our-pledge","title":"1. Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our  community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and  expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and  orientation.</p>"},{"location":"code_of_conduct/#2-our-standards","title":"2. Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others\u2019 private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"code_of_conduct/#3-our-responsibilities","title":"3. Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior \u00a8 and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits,  issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any  contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#4-scope","title":"4. Scope","text":"<p>This Code of Conduct applies within all project spaces, and it also applies when an individual is representing the project  or its community in public spaces. Examples of representing a project or community include using an official project e-mail  address, posting via an official social media account, or acting as an appointed representative at an online or offline event.  Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#5-enforcement","title":"5. Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [paul.ritsche@unibas.ch].  All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances.  The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement  policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined  by other members of the project\u2019s leadership.</p>"},{"location":"code_of_conduct/#6-attribution","title":"6. Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant homepage, version 1.4, available at https://www.contributor-covenant.org/version/1/4/ code-of-conduct.html. For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We are happy whenever you decide to contribute to DL_Track_US. However, when contributing to DL_Track_US, please first file an issue in our Github issue section. Label the issue with \u201cimprovement\u201d and describe your suggestion. Please formulate the title of the issue similar to this: Contribution - Your contribution idea. Please also state in the issue whether you want to implement it yourself / already implemented it in your code or if you would like us to implement it. We will be in touch with you. Please note we have a code of conduct, please follow it in all your interactions with the project.</p> <p>In case you have decided to implement your suggestion yourself and we agreed that you should file a pull request, take a look at the steps listed below.</p>"},{"location":"contributing/#1-pull-request-process-for-contributing-own-material","title":"1. Pull Request Process for contributing own material","text":"<ol> <li>Update the DL_Track_US_tutorial.pdf in the docs folder with details of changes to the interface should they be relevant for the user. Simply add the step where it is during the analysis or create a new chapter.</li> <li>Add a changelog to the changelog.d folder describing exactly what you changed in the project and the environment.</li> <li>When adding new functions, please follow the code and docstring styles used throughout the code. FYI, we used the Numpy styleguide.</li> <li>Create a new branch (named yourchange_yourinitials) and a Pull Request to merge your work on the main branch of the project.</li> </ol>"},{"location":"contributing/#2-report-a-bug","title":"2. Report a bug","text":"<p>In order to report a bug, please file an issue in our issue section on Github. Label your issue with the \u201cbug\u201d label and describe the bug you found. Please formulate the title of the issue similar to this: Bugreport - Your bug that occured. Please describe the occurence of the bug as reproducible as possible. It\u2019s best to share with us the following in the issue:</p> <ul> <li>operating system</li> <li>the error raised by your code</li> <li>all steps to reproduce the bug</li> <li>code that produced the bug</li> </ul> <p>We will then be in touch with you and try to solve the problem as quickly as possible. Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"contributing/#3-getting-support","title":"3. Getting Support","text":"<p>If you have any questions about the project, encountered problems / need help during the installation procedure or encountered problems / need help during the usage of DL_Track_US_US not related to bugs, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum. This is the space to have conversations, ask questions and post answers without opening issues.</p>"},{"location":"crop_video/","title":"Crop Video","text":"<p>In order to save time when analysing videos, you can cut out the frames without any movement at the start and/or end of videos. To do this, follow these steps:</p> <ul> <li>Once you started the GUI and the main GUI window opened, click on the Advanced Methods.</li> <li>In the Select Method Dropdown select \u201cCrop Video\u201d. The separate \u201cCrop Video Window\u201d will pop up.</li> </ul> <p> </p> <ul> <li>First, you need to specify the video you want to crop. Click the Load Video button and select the video.</li> </ul> <p></p> <p>After successfully loading the video, the UI should look like this.</p> <ul> <li>With the yellow slider you can watch through the video and determine the sections without muscle movement.</li> <li>Then you can type in the Start- and End Frame.</li> </ul> <p></p> <ul> <li>By clicking the Browse button you can selcect the folder, where the cropped video is saved.</li> <li>Finally, click Crop Video.</li> </ul> <p></p>"},{"location":"inspecting_masks/","title":"Inspecting Masks","text":"<p>Data quality is of utmost importance when labelling the images. In version 0.2.1 of DL_Track_US we included an option to inspect the labelled images and corresponding masks.</p> <ul> <li>Once you started the GUI and the main GUI window opened, click on the Advanced Methods button to select the relevant directories and model training parameters.</li> <li>In the Select Method Dropdown select \u201cInspect Masks\u201d. The separate \u201cMask Inspection Window\u201d will pop up. We will explain this window on the next page.</li> </ul> <p> </p> <ul> <li>First, you need to specify the relevant directories for the image/mask inspection.</li> <li>Three folders are of relevance here, \u201coutput_images\u201d, \u201cfascicle_masks\u201d, \u201caponeurosis_masks\u201d. They should have been created during the labelling process we explained in the previous chapter.</li> <li>Given that the number of fascicle/aponeurosis masks might differ, you can inspect both masks separately.</li> <li>Specify the directory containing the \u201coutput_images\u201d clicking the  Image Dir button.</li> <li>Specify the directory containing the respective \u201cfascicle/aponeurosis masks\u201d clicking the Mask Dir button.</li> <li>The Start Index allows you to specify the index/number of the image you want to start inspecting.</li> </ul> <p></p> <ul> <li>Clicking on the Inspect Masks button, you will start the inspection process.</li> </ul> <p>Given that the number of images and masks as well as the names of images and masks must be the same, one of two things will happen next:</p> <ol> <li>Number of images and masks is equal and naming is correct. You will see a messagebox telling you so. Click OK to continue.</li> </ol> <p></p> <ol> <li>Number of images and masks is not equal and/or naming is not correct. A table will appear telling you which image names are incorrect, in which directory they occur and if the number of images differs between the directories. Based on this, go on to delete/change the images/image names.</li> </ol> <p>Independently of what happened before, the \u201cMask Inspection GUI\u201d will open and the previous windows will be closed.</p> <p></p> <ul> <li>You can now follow the instruction displayed in the GUI.</li> <li>The labels will be projected on the image in an opague green.</li> <li>Be aware the the Delete button will permanently delete the image/mask pair in the respective folders. Making copies of the folders priorly might be advantageous, in case you want to keep the images/masks for corrections.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>We offer two possible installation approaches for our DL_Track_US software. The first option is to download the DL_Track_US executable file. The second option we describe is DL_Track package installation via Github and pythons package manager pip. We want to inform you that there are more ways to install the package. However, we do not aim to be complete and rather demonstrate an (in our opinion) user friendly way for the installation of DL_Track_US. Moreover, we advise users with less programming experience to make use of the first option and download the executable file.</p>"},{"location":"installation/#1-download-the-dl_track_us-executable","title":"1. Download the DL_Track_US executable","text":"<ol> <li>Got to the OSF webpage containing the DL_Track_US executable, the pre-trained models and the example files using this link.</li> <li>Download the DL_Track_US_example.zip folder and unpack the file.</li> <li>Find the DL_Track_US.exe executable located in the DL_Track_US_example/executable folder.</li> <li>Open the DL_Track_US_GUI by double clicking the DL_Track_US.exe file and start with the testing procedure to check that everything works properly (see Examples and Testing). In case you get an anti-virus notification, trust us and click it away. We assure you the software is harmless.</li> </ol>"},{"location":"installation/#2-install-dl_track_us-via-github-pip-and-pypiorg","title":"2. Install DL_Track_US via Github, pip and Pypi.org","text":"<p>In case you want to use this way to install and run DL_Track_US, we advise you to setup conda (see step 1) and download the environment.yml file from the repo (see steps 5-8). If you want to actively contribute to the project or customize the code, it might be usefull to you to do all of the following steps (for more information see Contributing Guidelines).</p> <p>Step 1. Anaconda setup (only before first usage and if Anaconda/minicoda is not already installed).</p> <p>Install Anaconda (click \u2018Download\u2019 and be sure to choose \u2018Python 3.X Version\u2019 (where the X represents the latest version being offered. IMPORTANT: Make sure you tick the \u2018Add Anaconda to my PATH environment variable\u2019 box).</p> <p>Step 2. (Only required for MacOS users, contributing or development) Git setup (only before first usage and if Git is not already installed). This is optional and only required when you want to clone the whole DL_Track_US Github repository.</p> <p>In case you have never used Git before on you computer, please install it using the instructions provided here.</p> <p>Step 3. (Only required for MacOS users, contributing or development) Create a directory for DL_Track.</p> <p>On your computer create a specific directory for DL_Track_US (for example \u201cDL_Track_US\u201d) and navigate there. You can use Git as a version control system. Once there open a git bash with right click and then \u201cGit Bash Here\u201d. In the bash terminal, type the following:</p> <pre><code>git init\n</code></pre> <p>This will initialize a git repository and allows you to continue. If run into problems, check this website.</p> <p>Step 4. (Only required for MacOS users, contributing or development) Clone the DL_Track_US Github repository into a pre-specified folder (for example \u201cDL_Track_US\u201d) by typing the following code in your bash window:</p> <pre><code>git clone https://github.com/PaulRitsche/DL_Track_US.git\n</code></pre> <p>This will clone the entire repository to your local computer. To make sure that everything worked, see if the files in your local directory match the ones you can find in the Github DL_Track_US repository. If you run into problem, check this [website]https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository).</p> <p>Alternatively, you can only download the environment.yml file from the DL_Track_US repo and continue to the next step.</p> <p>Step 5. Create the virtual environment required for DL_Track_US.</p> <p>DL_Track is bound to a specific python version (3.10). To create an environment for DL_Track_US, type the following command in your Git bash terminal:</p> <pre><code>conda create -n DL_Track_US python=3.10\n</code></pre> <p>Step 6. Activate the environment for usage of DL_Track_US.</p> <p>You can now activate the virtual environment by typing:</p> <pre><code>conda activate DL_Track_US\n</code></pre> <p>An active conda environment is visible in () brackets befor your current path in the bash terminal. In this case, this should look something like (DL_Track_US) C:/user/\u2026/DL_Track_US.Then, download the DL_Track_US package by typing:</p> <p>Step 7. Install the DL_Track_US package.</p> <p>Attention: The next part of Step 7 is NOT relevant for MacOS users:</p> <p>You can directly install the DL_Track_US package from Pypi. To do so, type the following command in your bash terminal:</p> <pre><code>pip install DL-Track-US==0.2.1\n</code></pre> <p>All the package dependencies will be installed automatically. You can verify whether the environment was correctly created by typing the following command in your bash terminal:</p> <pre><code>conda list\n</code></pre> <p>Now, all packages included in the DL_Track_US environment will be listed and you can check if all packages listed in the \u201cDL_Track_US/environment.yml\u201d file under the section \u201c- pip\u201d are included in the DL_Track environment. If you run into problems open a discussion in the Q&amp;A section of DL_Track_US discussions and assign the label \u201cProblem\u201d.</p> <p>Attention: The next part of Step 7 is ONLY relevant for MacOS users:</p> <p>Do not install the DL_Track_US package from Pypi. We advise you to use the provided requirements.txt file for environment creation. You need to create and activate the environment first (see Step 5 &amp; 6) and navigate into the folder that you cloned from Github (DL_Track_US) with the bash terminal. You can do that by typing \u201ccd\u201d followed by the path to the folder containing the requirements.txt file. This should look something like:</p> <pre><code>cd /.../.../DLTrack/DL_Track_US\n</code></pre> <p>Then you can install the requirements of DL_Track with:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Install the DL_Track package locally to make use of its functionalities with:</p> <pre><code>python -m pip install -e .\n</code></pre> <p>There are some more steps necessary for DL_Track_US usage, you\u2019ll finde the instructions in the usage section.</p> <p>Step 8. The First option of running DL_Track_US is using the installed DL_Track package. You do not need the whole cloned repository for this, only the active DL_Track_US environment. You do moreover not need be any specific directory. Type in your bash terminal:</p> <pre><code>python -m DL_Track_US\n</code></pre> <p>The main GUI should now open. If you run into problems, open a discussion in the Q&amp;A section of DL_Track_US discussions and assign the label \u201cProblem\u201d. For usage of DL_Track please take a look at the docs directory in the Github repository.</p> <p>Step 9. The second option of running DL_Track_US is using the DLTrack_GUI python script. This requires you to clone the whole directory and navigate to the directory where the DL_Track_US_GUI.py file is located. Moreover, you need the active DL_Track_US environment.</p> <p>The DL_Track_US_GUI.py file is located at the DL_Track_US/DL_Track_US folder. To execute the module type the following command in your bash terminal.</p> <pre><code>python DL_Track_US_GUI.py\n</code></pre> <p>The main GUI should now open. If you run into problems, open a discussion in the Q&amp;A section of DL_Track_US discussions and assign the label \u201cProblem\u201d. You can find an example discussion there. For usage of DL_Track_US please take a look at the docs directory in the Github repository.</p>"},{"location":"installation/#3-gpu-setup","title":"3. GPU Setup","text":"<p>Attention: The next section is only relevant for windows users!</p> <p>The processing speed of a single image or video frame analyzed with DL_Track_US is highly dependent on computing power. While possible, model inference and model training using a CPU only will decrese processing speed and prolong the model training process. Therefore, we advise to use a GPU whenever possible. Prior to using a GPU it needs to be set up. Firstly the GPU drivers must be locally installed on your computer. You can find out which drivers are right for your GPU here. Subsequent to installing the drivers, you need to install the interdependant CUDA and cuDNN software packages. To use DL_Track_US with tensorflow version 2.10 you need to install CUDA version 11.2 from here and cuDNN version 8.5 for CUDA version 11.x from here (you may need to create an nvidia account). As a next step, you need to be your own installation wizard. We refer to this video (up to date, minute 9 to minute 13) or this video (older, entire video but replace CUDA and cuDNN versions). There are procedures at the end of each video testing whether a GPU is detected by tensorflow or not. If you run into problems with the GPU/CUDA setup, please open a discussion in the Q&amp;A section of [DL_Track_US discussions]https://github.com/PaulRitsche/DL_Track_US/discussions/categories/q-a) and assign the label \u201cProblem\u201d.</p> <p>Attention : The next section is only relevant for MacOS users!</p> <p>In case you want to make use of you M1 / M2 chips for model training and / or inference, we refer you to this tutorial. There you will find a detailed description of how to enable GPU support for tensorflow. It is not strictly necessary to do that for model training or inference, but will speed up the process.</p>"},{"location":"manual_image_analysis/","title":"Manual Image Analysis","text":"<p>This page covers the manual image analysis. The images are evaluated manually by drawing the muscle thickness, fascicle length and pennation angles directly on the image. For this type of analysis, single images (not videos) are a prerequisite. These images should be contained in a single folder, like in the \u201cDL_Track_US_example/images_manual\u201d folder. </p> <p>If you haven\u2019t downloaded this folder yet, please do so now (link: DL_Track_US - Examples &amp; Models).  Unzip the folder and put it somewhere accessible.</p>"},{"location":"manual_image_analysis/#1-creating-image-directory","title":"1. Creating Image Directory","text":"<ul> <li>All images to be analyzed should be in a single folder.</li> <li>The \u201cDL_Track_US/image_manual\u201c folder contains 2 images.</li> </ul> <ul> <li>In contrast to automated image analysis, you do not need a flip_flag.txt file nor do you need neural networks that do predictions. </li> <li>In manual image analysis, you are the neural network.</li> </ul>"},{"location":"manual_image_analysis/#2-specifying-input-directories-in-the-gui","title":"2. Specifying Input Directories in the GUI","text":"<ul> <li>You will begin with specifying the path to the folder containing the images to be analysed.</li> <li>In this case we use the \u201cDL_Track_US_example/images_manual\u201d folder.</li> <li>By clicking on the Inputs button in the GUI a selection window opens were you need to select the images folder.</li> <li>Click Select folder to specify the path in the GUI.</li> </ul>"},{"location":"manual_image_analysis/#3-specifying-analysis-parameters","title":"3. Specifying Analysis Parameters","text":"<p>First, please select image_manual from the dropdown-menu.</p> <p></p> <p>Next, you need to specify the Image Type.</p> <ul> <li>The ending of the Image Type must match the ending of your images, otherwise no files are found by DL_Track_US.</li> <li>You can either select a pre-specified ending from the dropdown list or type in your own ending.</li> <li>Please keep the formatting similar to the Image Types provided in the dropdown list.</li> <li>All the images in the \u201cDL_Track_US_example/images_manual\u201d folder are of the Image Type \u201c.tif\u201d.</li> <li>Thus, you should select the \u201c/*.tif\u201d Image Type as shown below.</li> </ul> <p></p> <ul> <li>Once you have specified the Image Type, you can start with the analysis.</li> <li>You can start the analysis by clicking the Run button in the main GUI.</li> </ul> <p></p>"},{"location":"manual_image_analysis/#4-manual-analysis-of-image","title":"4. Manual Analysis of Image","text":"<p>After clicking the Run button in the main GUI, the \u201cManual Analysis window\u201d opens. </p> <ul> <li>Here is how it looks like:</li> </ul> <p></p> <p>Important to note:</p> <ul> <li>The actual lines you draw are not used during the computation of the architectural parameters.</li> <li>The start- and endpoints of each line are relevant.</li> <li>The start point is defined as the point where you clicked the left mouse button to start drawing the line.</li> <li>The endpoint is defined as the point where you released the left mouse button to stop drawing the line.</li> <li>The line follows the cursor as long as the left mouse button is pressed.</li> <li>The calculations of the scaling line length, muscle thickness, fascicle length and pennation angle are dependent on the number of specified lines/segments.</li> <li>Do NOT click somewhere random on the image during the analysis of a parameter and exactly follow the instructions. If additional clicks happened, start the analysis new by selecting the radiobutton representing the parameter again.</li> <li>If you do not follow the instructions presented in this tutorial, we cannot guarantee the correctness of the analysis results.</li> </ul> <p>First of all, you will scale the images manually so that the calculated architectural parameters are returned in centimetre rather than pixel units.</p> <ul> <li>Draw a one centimetre long straight line in the image.</li> <li>The distance of one centimetre is usually recognizable in the scaling bars in the image.</li> <li>You can initiate the scaling process by selecting the Scale Image radiobutton in the \u201cManual Analysis window\u201d.</li> <li>A messagebox will appear advising you what to do.</li> </ul> <p></p> <p>The drawn line should look like this.</p> <p></p> <p>As a next step you have the option to extend the muscle aponeuroses to ease the extrapolation of fascicles extending outside of the image.</p> <ul> <li>Select the Draw Aponeurosis button in the \u201cManual Analysis window\u201d and draw the aponeurosis lines on the image as shown below.</li> <li>A messagebox will appear advising you what to do.</li> </ul> <p> </p> <p>Now you can start with the muscle thickness assessment.</p> <ul> <li>Select the Muscle Thickness radiobutton in the \u201cManual Analysis window\u201d.</li> <li>A messagebox will appear advising you what to do.</li> <li>Draw three straight lines reaching from the superficial to the deep aponeurosis in the middle right and left portion of the muscle image.</li> </ul> <p> </p> <p>Next you can mark single fascicles on the image.</p> <ul> <li>Select the Muscle Fascicles radiobutton in the \u201cManual Analysis window\u201d.</li> <li>A messagebox will appear advising you what to do.</li> <li>Draw at least three fascicles per image in different regions of the image.</li> <li>It is possible to extrapolate the fascicles outside of the image region.</li> <li>Each fascicles MUST consist of three segments.</li> <li>Do not draw more or less segments per fascicles and pay attention to avoid any extra unwanted mouse clicks.</li> <li>One segment MUST start where the previous segment ended.</li> <li>Take a look at the image sequence below to see how it is done:</li> </ul> <p> </p> <p>Next you can manually analyse the pennation angle.</p> <ul> <li>Select the radiobutton Pennation Angle.</li> <li>A messagebox will appear advising you what to do.</li> <li>Draw at least three pennation angles per image at different regions of the image.</li> <li>Each drawn pennation angles MUST consist of two segments. The first segment should follow the orientation of the fascicle, the second segment should follow the orientation of the deep aponeurosis. The segments should both originate at the insertion of the fascicle in the deep aponeurosis.</li> <li>Please pay attention to avoid unwanted clicks on the image.</li> </ul> <p> </p>"},{"location":"manual_image_analysis/#5-saving-breaking-next-image","title":"5. Saving / Breaking / Next Image","text":"<p>There are three buttons in the \u201cManual Analysis window\u201d left to explain.  The first button is the Save Results button.</p> <ul> <li>The Save Results button is a very important button!</li> <li>Press the Save Results button once you have analyzed all parameters that you wanted to analyze and before continuing with the next image.</li> <li>An excel file with the name Manual_Results.xlsx is saved in the directory of the input images upon pressing the Save Results button. Therein, all analysis results are stored. Moreover, by pressing the Save Results, a screenshot of your current analysis is captured and stored. (Note: The image may look strange, as we can only approximate the coordinates and size of the manual analysis on your screen.)</li> <li>In your case all files are saved in the \u201cDL_Track_US_example/images_manual\u201d folder.</li> </ul> <p></p> <p>The second button we haven\u2019t explained yet is the Next Image button.</p> <ul> <li>By clicking this button, you can proceed to the next image in the input folder (in your case the \u201cDL_Track_US_example/images_manual\u201d folder).</li> <li>Please remember to press the Save Results button prior to proceeding to the next images, otherwise you analysis results for this image will be lost.</li> <li>When the Next Image button is pressed, the displayed image is updated.</li> </ul> <p> </p> <p>The last button we need to explain is the Break Analysis button.</p> <ul> <li>Pressing this button allows you to terminate the analysis and return to the main GUI window.</li> <li>A messagebox will appear asking you if you really want to stop the analysis.</li> <li>Once the Break Analysis button is pressed and you answered the messagebox with \u201cYES\u201d, the \u201cManual Analysis window\u201d will be automatically closed.</li> </ul> <p></p> <p>When you have saved your results clicking the very important button and followed our instructions during this tutorial, your input directory  \u201cDL_Track_US_example/images_manual\u201d should look like this. It should contain the images, saved screenshots, as well as the Manual_Results.xlsx file.</p> <p></p>"},{"location":"manual_image_analysis/#6-error-handling","title":"6. Error Handling","text":"<p>Whenever an error occurs during the manual image analysis process, the DL_Track_US GUI will open a messagebox. This looks always similar to this:</p> <p></p> <p>We tried to formulate these messageboxes as concise as possible. Just follow their instructions to fix the error and run the analysis anew. In case an error occurs that is not caught by an error messagebox, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum. Please take a look here how do best do this.</p>"},{"location":"manual_video_analysis/","title":"Manual Video Analysis","text":"<p>The next and last analysis type this tutorial covers is the manual video analysis. The video frames are evaluated manually by drawing the muscle thickness, fascicle length and pennation angles directly on the Image. For this type of analysis, single videos are a prerequisite. These videos should be contained in a single folder, like in the \u201cDL_Track_US_example/videos_manual\u201d folder. </p> <p>If you haven\u2019t downloaded this folder, please do so now (link: DL_Track_US - Examples &amp; Models). Unzip the folder and put it somewhere accessible.</p> <p>The manual video analysis type is identical to the manual image analysis type. The only difference is that the absolute video path must be specified instead of the File Type. The video is first converted and all the contained frames are separately stored as single images. Then, each frame image is analysed separately.</p>"},{"location":"manual_video_analysis/#1-creating-a-video-directory","title":"1. Creating a Video Directory","text":"<p>All videos to be analyzed should be in a single folder.</p> <ul> <li>The \u201cDL_Track_US_example/video_manual\u201c folder contains one video file.</li> </ul> <p></p>"},{"location":"manual_video_analysis/#2-specifying-input-directory-in-the-gui","title":"2. Specifying Input Directory in the GUI","text":"<ul> <li>Please select  video_manual from the dropdown-menu.</li> </ul> <p>Next, you need to specify the absolute File Path of the video file to be analysed.</p> <ul> <li>The example video file is placed in the \u201cDL_Track_US_example/video_manual\u201d folder.</li> <li>By clicking on the Video Path button in the GUI, a selection window opens were you need to select the example video file in the video_manual.</li> <li>Click open to specify the path to the video file in the GUI.</li> </ul> <p></p> <p>You can start the analysis by clicking the Run button in the main GUI</p> <p></p> <ul> <li>Once you clicked the Run button, the \u201cManual Analysis window\u201d will pop up.</li> <li>From here, all further steps are identical with the manual image analysis.</li> <li>The only difference though is that in the folder of the inputted video, a new folder is created containing all the single image frames.</li> <li>The scaling of the image, extending of the aponeuroses, single segment muscle thickness measurements, three segment muscle fascicle measurement and two segment pennation angle measurement are identical.</li> <li>Saving the results (with the very important button), continuing to the next image frame, terminating the analysis process and error handling is identical.</li> <li>Therefore, we kindly refer you to the Manual Image Analysis to see how all the architectural parameters are analysed.</li> </ul>"},{"location":"remove_video_parts/","title":"Remove Video Parts","text":"<ul> <li>Once you started the GUI and the main GUI window opened, click on the Advanced Methods.</li> <li>In the Select Method Dropdown select \u201cCrop Video\u201d. The separate \u201cCrop Video Window\u201d will pop up.</li> </ul> <ul> <li>First, you need to specify the video you want to remove parts from. Click the Load Video button and select the video.</li> </ul> <p>After successfully loading the video, the UI should look like this.</p> <ul> <li>With the yellow slider you can watch through the video.</li> <li>By clicking and dragging on the image with the left mouse button you can select the part you want to remove.</li> </ul> <p></p> <ul> <li>By clicking the Browse button you can selcect the folder, where the cropped video is saved.</li> <li>Finally, click Remove Parts.</li> </ul> <p></p>"},{"location":"training_your_own_networks/","title":"Training Your Own Networks","text":"<p>The DL_Track_US package GUI includes the possibility to train your own neural networks. We will demonstrate how to do this, with a few notes at the beginning:</p> <ul> <li>It is advantageous to have a working GPU setup, otherwise model training will take much longer. Take a look at our Github repository for further instructions.</li> <li>If you don\u2019t have any experience with training deep neural networks, please refer to this course. We advise you to start with the pre-defined settings. However, DL_Track_US does not allow to change the architecture of the trained neural networks.</li> </ul> <p>The paired original images and labeled masks required for the network training are located in the \u201cDL_Track_US_example/model_training\u201d folder. If you haven\u2019t downloaded this folder, please do so now (link: DL_Track_US - Examples &amp; Models).  Unzip the folder and put it somewhere accessible. We will demonstrate how to train a model that segments the muscle aponeuroses.</p> <p>Please keep in mind that the model training process will be illustrated by training a model for aponeurosis segmentation. The process is exactly the same for training a fascicle segmentation model. Solely the images and masks should then contain fascicles and fascicle labels.</p>"},{"location":"training_your_own_networks/#1-data-preparation-and-image-labeling","title":"1. Data Preparation and Image Labeling","text":"<ul> <li>The \u00abDL_Track_US_example/model_training\u201d folder contains to subfolders, apo_img_example and  apo_mask_example.</li> <li>The original images are located in the \u201capo_img_example\u201d folder.</li> <li>The corresponding masks are located in the \u201capo_maks_example\u201d folder.</li> <li>We advise you to keep a similar folder structure when you train your own models outside of this tutorial.</li> </ul> <ul> <li>Below you can see that the original image and the  corresponding masks have exactly the same name. This is SUPER MEGA important. Otherwise, the model is trained using the wrong masks for the images.</li> </ul>"},{"location":"training_your_own_networks/#2-specifying-relevant-directories","title":"2. Specifying Relevant Directories","text":"<ul> <li>As a next step, you can start the GUI.</li> <li>Once you started the GUI and the main GUI window opened, click on the Advanced Methods Button.</li> <li>A small window will pop up, where you can select the method.</li> <li>Click Train Model from the dropdown-menu.</li> <li>The window then folds out as shown in the picture below.</li> </ul> <p>Firstly, select the \u201cImage Directory\u201d.</p> <ul> <li>Click the button Images.</li> <li>A selection window will appear and you can select the folder containing the original images.</li> <li>Select the \u201cDL_Track_US_example/model_training/apo_img_example\u201d folder.</li> </ul> <p></p> <p>Your next step is to select the \u201cMask Directory\u201d.</p> <ul> <li>Click on the button Masks.</li> <li>A selection window will appear to select the folder containing the mask images.</li> <li>Select the \u201cDL_Track_US_example/model_training/apo_mask_example\u201d folder.</li> </ul> <p></p> <p>The last directory you need to select for training your own network is the \u201cOutput Directory\u201d.</p> <ul> <li>Click the button Output.</li> <li>In the Output directory, the trained model, the corresponding loss calculation results and a graphic displaying plotting the training epochs against the loss values will be saved.</li> <li>A selection window will appear and you can select any folder you like.</li> </ul> <p></p>"},{"location":"training_your_own_networks/#3-image-augmentation","title":"3. Image Augmentation","text":"<p>Image augmentation is a method to artifically increase the size of your training data. In this case, this means multiplying your images and masks based on a generator that changes certain properties of the images. You can find the details of this generator in the code documentation.</p> <p>Image augmentation optional but advisable if image number is low, i.e. &lt;1500.</p> <p>Given you have specified the relevant directories priorly, simply click the Augment Images button and see your images being multiplied. A Messagebox will indicate when the augmentation process is finished.</p> <p></p>"},{"location":"training_your_own_networks/#4-specifying-training-parameters","title":"4. Specifying Training Parameters","text":"<p>Now to specifying the training parameters.</p> <ul> <li>For the tutorial leave the pre-specified selections as they are.</li> <li>If you do not know what these training parameters mean, take a look at this course.</li> <li>The only thing we have to say is that you must NEVER use only three Epochs for actual model training.</li> <li>Such a small number of training Epochs is only acceptable for demonstration and testing purposes.</li> <li>For actual training of your own neural networks, go with at least 60 Epochs.</li> </ul> <p></p> <p>The only thing you have left to do for the training process to start is to click the Start Training button.</p> <p></p> <ul> <li>During the training process, three messageboxes will pop up.</li> <li>The first one will tell you that the images and masks were successfully loaded for further processing.</li> <li>The second one will tell you that the model was successfully compiled and can now be trained.</li> <li>The last one will tell you that the training process was completed.</li> <li>You do have a choice in each messagebox of clicking \u201cOK\u201d or \u201cCancel\u201d.</li> <li>Clicking \u201cOK\u201d will continue the training process, whereas clicking \u201cCancel\u201d will be cancelling the ongoing training process.</li> </ul> <p>Once the training process in finished, three new files will be placed in your  output directory.</p> <ul> <li>The trained model as Test_Apo.h5 file.</li> <li>The corresponding loss values for each epoch as Test_apo.csv file</li> <li>The graphical representation of the training process as Training_Results.tif file.</li> </ul>"},{"location":"training_your_own_networks/#5-using-your-own-networks","title":"5. Using Your Own Networks","text":"<p>How do you use you previously trained neural network?</p> <ul> <li>Simply select the path to your model by clicking the Apo Model or  Fasc Model buttons in the GUI, depending on which model you want to import.</li> <li>Subsequently to specifying all other relevant parameters for your analysis in the GUI (as you have learned a couple pages ago).</li> <li>DL_Track_US will now analyse your data using your own model.</li> </ul> <p></p> <p>Lastly, a short disclaimer when training your own model.</p> <ul> <li>It is bad practice using the same images for model training and inference.</li> <li>The model should not be used for analysing images it was trained on because it already knows the characteristics of these images.</li> <li>ALWAYS compare the results of your model to a manual evaluation on a few of your own images. Use different images (best from different individuals) for model training and comparison to manual analysis.</li> <li>If this seems strange to you, don\u2019t hesitate to ask for further clarification in the DL_Track_US discussion forum.</li> </ul>"},{"location":"training_your_own_networks/#6-error-handling","title":"6. Error Handling","text":"<p>Whenever an error occurs during the analysis process, the DL_Track_US GUI will open a messagebox. This looks always similar to this:</p> <p></p> <p>We tried to formulate these messageboxes as concise as possible. Just follow their instructions to fix the error and run the analysis anew. In case an error occurs that is not caught by an error messagebox, don\u2019t hesitate to report this in the Q&amp;A section in the DL_Track_US discussion forum.  lease take a look here how do best do this.</p>"},{"location":"training_your_own_networks/#7-image-labels","title":"7. Image Labels","text":"<p>When you train your own networks, you need to label your original ultrasonography images.</p> <ul> <li>We provide an automated script for image labellig.</li> <li>This script does not automatically label the images, but automates the selection processes and image / mask saving.</li> <li>The software you will perform the labelling in is called ImageJ / Fiji. You can download it here.</li> <li>The automated script \u201cImage_Labeling_DL_Track_US.ijm\u201d is located in the folder \u201cDL_Track_US/docs/labeling/\u201d in our Github repository.</li> <li>The easiest way to run the \u201cImage_Labeling_DL_Track_US.ijm\u201d script is by simply drag and drop it in the running Fiji / ImageJ window.</li> </ul> <p> </p> <p>Before you can start the labelling process:</p> <ul> <li>Create four folders in an easily accessible place.</li> <li>One folder containing the original images you want to label.</li> <li>Then create three more folders, one named \u201coutput_images\u201d, the second called \u201cfascicle_masks\u201d and the third called \u201caponeurosis_masks\u201d.</li> <li>In the \u201coutput_images\u201d the original images are saved with an adapted name.</li> <li>In the \u201cfascicle_masks\u201d and \u201caponeurosis_masks\u201d folder the respective masks are saved with the same name as the corresponding image in \u201coutput_images\u201d</li> </ul> <p></p> <p>When you have created all folders, press the Run button in the Fiji / ImageJ API to start the \u201cImage_labelling_DL_Track_US.ijm\u201d script.</p> <p></p> <p>Follow the instructions appearing in the messageboxes.</p> <ul> <li>To begin with, you need to specify the four directories.</li> <li>The first directory you need to select is the original image folder (called input dir).</li> <li>The second folder is the \u201caponeurosis_masks\u201d folder (called apo mask dir).</li> <li>The third is the \u201cfascicle_masks\u201d folder (called fasc mask dir).</li> <li>The last folder you need to specify is the \u201coutput_images\u201d folder (called image dir).</li> </ul> <p>Subsequent to specifying the directories, you are required to create the masks.</p> <ul> <li>First the aponeurosis mask, then the fascicle mask.</li> <li>Firstly, draw the superficial aponeurosis using the selected polygon tool by following the instructions in the messagebox.</li> <li>Draw around the superficial aponeurosis (double click to start drawing, click to add a segment, double click do stop drawing).</li> <li>Once you are finished, click the OK button in the messagebox to proceed to the selection of the lower aponeurosis.</li> <li>Please be careful to only include aponeurosis tissue in your selection and no surrounding tissue.</li> <li>The result should look like this for the upper and lower aponeurosis:</li> </ul> <p> </p> <p>Once you have selected the lower aponeurosis, click the OK button in the messagebox to proceed to the fascicle labelling. Follow the instructions in the messagebox.</p> <ul> <li>It is of utmost importance that you draw only over the actually visible parts of the fascicle segment.</li> <li>Make sure that you only label bright fascicle tissue that is clearly visible.</li> <li>Once you drew one fascicle with segmented line tool (double click to start drawing, click to add a segment, double click do stop drawing) click the OK button in the messagebox to proceed to the next fascicle segment.</li> <li>Draw as many segments as are clearly visible on the image.</li> <li>When you press the OK button in the messagebox without making a further selection, you will proceed to the next image in the original image folder and start again with the aponeurosis labelling.</li> <li>The result of you labelling should look something like this:</li> </ul> <p></p>"}]}